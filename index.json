{
  "Readme.html": {
    "href": "Readme.html",
    "title": "ezEngine Documentation | ezEngine",
    "keywords": "ezEngine Documentation This repository holds source files of the ezEngine documentation, which can be found at: https://ezEngine.net"
  },
  "index.html": {
    "href": "index.html",
    "title": "Welcome to ezEngine | ezEngine",
    "keywords": "Welcome to ezEngine ezEngine is a free, open source game engine written in C++. Its philosophy is to be modular and flexible, such that it can be adapted to many different use cases. EZ utilizes a plugin system to integrate features such as FMOD or Jolt Physics. This makes it possible to only include those features that you need, or to replace systems with a custom solution that works better for your requirements. Similarly, the EZ code base can be built in multiple tiers, where you either get the entire feature set, with a fully functional editor, asset management and renderer, or you can strip it down to just the base libraries and core engine functionality. This can be extremely useful if you need to build a lot of custom technology, but require a high-performance, reliable foundation. EZ has a strong emphasis on providing robust, easy to use and well-tested base functionality. It is successfully being used in such a capacity in commercial products. When to use ezEngine ezEngine is designed to be a great basis for complicated projects. It provides you with lots of functionality that is tedious and difficult to build, such as efficient STL like container classes, a high-performance scenegraph, resource streaming and much more. It can be used to build the tech for games, as well as for industry applications. In many code bases the lower level functionality is messy and buggy, because it is hard (and boring) to build these parts, and game developers rather spend time on making pretty pictures. In EZ the base functionality is clean, consistent, efficient and fully unit-tested. It builds on Windows, Mac, Linux and Android. Out of the box EZ can be used to create games just with scripting. However, it is meant for people who need or want to build their own technology and are looking for a great foundation to build on top of. The ezEditor is a powerful and robust tool that enables quick iteration on ideas with fast startup-times and WYSIWYG real-time editing. It is also completely optional, in case you need a different kind of workflow. EZ is also a very good fit for students interested in learning how modern game engines work. It is easy to setup, compiles fast, is well documented, and straight-forward to extend. We also welcome contributions in the form of code or art. When not to use ezEngine ezEngine is mainly developed on Windows. The renderer currently uses DX11. A Vulkan port is in development and the tools are being ported to Linux as well, however this is still in the early phase and not yet productively usable. It is also not comparable in feature completeness to commercial offerings such as Unreal or Unity. Although it does support scripting game logic both with TypeScript and Visual Scripting, it is not meant for low-code or no-code development. The scripting capabilities are limited, for many game ideas you need to be comfortable writing C++ code. Getting Started You can download prebuilt binaries or clone the latest dev branch. See the documentation for build instructions and available samples. Check out the FAQ. There are videos about various topics on our YouTube channel. If you need help, contact us. Everyone is welcome to contribute."
  },
  "pages/contact.html": {
    "href": "pages/contact.html",
    "title": "Contact | ezEngine",
    "keywords": "Contact Questions and Discussions For questions that are not covered by the documentation, please see the forum. We will try to answer as quickly as possible, but be aware that we don't always have the time to answer right away, and that we may be in a different time-zone than you (GMT+1). We are also reachable via Discord. Use this link to join the server: https://discord.gg/rfJewc5khZ NOTE: Our community is very small at the moment and many questions can only reasonably be answered by the EZ developers. Please don't spam the Discord channel with general C++ or build setup questions that could be answered with a web search or by reading the available documentation. Bugs If you find a bug, please file an issue. Announcements Sometimes we announce things on Twitter. E-Mail If you prefer to ask a question in private, you can try to contact us via ezengineproject@gmail.com. See Also How to Contribute"
  },
  "pages/docs/Miscellaneous/blackboard-template-asset.html": {
    "href": "pages/docs/Miscellaneous/blackboard-template-asset.html",
    "title": "Blackboard Template Asset | ezEngine",
    "keywords": "Blackboard Template Asset A blackboard template asset is used to set up a configuration of a blackboard. Components, such as the local blackboard component or the global blackboard component can reference these templates to configure their blackboards with the same configuration. This is prefereable to setting up the configuration directly on the component. Asset Properties BaseTemplates: If any other templates are referenced here, their entries are first be added to the blackboard. This template may add further entries or overwrite the previously added ones. Entries: When a template is used by a blackboard component these entries will be added to its blackboard. Some systems that use a blackboard may add their own entries, others expect an entry to already exist. For example the input component may write input state into a blackboard, but it will only do so for entries that already exist. See Also Blackboards Local Blackboard Component Global Blackboard Component"
  },
  "pages/docs/Miscellaneous/blackboards.html": {
    "href": "pages/docs/Miscellaneous/blackboards.html",
    "title": "Blackboards | ezEngine",
    "keywords": "Blackboards A blackboard is a simple data structure that holds data as key/value pairs. Each entry in a blackboard has a name (the key) and a basic value type, such as int, float or string. Entries can be added and modified at any time. Blackboards are a convenient data structure to share information between different systems. Apart from pure data storage, the blackboard implementation in EZ may also send messages whenever a value of an entry changes. This way a system can react immediately to a change, without having to poll the blackboard regularly. Using Blackboards In C++ code you can use the ezBlackboard data structure directly. In scenes and prefabs you can attach a blackboard component to an object. Systems that require a blackboard to function, such as animation controllers, will traverse the object hierarchy upwards to find a blackboard component which they can use to read and write their state. Global Blackboards Through the local blackboard component you add a blackbard to a specific object. These typically store object specific data that's used within that object hierarchy. However, you can also create global blackboards. These will exist as long as anyone references them. If, for instance, a game state holds on to a global blackboard, it will be available throughout the application lifetime, even across worlds. Global blackboards can be created from C++ using ezBlackboard::GetOrCreateGlobal() or with a global blackboard component. All global blackboards are identified by name, meaning that you can have many different ones, for different purposes, but if you use the same name in different components, they all end up using the same blackboard. Similarly, if ezBlackboardComponent::FindBlackboard() is used, and a non-empty name is provided, a global blackboard may get created, if no other matching blackboard is available. See Also Blackboard Template Asset Local Blackboard Component Global Blackboard Component"
  },
  "pages/docs/Miscellaneous/components/comment-component.html": {
    "href": "pages/docs/Miscellaneous/components/comment-component.html",
    "title": "Comment Component | ezEngine",
    "keywords": "Comment Component The comment component is for adding notes to objects in a scene. These comments are solely to explain things to other people that look at a scene or prefab. Note: Comment components are not meant for use at runtime. All instances are automatically stripped from a scene during export. Component Properties Comment: The string to store at the component. See Also Shape Icon Component"
  },
  "pages/docs/Miscellaneous/components/shape-icon-component.html": {
    "href": "pages/docs/Miscellaneous/components/shape-icon-component.html",
    "title": "Shape Icon Component | ezEngine",
    "keywords": "Shape Icon Component The shape icon component is automatically attached to new game objects. Its sole purpose is to give empty objects an icon in the 3D viewport, such that it is possible to select them easily. In some cases you may need an empty game object as an anchor, for example when setting up a rope. In such instances, it can be useful to keep the shape icon component. In most other cases it is preferable to remove it, once you attached the desired components to the game object. Note: Shape icon components are automatically removed from a scene or prefab during export. In an exported scene, none of these components exist and therefore never take up space of performance in any way. See Also Comment Component"
  },
  "pages/docs/Miscellaneous/config-file-resource.html": {
    "href": "pages/docs/Miscellaneous/config-file-resource.html",
    "title": "Config File Resource | ezEngine",
    "keywords": "Config File Resource The ezConfigFileResource is used to load files which contain lists of variable value assignments (\"key/value pairs\"). These variables are strictly typed (int, float, bool or string) and it is very efficient to look up these values at runtime. You can use this kind of configuration files for anything in your code. From general game settings, to defining the properties of different game elements. Since all resources can be hot reloaded at runtime, using ezConfigFileResources allows you to tweak values while playing to immediately see the effect. Important: Currently, interacting with resources is only possible from C++ code. File Structure The layout of config files is similar to C/C++ code, including the support for the C preprocessor. Declaring Variables To add a new variable, write its type, name and initial value: int i = 1 float f = 2.3 bool b = false string s = \"hello\" Overriding Existing Variables Once a variable has been declared the first time, it is an error to redeclare it the same way. Instead you need to add the override keyword: override int i = 11 override float f = 21.3 override bool b = true override string s = \"world\" It is also an error to declare a variable for the first time with the override keyword. This is to ensure that you have one place where the variable name is defined clearly, and to be able to point out where variables have been misspelled later on. Defining Value Names You can use #define to define a fixed name for a certain value: #define SmallValue 3 #define BigValue 5 int MyValue = BigValue This is more convenient to read and makes it easier to define and tweak values in one place. Hierarchical File Structure You can #include other config files to pull in their variable definitions: #include \"BaseConfig.ezConfig\" override int SomeValue = 7 Here the variable SomeValue must have been declared in BaseConfig.ezConfig (or another file included by that file). The code then overrides the existing variable with a custom value. If BaseConfig.ezConfig did not declare that variable, you will see an error in the log. Using this feature, it is very convenient to define variables (and their default values) for things like units in a game (players, enemies, etc) and then override variables where needed for specific unit types. Conditional Evaluation You can use all C preprocessor features, such as #ifdef to conditionally evaluate the config files: #define TESTING 1 #ifdef TESTING int PlayerHealth = 1000000 #else int PlayerHealth = 100 #endif See Also Resource Management (TODO) C preprocessor (Wikipedia)"
  },
  "pages/docs/Miscellaneous/custom-data.html": {
    "href": "pages/docs/Miscellaneous/custom-data.html",
    "title": "Custom Data | ezEngine",
    "keywords": "Custom Data See Also Config File Resource"
  },
  "pages/docs/Miscellaneous/global-blackboard-component.html": {
    "href": "pages/docs/Miscellaneous/global-blackboard-component.html",
    "title": "Global Blackboard Component | ezEngine",
    "keywords": "Global Blackboard Component A global blackboard component is used to ensure that a global blackboard exists and contains all the expected entries. A global blackboard is often added by a game state, but when only simulating a scene, no game state is active. Thus, code that relies on a global blackboard will not work as desired. By adding a global blackboard component to a scene or prefab, you can ensure that the blackboard is always set up. Component Properties Template: The blackboard template to use for configuring the global blackboard. For global blackboards this is the only way to configure the entries. ShowDebugInfo: If enabled, the component will draw a debug text overlay with the current entries and their values at the position of the game object. BlackboardName: The name for the blackboard. For global blackboards this is important to set. InitMode: What to do when the global blackboard component gets activated. Ensure Entries Exist: Only makes sure that all entries mentioned in the template exist in the global blackboard. The values are not changed for entries that already exist. Reset Entry Values: Overwrites all entry values to the initial value from the template. If additional entries exist, they are not modified. Clear Entire Blackboard: Clears the entire blackboard and then sets up the ones mentioned in the template. This means that additional (temporary) entries that have been stored in the global blackboard get removed. See Also Blackboards Blackboard Template Asset Local Blackboard Component Animation Graph"
  },
  "pages/docs/Miscellaneous/imagedata-asset.html": {
    "href": "pages/docs/Miscellaneous/imagedata-asset.html",
    "title": "ImageData Asset | ezEngine",
    "keywords": "ImageData Asset The image data asset references an image file (png, jpg, etc). However, contrary to a texture, an image data resource is not used for rendering, but instead is available to be sampled on the CPU. That means, it simply represents two-dimensional data with 4 floating point channels. For example the heightfield component uses an image data asset to determine the height of each vertex by reading it from an image file. See Also Textures Heightfield Component"
  },
  "pages/docs/Miscellaneous/local-blackboard-component.html": {
    "href": "pages/docs/Miscellaneous/local-blackboard-component.html",
    "title": "Local Blackboard Component | ezEngine",
    "keywords": "Local Blackboard Component The local blackboard component contains a blackboard. The component itself doesn't have any notable functionality. Instead it is used as the central storage, through which other systems can share their data and communicate. If a system requires a blackboard, it will typically search for a blackboard component by traversing its own object structure upwards (in C++ you can use ezBlackboardComponent::FindBlackboard() to do so). For example the animation graph is controlled by modifying blackboard entries, which the controller reads. You can modify the entries through custom code. For this, the blackboard component has to be attached either to the same object, or a parent object. Component Properties Template: The blackboard template to use for configuring the blackboard. This is often much more convenient, than to add a all the entries one by one on each component. It also makes changing a configuration later easier. BlackboardName: The name for the blackboard. This could be used to search for a specific blackboard, if multiple are available in the same hierarchy. ShowDebugInfo: If enabled, the component will draw a debug text overlay with the current entries and their values at the position of the game object. SendEntryChangedMessage: If enabled, all changes to blackboard entries will be broadcast as an event message of type ezMsgBlackboardEntryChanged. This allows other systems to react to every change, but also has a small performance cost. Entries: Entries that will be added at start with their initial values. Some systems will add their own entries, others expect an entry to already exist. For example the input component may write input state into a blackboard, but it will only do so for entries that already exist. Therefore, you need to add all entries that you want to receive from the input system here. See Also Blackboards Blackboard Template Asset Global Blackboard Component Animation Graph"
  },
  "pages/docs/Shipping/project-export.html": {
    "href": "pages/docs/Shipping/project-export.html",
    "title": "Project Export | ezEngine",
    "keywords": "Project Export Project Export is the step to create a package that contains all the files needed to play your game, excluding the files that are only needed for development. The data directories of your project contain a lot of files that are only needed during development. Additionally, they are usually stored in various locations, for example the Base directory, which is needed by all games, is located in the SDK folder, whereas your project files probably are located somewhere entirely different. Finally, you also need various binaries (EXE and DLL on Windows) to launch your game, which are again located somewhere different. The project export feature consolidates all these files into a single directory, so that it is easy to distribute. Export Project Dialog To get started with generating a self-contained package of your game, use the project export dialog that you find in the editor under Project > Export Project.... Select an output folder and click Export Project. Once the export is finished, it will automatically open the folder where the files have been copied to. The export also generates one .bat file for every scene in your project. These scripts launch the respective scene with ezPlayer. Configuring Project Export When you export your project for the first time, these files are added to your project directory: ProjectBinaries.ezExportFilter ProjectData.ezExportFilter The first file is used to determine which binaries should get included in the package. The second file is used to determine which data files should get included. By default these files #include predefined export filters: Data\\Base\\CommonData.ezExportFilter Data\\Base\\CommonBinaries.ezExportFilter These files set up the rules for typical use-cases. You can extend them in your project config files, or you can remove the #include and fully define your own rules. The ezExportFilter files contain two sections: [EXCLUDE] and [INCLUDE]. Without any filter, all files are included in the output package. To exclude certain files or file types, a pattern has to be added to the [EXCLUDE] section. However, sometimes you want to exclude all files of a certain type (e.g. exe) but include a single one regardless (for example your game.exe). In this case, add it to the [INCLUDE] section, to override the exclusion filter. Each line in the file represents one file path pattern: If it starts with a *, it matches paths that end with this pattern. If it ends with a *, it matches paths that start with this pattern. If it starts and ends with a *, it matches paths that contain this pattern. At any other location, * is not allowed. All paths are considered to be relative to their respective data directory. Note: For inspiration how to use these path patterns, see the files Data\\Base\\CommonData.ezExportFilter and Data\\Base\\CommonBinaries.ezExportFilter. Limitations At this time it is not analyzed which plugin DLLs are actually needed, instead all DLLs are included. Edit your ProjectBinaries.ezExportFilter to control this. Currently the export step always creates .bat files to load each scene with ezPlayer. There is no way to automatically set up something different. You currently can't automatically execute custom logic (C++ code, or a script) to finalize the package. See Also Profiling Supported Platforms ezPlayer"
  },
  "pages/docs/ai/AiPlugin/ai-plugin-overview.html": {
    "href": "pages/docs/ai/AiPlugin/ai-plugin-overview.html",
    "title": "AiPlugin Overview | ezEngine",
    "keywords": "AiPlugin Overview The AiPlugin is an optional engine plugin that provides functionality for doing typical game AI tasks. To enable the plugin, use the plugin selection dialog and enable the AiPlugin. Some functionality will show up in the form of components, other functionality may only be available through C++ code. To get access to the C++ functionality, your code needs to additionally link against the AiPlugin library. For example the Monster Attack Sample does so through its CMakeLists.txt file. Navigation The plugin provides functionality to create navmeshes on-demand at runtime. See this chapter for details. Additionally there is C++ functionality available for searching paths and steering characters along the found path. See the Monster Attack Sample, specifically the monster component, to see how this can be used. See Also Runtime Navmesh"
  },
  "pages/docs/ai/AiPlugin/navmesh-path-test-component.html": {
    "href": "pages/docs/ai/AiPlugin/navmesh-path-test-component.html",
    "title": "Navmesh Path Test Component | ezEngine",
    "keywords": "Navmesh Path Test Component This component is for testing the runtime navmesh. Place an object with this component in your scene, then specify another object as the PathEnd. Make sure you have a navmesh config and a path search config set up and chosen for this component to use. Path searches are only possible while simulating a scene, so press the play button to test it. Now the plugin will automatically generate the navmesh and display the found path. If it doesn't show anything, check the Visualize options. VisualizePathState can help figuring out what went wrong. For instance the start and end points might not be in a location where the navmesh is reachable, at all, or they might be too high above the ground. Also use the navmesh visualization functionality to make sure any mesh was generated successfully, at all. Component Properties VisualizePathCorridor: If enabled, the polygons that form the corridor of the path search result are visualized. VisualizePathLine: If enabled, the shortest line through the corridor is visualized. VisualizePathState: If enabled, the current state of the path search is printed as text at the location of this object. PathEnd: A references to another object that acts as the path's destination. NavmeshConfig: Which navmesh type to do the search on. PathSearchConfig: Which path search type to use for the path search. See Also Runtime Navmesh AiPlugin Overview"
  },
  "pages/docs/ai/AiPlugin/runtime-navmesh.html": {
    "href": "pages/docs/ai/AiPlugin/runtime-navmesh.html",
    "title": "Runtime Navmesh | ezEngine",
    "keywords": "Runtime Navmesh The AiPlugin adds functionality to generate navmeshes on-demand at runtime. This is a very convenient workflow, as the navmesh incorporates level changes every time you restart the simulation. Input Data The runtime navmesh generation uses the collision geometry and its surfaces as the input data from which to build the navmesh. The data is queried on-demand in an area around where path searches are done. To see the resulting navmesh, enable the navmesh visualization. Navigation Configuration In a scene document select Project > Plugin Settings > Ai Project Settings to open the configuration dialog. Ground Types The Ground Types tab is used to define the types of walkable (or non-walkable) ground that appear in your game. There is a maximum of 32 ground types. In this dialog you can edit their names (double click or F2 on an entry) and select whether an entry should generally be in use (checkmark). Entries that are not checked, will not show up in other places as selectable. The <None> and <Default> ground types always exist and cannot be removed or renamed. Which ground type a physical surface represents, is specified in the surfaces. If a surface uses the <None> ground type, it will not be part of the navmesh, at all, meaning the navmesh will have a hole there. For surfaces that are generally not walkable, by no character, this is preferable over using a dedicated ground type that is then ignored by a path search type. Navmesh Types The Navmesh tab is used to configure how a navmesh is generated. Most games only need a single navmesh type, but if you have agents of very different height or radius, you may need additional ones. Enable the navmesh visualization and utilize the the navmesh path test component to test how these options affect the result. Navmesh Properties Collision Layer: The collision layer to use for finding geometry from which to generate the navmesh. This is mainly used to filter out high detail geometry, to speed up navmesh generation. It can also be used to ignore ground types that are generally not traversable (such as water). Cell Size (XY): How detailed the navmesh generation shall be. This has to be less than the desired character radius, and the agent radius should be a whole multiple of this value. For example if your cell size is 0.2, the effective agent radius can only be 0.2, 0.4, 0.6 and so on. Cell Height (Z): The detail along the up axis with which to generate the navmesh. This mainly affects how well steps are detected and dealt with. Similar to the agent radius, the agent step height should be a multiple of the cell height. Agent Radius: How wide the characters are that walk over this type of navmesh. This affects how narrow passages can be, and how far away agents stay from walls. The agent radius should be a multiple of the cell size. Don't use multiple navmeshes only because characters shall have a slightly different radius. Use the same radius for all agents that are more or less the same size. Only use a separate navmesh type, for characters that are extremely different in size and should be forced to take different paths throughout narrow terrain. Agent Height: Similar to agent radius, this defines how tall a character may be. Should be a multiple of cell height. The same caveats as for agent radius apply. Agent Step Height: The maximum height of obstacles (such as stair steps) that an agent is allowed to step up. Should be a multiple of cell height. Agent Walkable Slope: The maximum slope of triangles that are considered for navmesh generation. Triangles steeper than this will be discarded as non-walkable. Path Search Types Each navmesh defines where an agent can walk, and which ground types are in which area. Path search types are used to configure how fast an agent can traverse each ground type and thus which areas the agent would prefer or avoid. Additionally a path search type can be used to prevent an agent from walking on certain ground types, at all. In this tab you can add different path search types, and for each one, you can configure the cost of traversing a ground type and whether it is allowed to be traversed at all. The higher the cost, the more an agent will try to avoid it and use a different route if possible. Traversal costs can't be zero or negative. Use the the navmesh path test component to see how these options affect path finding. Navmesh Visualization To be able to see the navmesh, use the CVar Ai.Navmesh.Visualize. There can be multiple different navmeshes for different character heights etc. This CVar enables visualization for one of them by index. So set it to 0 to see the first navmesh, 1 if you have a second one and so on. Important: No navmesh will be generated, as long as no path searches are done. Use the navmesh path test component to initiate a path search. See Also AiPlugin Overview Navmesh Path Test Component"
  },
  "pages/docs/ai/RecastPlugin/recast-navmesh-component.html": {
    "href": "pages/docs/ai/RecastPlugin/recast-navmesh-component.html",
    "title": "Recast Navmesh Component | ezEngine",
    "keywords": "Recast Navmesh Component The Recast Navmesh component is used to add a navigation mesh to a scene. See the chapter about the Recast Navmesh for details. Component Properties ShowNavMesh: If enabled, a debug visualization of the navmesh is rendered. Note: The same can be achieved through the CVar ai_ShowNavMesh which can be dynamically toggled at runtime. AgentHeight, AgentRadius: The height and radius of the average agent for which the navmesh will be generated. AgentClimbHeight: How high obstacles can be before they prevent the agent from stepping over. WalkableSlope: The slope angle that the agent is able to walk up. CellSize, CellHeight: The size of the cells used for voxelizing the level geometry, from which the navmesh is computed. Smaller cell sizes are better for correctness and precision, but also cost much longer to compute. Note that the cell height must be small enough to properly detect obstacle heights, otherwise the AgentClimbHeight won't work as expected. If necessary, prefer a finer grained cell height over a smaller cell size. MinRegionSize: The minimum area (in square meters) for every navmesh section. Islands that are smaller than this will be discarded. For all the remaining options refer to the Recast documentation. See Also Recast Navmesh Recast Integration"
  },
  "pages/docs/ai/RecastPlugin/recast-navmesh.html": {
    "href": "pages/docs/ai/RecastPlugin/recast-navmesh.html",
    "title": "Recast Navmesh | ezEngine",
    "keywords": "Recast Navmesh Navmeshes are special meshes that define which areas of a scene are traversable for an AI agent. Though they can also be used to restrict player movement, for example in games where falling down a ledge should not be possible. To add a navmesh to a scene, place a navmesh component to it. Navmeshes are unique to each scene, therefore they are not treated as reusable assets. Instead, each navmesh component generates and holds the data directly. Since generating a navmesh can take a long time, this process is exposed as an editor background operation. To update a navmesh, open the background operations panel and start the corresponding operation. To visualize the navmesh, enable the ShowNavMesh option on the navmesh component. Navmesh Generation Options The options that affect navmesh generation are available through the navmesh component. Using the Navmesh Currently the navmesh functionality is only usable directly through custom C++ code. The AI components that are available are not in a state where they are usable, but their code can be used for inspiration. You will need to write your own code and use the Recast/Detour functionality directly to implement agent steering or other features. The navmesh component registers its navmesh in the ezRecastWorldModule. You can then retrieve the scene's navmesh through ezRecastWorldModule::GetNavMeshResource(). From their on you can use Recast to plan a path, check whether a location is walkable, and so on. Be aware that the Recast integration is very basic. Any help improving it would be highly appreciated. See Also Recast Navmesh Component Recast Integration"
  },
  "pages/docs/ai/RecastPlugin/recast.html": {
    "href": "pages/docs/ai/RecastPlugin/recast.html",
    "title": "Recast Integration | ezEngine",
    "keywords": "Recast Integration IMPORTANT: This plugin has been superseeded by the AiPlugin and will most likely not be developed further or even removed. If you still have use for it, let us know. For now the code is still available. The Recast integration is functional, but not yet ready to be used productively. Help improving it is very much appreciated. Note: You have to enable ShowInDevelopmentFeatures in the editor settings for this component to show up. See Also Recast Navmesh Recast Navmesh Component"
  },
  "pages/docs/ai/sensor-components.html": {
    "href": "pages/docs/ai/sensor-components.html",
    "title": "Sensor Components | ezEngine",
    "keywords": "Sensor Components Sensor components are used to detect objects in a certain area and inform other game code about them. Contrary to triggers, they use the spatial system, so they work even without a physics engine. However, the sensor component can utilize additional physics raycasts, to determine whether something inside the volume is also visible and not occluded by walls. Sensor components are meant for AI use cases, such as determining line of sight, hearing noises or even smelling odors. Generally the sensors query the spatial system to detect certain objects. Use the marker component to make something detectable. For example, to make a creature able to smell the player, regularly drop markers at the player's current location, that vanish after a while, so that the creature can detect and follow these markers. State Reporting Sensors keep track of the objects that entered their volume. During every update they determine whether objects are still inside the volume or whether their visibility changed (line-of-sight occluded). If anything changes, a sensor sends the message ezMsgSensorDetectedObjectsChanged, which contains the full array of currently detected objects. Performance Considerations Sensor components poll the world in regular intervals and thus incur a performance cost. The UpdateRate determines how often this polling happens. Internally updates from many sensors are automatically distributed evenly across frames, to prevent performance spikes at regular intervals. Still, it is best to reduce the update rate as much as possible. For example in a game with large levels, you should check how close the player is to an NPC and dynamically adjust the update rate. At a large distance, the sensor can be set to update only every second, or you could even deactivate the sensor entirely. Similarly, you can use the 'alterness' state of an NPC to increase or decrease the sensor update rate. It is also possible to set the update rate to Never which disables automatic updates. In this case you can explicitely instruct to sensor to do an update only on demand, via the C++ function RunSensorCheck(). This is for more advanced usage, for instance when writing custom AI code. Finally, you should decide whether doing a visibility check is always necessary. The sensor would do this check for every possible target at every update. However, for a lot of game logic, once something has attracted attention, further visibility checks are not necessary. In such a case, it can be more efficient to do visibility raycasts only while a creature is not yet alert. Component Properties Shared Component Properties UpdateRate: How often the sensor component should query the world for changes. The higher the update rate, the more responsive it will be, and the less likely that short events are missed. However, higher update rates also require more processing time. SpatialCategory: The spatial category of objects that should trigger the sensor component. TestVisibility: If enabled, the sensor will cast additional rays using the physics engine, to determine whether the target is occluded by walls or clearly visible. CollisionLayer: The collision layer to use for the visibility raycast. ShowDebugInfo: If enabled, additional debug geometry is rendered to visualize the sensor volume and state. Color: This color is used for the debug visualization. Can be used to easily distinguish what type of sensor this is. Sphere Sensor Component Radius: The size of the sensor sphere. Cylinder Sensor Component Radius, Height: The dimensions of the sensor cylinder. Cone Sensor Component NearDistance, FarDistance, Angle: These all define the cone volume. Note that the cone not only has an angle and a length (FarDistance) but also a NearDistance. Enable ShowDebugInfo to see the exact cone shape. The near distance allows to ignore things that are up close, or to have that range covered by another sensor shape. See Also Marker Component State Machine Component Custom Code Blackboards"
  },
  "pages/docs/animation/common/color-gradients.html": {
    "href": "pages/docs/animation/common/color-gradients.html",
    "title": "Color Gradients | ezEngine",
    "keywords": "Color Gradients Color gradients are used to color things using a 1D lookup. They are typically set up once and shared across many assets, such as particle effects. Modifying a color gradient will affect all resources that use the gradient. Therefore it is advisable to create common gradients early during development. The color gradient can be sampled along the X-axis. This is frequently used to sample a color over time, but other parameters may be used for the lookup as well. If a gradient is supposed to be used in a looping fashion, the first and last color have to be set up to match. Gradient Editor The image below shows a color gradient and its editing controls: The gradient is displayed in four different ways: The first row shows a black and white representation of the alpha channel. The second row shows the color values of the gradient multiplied with the alpha channel, to give an impression how the alpha channel may affect the color perception, when used for transparency. The third row uses the alpha channel to fade between the color values and the background checker pattern, to visualize the effect of the alpha channel in a different way. The fourth row only shows the color channel. Not all gradients use the alpha channel. For those, the first row will be entirely white and the second to fourth row will all appear identical. Editing Keyframes Above the alpha channel and below the color channel you will notice greyscale and colored circles. These are keyframes that define the value at that specific position. All values in between are interpolated. You can grab a keyframe and move it left or right. Double clicking a color keyframe will open a color picker to change its value. When you select a keyframe, its exact position and value can also be changed with the UI elements at the bottom. To create a keyframe double click either the color channel bar or the alpha channel bar (where the respective keyframes are shown as circles). A newly inserted keyframe always gets the interpolated value at that position. To delete a keyframe just select it and press the Del key. View You can scroll left and right using right click and drag. You can zoom in and out using the mouse wheel. The [ Frame ] button will zoom and scroll the view such that all keyframes are framed. Gradient Range Each keyframe has a position along the X axis. Most code that looks up a gradient doesn't actually care about the exact positions. Instead the gradient is looked up in a normalized way, meaning the leftmost keyframe is mapped to position 0 and the rightmost keyframe is mapped to position 1. For instance, particle effects do all of their lookups this way. However, it is good practice to author the gradients already within this range, unless you have a use case where you indeed want keyframes outside the [0; 1] range. To make this easier, you can use the Adjust X to [0 - 1] button. This will automatically map all existing keyframes to this range for you. The dotted lines indicate which keyframe is the leftmost and which is the rightmost. See Also Curves Property Animation (TODO)"
  },
  "pages/docs/animation/common/curves.html": {
    "href": "pages/docs/animation/common/curves.html",
    "title": "Curves | ezEngine",
    "keywords": "Curves Curves are used to animate properties. They are typically set up once and shared across many assets, such as particle effects. Modifying a curve will affect all resources that use it. Therefore it is advisable to create common curves early during development. The curve can be sampled along the X-axis. This is frequently used to sample a value over time, but other parameters may be used for the lookup as well. If a curve is supposed to be used in a looping fashion, the first and last control point have to be set up to match. Curve Editor To add a control point, double click where you want to insert one. View Right click and drag to pan the view. Use the mouse wheel to zoom in and out. Hold Shift or Ctrl to zoom the view only along the X axis or the Y axis. Press Ctrl + F or select Frame from the context menu to frame the view either on the selected control points, or the entire curve, when no point is selected. Selection Left click a point to select it. Press ESC to clear any selection. Drag a rectangle to select multiple points. Holding Shift always adds points to the selection. Holding Alt always removes points from the selection. Holding Ctrl toggles the selection of a control point. Distinguishing between add and remove is particularly useful when changing the selection by dragging a rectangle. Left click and drag selected points to move them around. Press Shift after you started dragging, to limit modifications to either left/right or up/down. The axis along which you moved the control points the most, before you pressed Shift, determines which axis gets limited. Drag the handles at the edges of the selection rect, to scale the selected points. Tangents Every control point has two tangents to determine the slope of the curve coming from the left into the point and going out to the right of the point. Each tangent can use one of four modes. Some modes are fully automatic, some allow you to edit the tangents. To not clutter the UI, tangents that can be edited are only shown for selected control points. To change the mode, select a control point, open the context menu and choose from Left/Right/Both Tangents > .... Auto: The default mode. Automatically configures the tangents to create a smooth curve. Bezier: Gives you full control over the tangents. Both slope and length of the tangents will affect the curve. Fixed Length: Although you can change both slope and length of the tangents, only the slope affects the curve. This is often easier to use than Bezier. Linear: This mode deactivates any curvature. Allows you to make hard corners. Link / Break Tangents By default adjusting one tangent at a control point, mirrors the change over to the other tangent. That's because the two tangents are linked. If you want the tangents to be independent of each other, you can break the link between the tangents. Use the context menu to do so. Looping Curves The rightmost control point determines the overall length of the curve. As you can see, the editor repeats the display of the curve at that point, overlayed with a grey pattern. This enables you to see how the curve would look when used in a looping fashion. To make a curve loop nicely, open the context menu and select Curve > Loop: Adjust First/Last Point. This will modify either the first or the last control point in the curve to match up with the respective other point. Make sure that the two control points use the same tangent mode. You may need to switch to Bezier or Fixed Length tangents to make the curvature match perfectly. Curve Presets In the context menu under Presets you will find a number of presets for commonly used curves. You can also save the current curve as a preset. You can save presets anywhere, but as long as you save it under the path Editor/Presets/Curves (in any data directory), it will automatically show up in the context menu. You can also use sub-folders to organize presets, as folders will introduce sub-menus. See Also Color Gradients Property Animation (TODO)"
  },
  "pages/docs/animation/paths/follow-path-component.html": {
    "href": "pages/docs/animation/paths/follow-path-component.html",
    "title": "Follow Path Component | ezEngine",
    "keywords": "Follow Path Component The follow path component is used to move an object along a path. See the path component for how to create the path shape. This component is mainly meant for mechanical movement, such as objects moving on a rail. Movement is with constant speed and strictly along the path. It can be used for moving effects (lights, particles) around, where the mechanical behavior may not be too obvious, but for things like camera paths or anything else that should have natural acceleration, it won't be sufficient. Component Properties Path: The object on which the path to use is attached. If none is provided, the parent objects are searched for an object that contains a path component. That means you can put an object into a prefab and then place it onto different paths by adding the prefab instance as a child of the path shape. StartDistance: At what distance along the path to start traversing it. If the Path reference is set, changing this value will properly preview where the object would start. Running: Whether the component is currently running or paused. This can be used at runtime from scripts, to start and stop traversal. It is also automatically set to false, when the end of the path is reached and no looping behavior is active. Mode: Whether to traverse the path in a loop or only once. Speed: How fast to move along the path. LookAhead: The component will rotate the object according to the path direction. For this it samples the path some distance ahead. The farther the look-ahead the earlier the object will rotate into upcoming curves. At a very low look-ahead, it will rotate very rigidly. Smoothing: With zero smoothing the position of the object will be exactly that of the path. With some smoothing, the position doesn't change as abruptly. For mechanical objects attached to a rail, this should be zero, for more organic movement, increase the value towards one. FollowMode: How the transform of the owner object gets modified: OnlyPosition: Only the position gets moved along the path. The orientation of the object stays unaffected. AlignUpZ: While moving along the path, the object rotates only around the up axis (Z) to look into the direction of the path. This is useful for moving platforms that should always stay perfectly flat. FullRotation: The object is both moved and oriented accoring to the path. TiltAmount, MaxTilt: If FollowMode is AlignUpZ, MaxTilt defines whether the object may tilt a little when turning. TiltAmount specifies how much tilting will occur. See Also Path Component Path Node Component"
  },
  "pages/docs/animation/paths/path-component.html": {
    "href": "pages/docs/animation/paths/path-component.html",
    "title": "Path Shape Component | ezEngine",
    "keywords": "Path Shape Component This component is used to set up path shapes. Other components, such as the follow path component can then use the path shape to animate objects or to build geometry. How to Configure a Path Shape Add a game object and attach a path component to it. Add several game objects as child objects (at least three) Attach a path node component to each child object. Give each child object a different name. It is easiest to just give them numbers in the order in which you want them to be used in the path. Add those names in the desired order to the Nodes property of the path component. Position the child objects in the world to form a path. Make sure the Visualize Path flag is enabled on the path component, so that you can see a preview of the shape. Additionally to the options on the path component itself, you can also edit each node's properties, to adjust the curvature, and whether the path should curl around itself. What to do with a Path Shape A path component by itself has no functionality other than to define a shape. Currently these component types can utilize path shapes: Follow Path Component Component Properties Flags: Preview flags for the path shape. The visualize path flag is enabled by default, once you are done setting up the path, you should disable it. Closed: Whether the path should loop in on itself or not. Detail: How detailed the path will be. A finer detail (low value) means there are fewer sharp turns along the curves, but also takes up more memory and time to compute. Unless you notice obvious problems with the path tesselation, this value should stay at its default value. Nodes: This array needs to reference all nodes by name. If a non-existing name is used, it is ignored. See Also Path Node Component Follow Path Component"
  },
  "pages/docs/animation/paths/path-node-component.html": {
    "href": "pages/docs/animation/paths/path-node-component.html",
    "title": "Path Node Component | ezEngine",
    "keywords": "Path Node Component The path node component is used in conjunction with a path component to build a path shape. It must be attached to a child object of the object where a path component is present. The path node component has no runtime functionality and mainly exists for editing paths, as it provides a shape icon in the editor for node selection, and can be moved around with the standard editing tools. At runtime, the path component stores all the important data in a more optimized data structure. Important: Objects that only contain a path node component are removed from the scene during scene export. They have no use at runtime and are solely used for editing purposed. Thus in an exported scene you will neither find any path node components, nor the objects that they were attached to, unless you attach other components or child objects to them. Component Properties Roll: Sets the roll angle at this node, ie. how much the path curls around its own forward direction here. This doesn't affect the path shape itself, however, components that use the path, such as the follow path component, may use this information for rotation. Tangent1, Tangent2: The mode for the tangents of the incoming and outgoing path segments: Auto: The curvature at the node is automatically adjusted to be smooth. Linear: The path takes the shortest route in or out of the node. Used to create sharp corners without any curvature. See Also Path Component Follow Path Component"
  },
  "pages/docs/animation/property-animation/color-animation-component.html": {
    "href": "pages/docs/animation/property-animation/color-animation-component.html",
    "title": "Color Animation Component | ezEngine",
    "keywords": "Color Animation Component The color animation component allows you to apply an animated color gradient to other components, such as meshes or light sources or any other component type that handles the message type ezMsgSetColor. Important: This component has no effect on its own. It tries to change the color of other attached components. If no other component has a main color or doesn't handle the message ezMsgSetColor, there will be no effect. Note: If an attached component does handle the ezMsgSetColor, but doesn't properly update its color dynamically when combined with this component, it may not invalidate its cached render data correctly. A temporary work around is to set the game object's mode to Force Dynamic. Component Properties Gradient: The color gradient to use. The gradient will be sampled from left to right over Duration seconds. Each time the sampled color is put into an ezMsgSetColor and that message is sent to all other components that are attached to the same object. Duration: The duration that the color gradient should last before it is being repeated. SetColorMode: The mode with which to modify the color of the affect object. Although the color can be blended into the target object's color, for many components this quickly results in a fully black or fully white result, as the modifications accumulate with each change. AnimationMode: How to continue sampling the color gradient, once the end has been reached. RandomStartOffset: If enabled, the component starts with a random time offset. This way prevents synchronous playback, if multiple objects use the same animation. ApplyToChildren: Whether to send the ezMsgSetColor only to components on the same game object, or also to all components in the entire sub-graph. This can be used to modify the color of many objects in sync. See Also Property Animation (TODO) Color Gradients"
  },
  "pages/docs/animation/property-animation/move-to-component.html": {
    "href": "pages/docs/animation/property-animation/move-to-component.html",
    "title": "MoveTo Component | ezEngine",
    "keywords": "MoveTo Component A light-weight component that moves the owner object towards a single position. The functionality of this component can only be controlled through (script) code. The component is given a single point in global space. When it is set to running it will move the owning object towards this point. Optionally it may use acceleration or deceleration and a maximum speed to reach that point. Since the target position is given through code and can be modified at any time, this component can be used for moving objects to a point that is decided dynamically. For example an elevator can be moved to a specific height, depending on which floor was selected. Or an object could follow a character, by updating the target position regularly. The component sends the event ezMsgAnimationReachedEnd and resets its running state when it reaches the target position. Component Properties Running: Whether the component is currently moving the owner. Has to be set to true to start moving towards the target position. Setting this to false before the target was reached will interrupt the movement immediately. TranslationSpeed: The maximum speed at which to move towards the target. TranslationAcceleration: The acceleration to use to reach the translation speed. TranslationDeceleration: The deceleration to use to slow down when the target destination is being reached. Component Script Functions SetTargetPosition(pos): Call this to change the target position. Nothing will happen, though, unless Running is also set to true. See Also Follow Path Component Slider Component"
  },
  "pages/docs/animation/property-animation/property-animation-asset.html": {
    "href": "pages/docs/animation/property-animation/property-animation-asset.html",
    "title": "Property Animation Asset | ezEngine",
    "keywords": "Property Animation Asset Property animations are fully functional (except for some known limitations), but undocumented. See Also Property Animations (TODO) Property Animation Component (TODO)"
  },
  "pages/docs/animation/property-animation/property-animation-component.html": {
    "href": "pages/docs/animation/property-animation/property-animation-component.html",
    "title": "Property Animation Component | ezEngine",
    "keywords": "Property Animation Component Property animations are fully functional (except for some known limitations), but undocumented. See Also Property Animations (TODO) Property Animation Asset (TODO)"
  },
  "pages/docs/animation/property-animation/property-animation-overview.html": {
    "href": "pages/docs/animation/property-animation/property-animation-overview.html",
    "title": "Property Animations | ezEngine",
    "keywords": "Property Animations Property animations are fully functional (except for some known limitations), but undocumented. See Also Property Animation Asset (TODO) Property Animation Component (TODO)"
  },
  "pages/docs/animation/property-animation/reset-transform-component.html": {
    "href": "pages/docs/animation/property-animation/reset-transform-component.html",
    "title": "Reset Transform Component | ezEngine",
    "keywords": "Reset Transform Component This component sets the local transform of its owner to known values when the simulation starts. It is meant for use cases where an object may be activated and deactivated over and over. For example due to a state machine switching between different object states by (de-)activating a sub-tree of objects. Every time an object becomes active, it may want to start moving again from a fixed location. This component helps with that, by reseting the local transform of its owner to such a fixed location once. After that, it does nothing else, until it gets deactivated and reactivated again. Component Properties ResetPositionX, ResetPositionY, ResetPositionZ: Whether to reset the x, y and z component of the local position. LocalPosition: The local position value to reset the owner's transform to. ResetRotation: Whether to reset the local rotation. LocalRotation: The local rotation value to reset the owner's transform to. ResetScaling: Whether to reset the local scaling. LocalScaling, LocalUniformScaling: The uniform and non-uniform local scaling value to reset the owner's transform to. See Also Property Animations (TODO) Follow Path Component"
  },
  "pages/docs/animation/property-animation/rotor-component.html": {
    "href": "pages/docs/animation/property-animation/rotor-component.html",
    "title": "Rotor Component | ezEngine",
    "keywords": "Rotor Component The rotor component is a very useful utility component to rotate an object around an axis. This can be used for simple animations, such as having an itme in the world spin around itself. Video: How to make objects rotate and bounce Component Properties Speed: The maximum speed at which the rotor will turn. Running: Whether the rotor will move right from the start. If this is disabled, external code needs to switch the state to on, for the rotor to do anything. ReverseAtEnd, ReverseAtStart: Whether the rotor should automatically reverse its direction when it reaches the end, and when it reaches the start point again. If both are enabled, the rotor will turn back and forth indefinitely. Otherwise it will stop either at the end, or at the start. In both cases the Running state is reset to false, and the rotor can be restarted by setting the Running state to true again. Axis: The local axis around which the rotor should turn. AxisDeviation: A random deviation to apply to the Axis. This allows you to place multiple copies of the same object next to each other, and have them all rotate slightly differently. DegreesToRotate: How far to turn before the rotor needs to stop or reverse direction. This is not limited to 360 degrees, it may represent multiple revolutions. Use 0, if the rotor should just rotate continuously in one direction. In that case acceleration and decelaration have no effect. Acceleration, Deceleration: How fast to speed up and slow down. Use zero if the rotor should reverse course instantly. See Also Slider Component Property Animation (TODO)"
  },
  "pages/docs/animation/property-animation/slider-component.html": {
    "href": "pages/docs/animation/property-animation/slider-component.html",
    "title": "Slider Component | ezEngine",
    "keywords": "Slider Component The slider component is a very useful utility component to move an object back and forth along a single axis. This can be used for simple animations, such as having a pickup item bounce up and down, or for simple buttons or sliding doors. Video: How to make objects rotate and bounce Component Properties Speed: The maximum speed at which the slider will move. Running: Whether the slider will move right from the start. If this is disabled, external code needs to switch the state to on, for the slider to do anything. ReverseAtEnd, ReverseAtStart: Whether the slider should automatically reverse its direction when it reaches the end, and when it reaches the start point again. If both are enabled, the slider will go back and forth indefinitely. Otherwise it will stop either at the end, or at the start point. In both cases the Running state is reset to false, and the slider can be restarted by setting the Running state to true again. Axis: The local axis along which the slider should move. Distance: The distance that the slider should travel before stopping or returning. Acceleration, Deceleration: How fast to speed up and slow down. Use zero if the slider should reverse course instantly. RandomStart: If set to zero, the slider will always start from the beginning. Otherwise a random time offset between zero and RandomStart is used to pre-simulate the slider position. This is useful if you have multiple moving objects next to each other and you don't want them all to move in perfect unison. If you just want any random start position, just pick a very large RandomStart value. Scripting SetDirectionForwards: Allows to force the direction of the slider to either forwards or backwards along its axis. IsDirectionForwards: Queries whether the slider is currently moving forwards or backwards. ToggleDirection: Toggles the current direction of the slider. See Also Rotor Component Property Animation (TODO) MoveTo Component"
  },
  "pages/docs/animation/skeletal-animation/animated-mesh-asset.html": {
    "href": "pages/docs/animation/skeletal-animation/animated-mesh-asset.html",
    "title": "Animated Mesh Asset | ezEngine",
    "keywords": "Animated Mesh Asset The animated mesh asset is very similar to the mesh asset. However, it adds the necessary data to a mesh such that it can be used for skeletal animation. Animated meshes are placed in a scene with a dedicated animated mesh component. Which animations are played on it can be controlled with a simple animation component or an animation controller component. Video: How to import an animated mesh Asset Properties MeshFile: The file that contains the mesh data. For animated meshes prefer to use GLB (binary GLTF) files. FBX files can be used as well, though due to FBX's complexity chances are higher that it won't work as expected. The referenced file must contain the mesh data with skinning information. It doesn't need to contain any animation clips. DefaultSkeleton: The skeleton asset that is used to skin the animated mesh by default. RecalculateNormals, RecalculateTangents: See the mesh asset properties. NormalPrecision, TexCoordPrecision: See the mesh asset properties. BoneWeightPrecision: How precisely to store the bone weights. For highly detailed regions, such as character faces, it may be necessary to increase the precision, to prevent artifacts. NormalizeWeights: Usually all bone weights should add up to 1 on each vertex. To enforce this, bone weights are usually normalized. However, some meshes violate this rule and normalizing the weights introduces artifacts. Only disable this, option, if it very obviously fixes artifacts. ImportMaterials: See the mesh asset properties. Materials: See the mesh asset properties. See Also Animated Mesh Component Skeleton Asset Skeletal Animations Animation Clip Asset Simple Animation Component"
  },
  "pages/docs/animation/skeletal-animation/animated-mesh-component.html": {
    "href": "pages/docs/animation/skeletal-animation/animated-mesh-component.html",
    "title": "Animated Mesh Component | ezEngine",
    "keywords": "Animated Mesh Component An animated mesh component is used to instantiate an animated mesh asset. Animated mesh components are mostly identical to regular mesh components except that they can only be used with animated mesh assets. An animated mesh will be skinned with its current animation pose. Which pose is applied to an animated mesh can be controlled with a simple animation component or an animation controller component. Component Properties Mesh: The animate mesh asset to render. Color: See mesh component. Materials: See mesh component. See Also Meshes Skeletal Animations Simple Animation Component Animated Mesh Asset"
  },
  "pages/docs/animation/skeletal-animation/animation-clip-asset.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-clip-asset.html",
    "title": "Animation Clip Asset | ezEngine",
    "keywords": "Animation Clip Asset The animation clip asset is used to import a single animation for an animated mesh. An animation clip represents a single motion, such as a walk cycle, a jump or other action. Simple animations can be played on a mesh using a simple animation component. For complex behavior you will need to use multiple clips and fade from one to the other at the right times. Use an animation graph for that. Important: In animation graph nodes animation clips are not referenced directly but rather through a name mapping. This mapping is configured in the animation graph asset. Asset Properties File: The file from which to import the animation clip. UseAnimationClip: The (case sensitive) name of the animation clip to import from the file. Transform the asset once to populate the list of AvailableClips. Then type the name of the desired clip into this field and transform the asset again. If a clip doesn't show up in the list, make sure it is correctly exported. See the chapter Authoring and Exporting Animations with Blender for known issues. AvailableClips: After you transform the asset, this list will show all the animation clips that have been found in the given file. FirstFrame, NumFrames: It is best to put every animation into a separate clip and export them that way. However, sometimes files contain only a single animation and each clip is found at another interval. By specifying the index of the first frame and the number of frames to use, you can extract individual clips from such data. Note that setting NumFrames to zero always means to use all the remaining frames after the first frame. Note: It can be difficult to know the exact indices. Sometimes the data is authored at 24 frames per second and also exported that way, then you can plug in the numbers straight away. However, GLTF/GLB files are always exported at 1000 FPS. That means if your animation clip was authored with 24 FPS and starts at the one second mark, in the GLB file this wouldn't be at keyframe 24, but instead at keyframe 1000. PreviewMesh: The animated mesh to use for previewing this animation clip. This has to be set to see any preview. RootMotion: If the animation clip should be able to move the game object, this can be achieved through root motion. This option allows you to select how root motion should be incorporated into the animation clip. For the time being the only mode available is constant motion, which means that when this clip is played, the parent object will be moved at a constant speed into a single direction. This can be used for walking animations, but it might be tricky to avoid foot sliding. Playback The toolbar buttons allow you to play/pause/reset and slow-down the animation playback. Additionally you can use the time scrubber right below the 3D viewport to manually play the animation. It is best to pause the automatic playback then. Event Track Below the time scrubber there is an additional strip to edit animation events. Here you can add events that shall occur at specific times during the animation clip playback, such as foot-down or fire-weapon. Use the time scrubber above to play the clip and inspect at which time the event shall occur. Then right click into the event track and select Add Event. Which type of event will be added is specified with the combo box at the bottom right. See Also Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-events.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-events.html",
    "title": "Animation Events | ezEngine",
    "keywords": "Animation Events Animation events are markers that are placed in animation clip assets to indicate that something of interest happens at a certain time during playback. These events can be used for mundane things like foot down markers, in a walking animation, to vital gameplay information like shot fired in an attack animation. Whenver the EZ animation system plays back any animation clip, it also inspects the event track. For every event that it encounters, it broadcasts an ezMsgGenericEvent with Message set to the value of the event's name. If you have a TypeScript or Visual Script (or custom C++ components that is also an event handler) attached to any parent node of the animated mesh, you can handle this type of event and react with the desired game logic. In the video above, the clip contains a marker for when a shot is fired. A script reacts to this by spawning a projectile. In this example one could also just spawn the projectile at the same time at which the shoot animation is triggered, however, if the animation was longer, the exact time at which to do this, would be more difficult to estimate and using an event makes it much easier. The clip above shows the event track at the bottom of an animation clip asset. This is how you add event markers to a clip. See Also Skeletal Animations Event Nodes Custom Code Messaging"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-blackboard.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-blackboard.html",
    "title": "Blackboard Nodes | ezEngine",
    "keywords": "Blackboard Nodes The animation graph provides nodes to read and write values from and to a blackboard. For this, the game object on which the animation controller component is attached, or one of its parent nodes, also needs to hold a blackboard component. Note: If no blackboad is available, these nodes will typically silently not do anything. If a blackboard is available, but the desired entry is not (yet) in the blackboard, they may add the entry or log a warning and assume a default value. Set Number Node / Set Bool Node When activated, this node writes a given value to the blackboard. Node Properties Blackboard Entry: The name of the blackboard entry (variable) to write to. Number / Bool: The value to write in case the value input pin is connected. Input Pins Activate: When triggered, the node changes the blackboard value. Number / Bool: The value to write. If not connected, the value configured on the node is used. Get Number Node / Get Bool Node Outputs the value of a specific blackboard entry. Node Properties Blackboard Entry: The name of the blackboard entry (variable) to read. Output Pins Number / Bool: The value of the entry. If the entry doesn't exist, the pin outputs zero. Check Number Node This node monitors a blackboard value and compares it to a reference value. When the result of the comparison changes, the On True or On False output pin gets triggered for one frame. Node Properties Blackboard Entry: The name of the blackboard entry (variable) to monitor. Reference Value: A reference value for the comparison. Comparison: The way the two values get compared. Output Pins On True: Gets triggered for one frame when the comparison result changes to true. On False: Gets triggered for one frame when the comparison result changes to false. Is True: Outputs the result of the comparison. This is a data pin that can always be read, contrary to the other two pins that are event pins and only get triggered when something changes. Check Bool Node This node monitors a boolean blackboard value and compares it to true. When the result of the comparison changes, the On True or On False output pin gets triggered for one frame. Node Properties Blackboard Entry: The name of the blackboard entry (variable) to monitor. Output Pins On True: Gets triggered for one frame when the value changes to true. On False: Gets triggered for one frame when the value changes to false. Bool: Outputs the blackboard value. This is a data pin that can always be read, contrary to the other two pins that are event pins and only get triggered when the value changes. OnChanged Node This node monitors a blackboard value and triggers its output event node when the value changes. This should be used when any change to a variable is of interest. Node Properties Blackboard Entry: The name of the blackboard entry (variable) to monitor. Output Pins On Value Changed: Gets triggered for one frame when the value changes somehow. See Also Animation Graph Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-blendspace1d.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-blendspace1d.html",
    "title": "Sample Blendspace 1D Node | ezEngine",
    "keywords": "Sample Blendspace 1D Node The Sample Blendspace 1D node is used to linearly interpolate between a fixed set of animations. Every animation clip is assigned a position value in 1D space. The Lerp input pin value determines how to interpolate those clips. The output pose will be either exactly one of those clips, or a mix between two clips, but never more than that. So if one clip is placed at position 0 and another at position 1, you can fade from the first clip to the second by passing in a lerp value between 0 and 1. The length of each clip may be different, however, the lookup positions across all clips are synchronized. That means if two clips are being mixed, and the first clip is sampled right at its middle, then the second clip will also be sampled at its middle, even if this is a completely different time offset (say 1 second versus 1.5 seconds). At which speed to move the sample position forwards, is determined by the length of the two animation clips that the lerp value is closest to. This node is useful if you have an action that can be done at different speeds and you want to cover all possible speeds with just a few different animation clips. The most intuitive example is a walk/run motion. You only need two animation clips, one for slow walking and one for fast running, and this node allows you to generate any speed in between through interpolation. For this to work, all animation clips have to follow the rule that they do the same motion at the same relative time offset. So in the case of a walk/run motion, both clips have to start with the same foot forwards, then move the other foot and finally move the first foot again, such that the animation is looped. The clips can have different lengths, though, so the run clip might be shorter than the walk clip (and therefore faster). In the video above you can see such a transition in action. The lerp input value is varyied to demonstrate how the resulting interpolated animation looks. Here the node also has an idle and a walk backward clip, so it can interpolate between even more states. Node Properties Most node properties are the same as on the sample clip node. Clips: A list of animation clips between which this animation node will interpolate. The node will only ever sample the two clips whose Position values are closest the the value provided through the Lerp input pin. Additionally, the playback speed for each clip may be tweaked. Input Pins Most input pin properties are the same as on the sample clip node. Lerp: This value determines which animation clips get mixed together. If the lerp value is in between two Position values of two clips, the output pose will be the linear interpolation of those two clips. If the lerp value is lower than the lowest Position value or higher than the highest, the output will be exactly that animation clip (there will be no extrapolation). Output Pins Most output pin properties are the same as on the sample clip node. See Also Animation Graph Skeletal Animations Sample Clip Node Sample Blendspace 2D Node"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-blendspace2d.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-blendspace2d.html",
    "title": "Sample Blendspace 2D Node | ezEngine",
    "keywords": "Sample Blendspace 2D Node The Sample Blendspace 2D node is used to interpolate between a set of animations which are positioned in 2D space. You give it a 2D coordinate, and it will determine which animation clips are relevant and mix them together with proper weights depending on how close the coordinate is to each clip. The purpose of this node is to generate a continuous animation space from just a few discrete clips. This is often used for locomotion, where you only have animation clips for walking into a fixed number of directions and at certain speeds, but you'd like to be able to move a character into any direction and at any speed in between. This node can generally be used to combine animations that can be thought of as having a position on a 2D plane. For example if you have animations for aiming forwards, to the left, right, up and down, you can use the blendspace 2D node to generate any pose in between. Be aware that the poses will be combined linearly, though. If the poses from two clips are too different, the result may not look very good. In this case it is best to create additional clips with in-between poses. How To Use You add multiple animation clips and give each clip a position (X and Y). As with the blendspace 1D node, the playback of all clips is synchronized, meaning that the length of each clip may differ, but they will be played back such that they start and end in unison. That means your clips must be authored accordingly, so for example for locomotion all clips should start with the left foot forwards, then move the right foot forwards, then the left again. From that point on the clips will be looped. What the coordinates represent is up to you. For locomotion you could say that X represents left/right movement and Y forwards/backwards. You would then position a walk left clip at (-1, 0) a walk right clip at (+1, 0) a walk forward clip at (0, +1) and a run forward clip at (0, +2). Through the X and Y input pins you provide a 2D coordinate. During testing you may hook this up directly to an input node, though later you'll probably need more control. The node will then take that input coordinate to decide which clips should be used with what influence, and mix them together to a single output pose. Node Properties Most node properties are the same as on the sample clip node. Input Response: A time duration over which changes to the X and Y input values are applied. This prevents sudden extreme changes. For example when X and Y are connected to physical buttons, which are just turned on or off, the final animation would jerk between those extremes. In a finished game you may want to smooth out the input yourself, but for starters this node can do a basic smoothing of the input values for you. Thus, if an input value switches from 1 to 0, an Input Response of 50ms means that the used value will transition smoothly towards 0 over that amount of time and thus the output pose will also transition smoothly. Center Clip: An optional clip for the position (0, 0). This clip is always played at its own speed and not synchronized to the other clips. It is meant for idle state animations. It may be much longer and contain many subtle motions for variation. If such behavior is not desired and instead you want the center clip to be synchronized with the rest, you can instead place a clip at position (0, 0) as well. Clips: The various clips. Each clip must have a unique 2D position assigned. Input Pins Most input pin properties are the same as on the sample clip node. X, Y: The input coordinate to select how to blend the Clips. It directly relates to the clips` positions. Output Pins Most output pin properties are the same as on the sample clip node. See Also Animation Graph Skeletal Animations Sample Clip Node Sample Blendspace 1D Node"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-bone-weights.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-bone-weights.html",
    "title": "Bone Weight Nodes | ezEngine",
    "keywords": "Bone Weight Nodes Bone weight nodes are used to generate a weight mask. The mask defines how strongly an animation clip will influence different parts of the skeleton. This is frequently used to apply an animation only to certain parts of a character, for example only the upper or lower body, or even only the left or right arm. For example it is common to play a walking animation only on the bones below the hip, whereas on the spine and upwards one would want to play an attack animation. Since animations are often authored for the entire skeleton, it is therefore necessary to mask out unwanted parts. Bone weights are often in the range of zero to one, with zero meaning that that bone is entirely unaffected by an animation and one means it is fully affected. However, for convenience, weights above one are allowed as well. The system simply normalizes the weights on every bone at the very end. This way, if one animation affects a bone with a weight of one, and another animation affects the same bone with a weight of nine, the first one will only have 10% influence and the second has 90% influence. That makes it easier to layer an important animation on top of a base animation. By simply setting a very large weight (10 or more) an animation can easily override a part of the body, without having to use an inverse mask to filter out the base animation. Important: Not all animations will work correctly when they are layered on top of each other. If one animation rotates a bone into one direction, and another animation rotates the same bone very differently, it is possible for the interpolation of the rotations to result in an invalid value. This will manifest as jerking or jumping bones at specific points in the animation. If that happens, you have to use an inverse bone mask to fully filter out the base animation, such that in the end only one of the animations really influences those bones. To use a bone weight mask, connect it to an output node. Bone Weights Node The Bone Weights node creates a mask for every bone in the skeleton. By default, the mask is zero for every bone. You then add bones by name to the RootBones array. Every bone that is reachable from any of the root bones, will get a weight of one. You can specify multiple root bones, in case that an animation should for example affect both arms, but not the spine and head. Node Properties Weight: The overall weight for the mask. A higher weight means that animation clips that use this weight mask will have stronger influence on the final pose. RootBones: An array of bone names from where the weight mask should be set to one. Typically this only holds a single entry, for example the hip bone (to affect both legs) or a spine or shoulder bone (to affect the arms and head). Output Pins Weights: This represents the full bone mask and can be passed to other nodes, to make them only affect the desired bones. InverseWeights: If this pin is connected, the node generates the inverse mask as well. So for example, if the node would generate a mask that only affects the head, then the inverse mask will affect everything but the head. Bone Weights Switch Node This node is used to switch between different bone weight masks. For example a walk animation should be played on the whole body, but once an upper body animation becomes active, the walk animation should only be played on the lower body. Node Properties Weights Count: How many weight input pins the node should have. Input Pins Index: Which input weight to select as the output. Weights[]: Array of input weights. Output Pins Weights: The output weight. See Also Animation Graph Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-debug.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-debug.html",
    "title": "Debug Nodes | ezEngine",
    "keywords": "Debug Nodes These animation graph nodes are used to find problems. Log Nodes The Log Info and Log Error nodes print a string to the log whenever they get activated. Node Properties Text: The text to print. This may include placeholders for the input values. Use {0}, {1}, {2}, etc to embed the value from the respective Numbers[] pin. Number Count: Specifies how many number input pins the node should have. Input Pins Activate: Every frame in which this pin gets triggered, the node will log Text to the log. Numbers[]: These pins allow you to pass in number values for embedding in the output text. See Also Animation Graph Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-events.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-events.html",
    "title": "Event Nodes | ezEngine",
    "keywords": "Event Nodes Event nodes are used to broadcast event messages on the game object on which the animation graph is running. This allows other code to react at the right moment to things like an animation being finished. Event nodes allow you to broadcast custom events under exactly defined conditions. Additionally, every time an animation clip is played, and actively contributes to the final pose, events that are defined on that clip will automatically be broadcast on the associated game object. Note that the animation graph itself cannot react to events. For that purpose use custom code. Send Event Node When this node is triggered, it broadcasts an ezMsgGenericEvent with Message set to the value of Event Name. Node Properties Event Name: The string that is used as the Message property of the ezMsgGenericEvent that is broadcast. Input Pins Activate: When this pin gets triggered, the message is sent. See Also Animation Graph Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-input.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-input.html",
    "title": "Input Nodes | ezEngine",
    "keywords": "Input Nodes Input nodes expose the state of input devices to the animation graph. Input nodes are mainly provided for convenience during prototyping, as they may circumvent key mappings and general game state (e.g. whether the player is even allowed to move a character at all, at the moment). For a proper game, it is better to use an input component to forward input state to custom code and then decide there which animation shall get played. Then you can forward that state to the animation graph, through a blackboard. The animation graph itself would retrieve what it should do through the blackboard nodes. Controller Node This node reads the raw state of the connected controller 1. It outputs the button states as data pins. This node completely ignores any kind of button mapping. It is purely meant for prototyping scenarios, where it can be very convenient. Output Pins This node has number output pins for the sticks and triggers and bool output pins for the buttons. See Also Animation Graph Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-logic-math.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-logic-math.html",
    "title": "Logic and Math Nodes | ezEngine",
    "keywords": "Logic and Math Nodes The animation graph provides nodes to evaluate basic arithmatic and logic. This is meant for very simple use cases and for quick prototyping. Often animation logic requires much more complex rules than what would be feasible to express in the animation graph. Instead use custom code to decide which animation should run under which circumstances, and pass the result to the animation graph via a blackboard. The animation graph can then simply read the state for each animation using the blackboard nodes. AND Node This node checks whether all boolean input values are true and sets its own Is True and Is False output pins accordingly. Node Properties Bool Count: How many input pins the node should have. OR Node This node checks whether any boolean input value is true and sets its own Is True and Is False output pins accordingly. Node Properties Bool Count: How many input pins the node should have. NOT Node Outputs the oppositve value of its input. Compare Number Node This node can be used to check whether a number value compares in a certain way against a reference value. For example whether some input value is larger than 0.5. Node Properties Reference Value: The reference value to compare the incoming value against. Comparison: The mathematical operation with which to compare the two values. Input Pins Number: The number to compare against the reference value. Reference: If connected, this acts as the reference value. Output Pins Is True: A boolean output for the result of the comparison. Bool To Number Node Converts a boolean value (true or false) to a number. By default this node converts false to 0 and true to 1, but you can select other numerical values to use. Node Properties False Value, True Value: The number value to output for when the input is false or true. Event AND Node Used to filter an event pin to only activate the next node, if additionally to the event another condition is met at the same time. Event pins are triggered only infrequently. For example a button press may result in an event pin being triggered. However, you often don't want that event to immediately have an effect, because often the currently playing animations don't allow this. So you may have additional state keeping track of whether some reaction would be possible. The Event AND node makes it easy to listen to an event pin and forward the event only if another condition is also true. Input Pins Activate: An event pin that will trigger the evaluation. Bool: A condition that must be true for the output pin to get triggered. As long as this is false all activations are ignored. Output Pins On Activated: If both the Activate and Bool input pins are true at the same time, this pin gets activated for one frame. Expression Node The expression node takes up to four different numbers as its input, plugs them into a user provided expression and outputs the result. The expression must be syntactically correct, otherwise the node prints an error to the log. Node Properties Expression: The expression can use the following: Numbers in floating point format (e.g. 1, 2.3, -78) +, -, *, /, % (modulo) Parenthesis ( and ) to specify precedence The variables a, b, c and d representing the input pin values The functions abs and sqrt Examples: a * 0.5 - b abs(a) + abs(b) (a + 1) % 2 Input Pins a, b, c and d: Input values to the mathematical expression. Unconnected pins are treated as having the value zero. Output Pins Result: Outputs the result of the evaluated expression. See Also Animation Graph Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-output.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-output.html",
    "title": "Output Nodes | ezEngine",
    "keywords": "Output Nodes Every animation graph must have at least one output node. All nodes that shall affect the result must ultimately be connected to an output node. The pose result nodes do not define the final result, though. Rather the final result is a mix of all poses that reach a pose result node. This makes it easier to set up many different animations and also split work up into several animation graphs. You can quickly deactivate an entire part of the graph by removing the connection to the output node. Nodes that are not connected to the output are not evaluated at runtime and therefore don't cost any performance. Pose Result Node The pose result node is used to output one animation result. A more complex animation graph will contain many pose result nodes. All poses from active pose result nodes (the ones that have a non-zero Target Weight) will be combined according to their weight. Usually you would set up the graph such, that all actions that could overlap flow into their own pose result nodes. For example you may have one sub graph that is responsible for walk animations and another one for jump animations. When jumping, the system should fade over from walking into jumping and thus there is a short duration of overlap. To achieve this, both sub graphs would have their own pose result node and when the jump starts, the jump sub graph fades its output in (by setting its Target Weight to 1) and the walk sub graph fades its pose out respectively. The animation system takes care of combining the output poses accoring to their current weight. Now consider that you may have two different jump animations, one for when the character is standing and one while it is walking. These cases are mutually exclusive, and thus there is no need for separate output nodes for the two jump styles. In this case one would rather use one of the pose blending nodes to select the desired jump animation and feed it into one output node. However, you could use different pose result nodes, regardless, if it is easier to set up the animation this way, there is no performance penalty for using additional nodes as long as the number of simultaneously active nodes is low. Node Properties Fade Duration: How long to fade the result pose in or out when the Target Weight changes. Input Pins Pose: The pose to combine with all other active pose results. Target Weight: How strongly to apply the pose to the output. This should be used to signal whether the output is active, at all. Pass in 0 once an animation should be deactivated. Pass in 1 while it is active. The Fade Duration is used to ramp up or down the overall weight. If this pin is not connected, the node is considered to be always active and thus always combined with the other outputs, which may result in undesirable behavior, if this sub-graph sometimes doesn't actually provide a real pose. Fade Duration: When connected, overrides the Fade Duration property. Weights: The bone weights to use to apply this pose only to a subset of bones. If the pin is not connected, the pose applies to the full skeleton. Output Pins On Faded Out: This event pin is triggered when the output node has finished fading out. On Faded In: This event pin is triggered when the output node has finished fading in. Current Weight: A data pin that outputs the currently used target weight, which changes over the fade duration. Careful! Using the output pins it is easily possible to build a circular graph, which is not allowed. Make sure that the output of this node is not fed into the input of itself. Use blackboard nodes or event nodes to forward information and, if necessary, feed it back as input during the next graph update. Root Rotation Node This node is used to add angular root motion to the final pose. It enables the animation to change the rotation of the game object on which it is played. This is mainly convenient for simpler use cases and for prototyping. In more complex scenarios you may prefer to control the object's orientation with custom code. Input Pins Rotate X/Y/Z: The angular rotation to add on the selected axis. See Also Animation Graph Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-pose-blending.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-pose-blending.html",
    "title": "Pose Blending Nodes | ezEngine",
    "keywords": "Pose Blending Nodes An animation graph typically samples more than one animation. Sometimes these animations should be combined and sometimes you just wish to smoothly switch from one animation to another. Lerp Poses Node The lerp poses node linearly interpolates between two poses. It is typically used to fade over from one pose to another and potentially to a third. Note that the node only controls the mix of the two animation poses, it is not used for synchronizing playback across clips. See the Sample Blendspace 1D Node for that. Node Properties Lerp: The linear interpolation value that determines which clips get combined. For values between 0 and 1 the input poses 0 and 1 get combined. For example at 0.5 the two poses get combined half and half, whereas at 0.9 clip 0 is used with 10% strength and clip 1 is used with 90% strength. For lerp values between 1 and 2 the poses 1 and 2 get combined, for example at 1.5 the two are combined half and half. Poses Count: How many input pose pins to show. Input Pins Lerp: A pin to dynamically pass in the linear interpolation value. See the node properties above. Poses[]: The array of input poses. Output Pins Pose: The combined output pose. Pose Switch Node The Pose Switch Node is used to quickly but smoothly transition from one pose to another. The incoming poses are typically different animations. Note that the pose switch has no influence over the playback position of an incoming pose, so you may need to make sure that the target animation to switch to, gets restarted from the beginning. Node Properties Transition Duration: The time to take for transitioning from the previously active pose to the now active pose. Poses Count: How many input pose pins to show. Input Pins Index: Which input pose to use. If the index changes, the node will transition from the previously active pose to the new pose using linear blending. The index may switch from any value to any other value in range, it doesn't need to change only up or down. Poses[]: The array of available poses. Output Pins Pose: The combined output pose. See Also Animation Graphs Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-pose-generation.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-pose-generation.html",
    "title": "Pose Generation Nodes | ezEngine",
    "keywords": "Pose Generation Nodes These nodes are responsible for generating poses. They may create them procedurally, by sampling animation clips or a combination of the two. Rest Pose Node This node always outputs the rest pose of a skeleton. It has no configuration options or input pins. It is mainly used to provide a fallback pose when nothing else is available, especially during testing. Sample Blendspace 1D Node This node uses a 1D coordinate to select two clips for synchronized sampling and blending between them. See Sample Blendspace 1D Node for details. Sample Blendspace 2D Node This node uses a 2D coordinate to select from a range of clips which ones to sample in a synchronized fashion and blend between them. See Sample Blendspace 2D Node for details. Sample Clip Node This node plays back a single animation clip. See Sample Clip Node for details. Sample Clip Sequence Node This node plays back multiple clips in sequence, optionally using one clip for start (fade in), one or multiple clips for the middle section (which may loop) and optionally one clip for the end (fade out). See Sample Sequence Node for details. Sample Frame Node This node samples an animation clip at a specific, constant time and outputs that pose. There is no automatic playback. You can pass in the sample position to dynamically select which frame to sample, but often this is used to just sample the first or last frame of an animation clip to hold a specific pose. Node Properties Clip: The animation clip to sample. Norm Pos: The normalized sample position. This is a value between 0 and 1 where zero maps to the start of the clip, 0.5 to the exact middle and one to the very last frame of the clip. Input Pins Norm Pos: If connected, overrides the Norm Pos property. Abs Pos: If connected, overrides the Norm Pos value with an absolute playback position, which means you can pass in the actual time value where to sample a clip. Note that you could use this for proper playback of a clip, but it lacks features such as getting informed when the clip finishes, animation events and so on. Prefer to use a Sample Clip node instead. Output Pins Pose: The pose of the animation clip at the given position. See Also Animation Graphs Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-sample-clip.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-sample-clip.html",
    "title": "Sample Clip Node | ezEngine",
    "keywords": "Sample Clip Node The sample clip node is the most basic node to play an animation. It is used for typical playback of a single animation either once or in a loop. Node Properties Loop: Whether to play the animation just once from start to finish, or loop it endlessly. Playback Speed: Adjusts the speed with which the animation is sampled. Apply Root Motion: Whether root motion should be sampled and passed through. Clip: The animation clip to play. Input Pins Start: When this pin gets triggered, the node starts playback. If it is already playing, playback is reset to the start. If this pin is not connected playback starts right away, which is useful for nodes that play in an endless loop anyway. Loop: If connected, overrides the Loop property. When playback reaches the end and loop is enabled, it restarts, otherwise it stops playing and the On Finished output pin gets triggered. Speed: If connected, overrides the Playback Speed property. Output Pins Pose: The sampled pose. A valid pose is only produced during playback, once the node is inactive, there is no pose output. On Started: This output pin gets triggered every time playback is started or restarted, either because of user input or because playback reached the end and was looped. On Finished: This output pin gets triggered when playback reaches the end and looping is disabled. See Also Animation Graph Skeletal Animations Sample Sequence Node Sample Blendspace 2D Node"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-sample-sequence.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/anim-nodes-sample-sequence.html",
    "title": "Sample Sequence Node | ezEngine",
    "keywords": "Sample Sequence Node The sample sequence node plays multiple clips in a row. One clip is used to enter the animation sequence, then there can be one or multiple different clips that may be played in a loop, and once the loop is exited, another clip can be played to finish the sequence. Such sequences are common for actions such as jumping or climbing a ladder. The start clip transitions the character from a start state, such as idle or walking into the new state, such as jumping. The middle clip is then played as long as the jumping state needs to continue, and once the character hits the ground again, the end clip is played to transition back. The video above shows such a sequence. Here the node uses a point gun and a shoot gun clip for the middle part of the sequence, but it doesn't use a start or end clip at all (they are optional). Using the Clip Index input pin, the game code can switch at any time whether the gun is pointed or shot. One of the two clips is played in a loop as long as the game code decides to keep this state active. Here raising and lowering the arm is simply a result from fading the animation in and out over a short duration, but if desired these could also be dedicated animation clips. Node Properties Most node properties are the same as on the sample clip node. Start Clip: The animation clip to start with. This clip should end on a keyframe from where the Middle Clips can continue seemlessly. Middle Clips: One or multiple animation clips to play after the Start Clip. These get looped as long as the Loop property is enabled. If more than one clip is added, which one to play can be selected using the Clip Index pin. Otherwise a random one will be selected on every iteration. End Clip: The clip to play when the looped property is disabled after the start and middle clip are finished. Input Pins Most input pin properties are the same as on the sample clip node. Clip Index: This pin can be used to select which of the Middle Clips to play next. In the video above this is used to select whether the gun should get fired or not. Output Pins Most output pin properties are the same as on the sample clip node. On Middle Started: This event pin is triggered every time a middle clip starts playing. On End Started: This event pin is triggered when looping is disabled and the End Clip starts playing. See Also Animation Graph Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/animation-controller-component.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/animation-controller-component.html",
    "title": "Animation Controller Component | ezEngine",
    "keywords": "Animation Controller Component The animation controller component is used to apply complex animation playback and blending functionality to an animated mesh. It is the big brother of the simple animation component. Instead of playing just a single animation clip, it uses an animation graph asset to determine the animation pose. The component itself doesn't do much, other than updating the animation pose and sending it to the animated mesh. For how to control the animation playback, please see the Animation Graph chapter. Component Properties AnimGraph: The animation graph to use. RootMotionMode: Selects how root motion is applied to the owning game object. InvisibleUpdateRate: How often to update the animation when the object is not visible. For performance reasons the update rate should be very low or even paused when an object isn't visible. However, since animations may have an important impact on gameplay, it can be undesirable to have a lower update rate even when the object is not visible. Note that this affects the update rate of objects that are not visible by the main camera, but by a shadow casting light. Objects whose shadow can be seen generally get updated, but at a low rate, unless this setting forces a higher update rate. See Also Skeletal Animations Simple Animation Component Animation Graph"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/animation-graph-asset.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/animation-graph-asset.html",
    "title": "Animation Graph Asset | ezEngine",
    "keywords": "Animation Graph Asset The animation graph asset is used to configure complex animations. See the animation graph chapter for a conceptual description. Animation Graph In the animation graph nodes represent actions. Data flows from left to right. Nodes have input pins and output pins which represent different kinds of data, such as trigger events, number values, animation poses and bone weights. The goal of an animation graph is to sample a number of animation clips, combine them together, and generate a final pose which can then be applied to an animated mesh. Creating Nodes Right click into the main area to open a context menu. Here you select which nodes to add to the graph. Every graph requires at least one output node and a pose generation node. Connecting Nodes You connect nodes through their pins. Just left click and drag from one output pin to another input pin. The UI will display which pins can be connected once you start dragging. Since every pin represents a certain data type, only compatible pin types may get connected. Some pins allow to connect to multiple other pins, or have multiple incoming connections. If a pin does not allow this, creating a new connection removes previous connections automatically. Nodes that are not ultimately connected to an output node, will not have any effect. Node Properties Nodes may additionally have properties. These are displayed in the property pane when a node is selected. See the documentation for the different node types for more detailed descriptions. Animation Mapping When no node is selected, the properties pane displays the general asset properties. Here you can edit the Animation Clip Mapping array. Every array entry represents one animation. From the drop down you select the name of the animation. If the desired animation name doesn't exist yet, use the option to add one. These names are used throughout your project to identify animations and should be chosen carefully. Select an animation clip asset to map from the animation name to the clip. In the animation graph you never select animation clips directly, but rather always choose the animation by name. Because the mapping is defined in one central place in the animation graph asset, it is easy to replace an animation by another clip, simply by changing the mapping. In the future this feature is also meant to be used to switch between entirely different animation sets and be able to reuse an animation graph between different creatures and different skeletons. This is, however, not yet implemented, but if necessary, can already be emulated to some degree using graph composition (see next section). Compositing Animation Graphs An animation graph may reference another animation graph, to include the functionality of that graph in itself. This makes it possible to define different animation setups over multiple animation graph assets, making editing easier and making it possible to share certain animations between different graphs. When one graph includes another graph, the animation clip mapping of the parent graph takes precedence over the included graph, which allows to override the mapping of the included graph. Using Animation Graph Assets Once an animation graph asset is set up, it can be applied to an animated mesh by adding an animation controller component to the same game object. The component will evaluate the graph in every update, and send the final animation pose to the animated mesh. To control what the animation controller does, you may also need a blackboard for storing state, and potentially a script component to decide when which animation should be active. See Also Skeletal Animations Animation Controller Component Blackboards Simple Animation Component"
  },
  "pages/docs/animation/skeletal-animation/animation-graphs/animation-graph-overview.html": {
    "href": "pages/docs/animation/skeletal-animation/animation-graphs/animation-graph-overview.html",
    "title": "Animation Graphs | ezEngine",
    "keywords": "Animation Graphs Animating characters is a complex task. Even simple creatures typically already need tens of different animation clips for locomotion and basic actions. However, having the animation clips is not enough, they also need to be played on the animated mesh in such a way that animations blend over nicely and play perfectly in sequence. Additionally you may need to apply an animation only to a certain part of the body. Animations should fade in and out smoothly as they are activated and deactivated, and certain clips need to be synchronized to achieve the desired effect. An animation graph is used to configure how each animation clip of a character should behave when it is actively played on the mesh. Animation graphs are the basis for complex animation playback. They also provide some very basic functionality for logic and math, such that one can build simple state machines. For more complex decisions which animations should play when, use custom code. Creating and Using Animation Graphs Animation graphs are configured through the animation graph asset type. To apply the output pose of an animation graph to an animated mesh, add an animation controller component to the same game object. The controller will send the output pose to the mesh every frame, but only while the scene is simulated. To control what an animation graph will output, you typically also need a blackboard. The blackboard is used to store state. Scripts or other custom code decide which animations should be played and write that state to the blackboard. The animation graph in turn reads state from the blackboard and then activates the desired animation clip playback. The graph can also write back state to the blackboard, for example to communicate back that an animation clip has finished playing. Animation Graph Concept The following image shows a basic animation graph: The flow is from left to right. The graph sets up two animations. One idle animation and one wave animation. The two top nodes configure the idle animation. The blue node is a pose generation node, in this case a node that simply samples an animation clip. The node is set to loop and it has no additional input pin connections, which is why it will automatically start and play indefinitely. Its output is fed directly into a pose result node which means that this pose will always be active. The other five nodes make up a second animation. In this case a wave animation that can be enabled at will. At its core it works the same as the idle animation. A sample clip node samples the wave animation clip and forwards it to a pose result node. When multiple pose results are available at the same time, they get mixed together. However, the wave animation should not play all the time, so a check bool node is used to query a blackboard value. Some custom code must decide whether the wave animation should be played and write this information to the blackboard. Since the Start pin of the sample clip node is connected, the node will only start playing the animation when the start pin gets triggered. Additionally, since the wave animation should not always be active, we should set the Target Weight of the pose result to 0 to disable the pose output. Here we simply convert the Bool result of the check bool node to a number (0 or 1) and forward it as a weight to the pose result. Thus, as long as the wave animation should play, the pose result will be used. But when the wave animation should stop (which may be at any time even in the middle of the animation), the pose result target weight will be set to zero, and the pose result node will quickly fade out the animation. Finally, we want the wave animation to only play on the upper body of the creature, so we generate a bone weights mask and forward that to the pose result as well. This makes sure that the wave animation only affects bones from the torso and above. Summary The animation graph uses a graph based workflow to let you visually configure how animation clips are combined. Nodes have inputs to control their behavior, and they output data or state that can (or must) be forwarded to the next step in the pipeline, until an animation pose ultimately reaches the output. The system is intelligent enough to optimize away operations that don't affect the output. You typically control which animations are played through a blackboard. For quick prototyping you can also use the input nodes to get certain input data directly into the graph. Simple animation state machines can be built directly in the animation controller graph using the logic and math nodes as well as the blackboard nodes. For more complex logic you should use custom code. See Also Skeletal Animations Animation Graph Asset Animation Controller Component Animated Mesh Component"
  },
  "pages/docs/animation/skeletal-animation/blender-export.html": {
    "href": "pages/docs/animation/skeletal-animation/blender-export.html",
    "title": "Authoring and Exporting Animations with Blender | ezEngine",
    "keywords": "Authoring and Exporting Animations with Blender This page contains various pieces of information that are good to know when one uses Blender to build and export animated meshes. It is assumed that you know Blender well enough to create animated meshes. Exporting Animated Meshes To get animated meshes out of Blender and into ezEngine, export the animated mesh to a binary GLTF file (.glb). You can enable +Y up or not. In both cases you need to adjust the transformation on the skeleton asset. Make sure that the GLTF export contains Animations and Skinning information. Don't disable animation sampling on export. Be aware that GLTF uses 1000 frames per second for all exported animation clips. Blender, by default, uses 24 frames per second. If you want to only use a sub-range of an animation in EZ, you will need to re-calculate the frame indices accordingly. You can set Blender to use 25 or 50 frames per second to make this calculation easier. Enable Export Deformation Bones Only to strip IK pole targets and other unnecessary bones from the file. Importing Meshes into EZ When importing a mesh, EZ remaps the model space to its own convention. You may need to change the mapping, to get the desired result. For static meshes, this is configure on the mesh asset. For animated models, the mapping is chosen on the root node of the corresponding skeleton asset. EZ uses the following convention: +X is the forward axis +Y is the right axis +Z is the up axis By default all code uses +X as its main direction. For example AI nodes move characters forwards along the +X axis, spot lights and cameras \"look\" into the +X direction and so on. In Blender it is common to have a character look along the -Y axis so that it faces the user when pressing Numpad 1. This also means that the right side of the character will be along the -X axis. If you export such a mesh to a GLB and enable Y UP convention, you need to configure the mapping this way: Set Right Dir to Negative X Set Up Dir to Positive Y Set FlipForwardDir to off Authoring Meshes Make sure all triangles face into the same direction. Use Blender's Face Orientation viewport option to see whether there are flipped triangles. If there are flipped triangles, they will show up incorrectly in EZ. Authoring Animations EZ only supports skeletal animations via skinned meshes. That means every vertex in the mesh needs to have a bone assigned via vertex weights. Blender can move entire objects through bone animations, but if they are only parented to a bone, and don't use vertex skinning (vertex weights), EZ will not show those objects as animated. Use the Vertex Group Weights visualization in Blender to inspect which vertices are set up properly and which aren't. EZ does not support scaling of bones. All bones must have scaling values of 1. If you have an object scaled in object mode and attached to a bone, the scaling will be represented by the bone, so even if your animation keyframes do not use scaling, the exported animation track does. To fix this, select your scaled object and use Mesh > Apply > Scaling to bake the object scaling into the vertex positions and get rid of the scaling in the bone transforms. EZ uses a maximum of 4 bones per vertex. By default Blender's GLTF export already restricts vertex weights to 4 bones, though there is an option to allow more influences. This won't have a positive effect in EZ though. Be aware that Blender exports ALL keyframes of an animation. The preview window of an animation has no effect on the exported animation data. Blender always sets the first keyframe of all animations to index 1 and that is also how the data is exported. EZ expects the first keyframe to be at index 0, though. So set the animation range in Blender to start at index 0 and put the first keyframe there. Use the Action Editor in Blender to create and manage multiple animations in a single file. Be sure to set the Fake User flag on all of them to not lose any work. Now it gets weird: If you add multiple animations in Blender, usually 4 to 6 of them will be exported in the GLTF file. Once you add more animations, seemingly random ones won't be exported. This is because Blender thinks those animations are unused and won't export such unreferenced animations. The fix is to fake reference every animation in Blenders NLA editor. The way to do this, is to push the Stash button in the action editor on every animation. You will then see this reference show up in the scene outline/hierarchy window. Don't rename the stashed item! Every animation should be stashed only once, and if you keep the auto-generated name, Blender won't create another stash reference, if you push the stash button multiple times on the same animation. If you rename the item, that won't work anymore. To delete an animation that has been stashed (and thus referenced by the NLA editor), remove the Fake User flag and also open the NLA editor and delete any reference to the animation by pointing with the mouse on a track and pressing X or Del. Once no reference exists anymore, Blender removes the animation when you save and close the program. Animation Cycles To create an animation that can be repeated, such as walk cycles, the first and the last keyframe must be identical. Furthermore, Blender will typically use cubic interpolation between the keyframes. For the first and last keyframe this will result in an interpolation that slows down and speeds up and is therefore not smooth. The simplest solution is to set these (or all) keyframes to use linear interpolation instead. Another option is to insert duplicated dummy keyframes before the first and after the last keyframe, to force the desired interpolation, but then you need to configure the animation clip in EZ to only use the proper sub-range of keyframes, which can be tricky to figure out. Rigging Meshes There are many good tutorials how to rig meshes. However, here are some additional tips, some that are specific to using animated meshes in game engines. Make sure your mesh is rigged perfectly before you start animating. Even small adjustments to the rig later may require you to redo all your animations. Make sure that all bones are connected correctly to each other. For example, hand and foot bones MUST be connected to their respective arm and leg bones. You must not have any disconnected bones that would be connected in a real skeleton. Some tutorials suggest to disconnect hand and foot bones and use a copy transform constraint instead, when setting up IK in Blender. This is a really bad practice. It will appear to work at first, but once you use partial animation blending (for example to play an animation only on the upper or lower body), it won't work, because the disconnected bones are not part of the correct hierarchy. Similarly, setting up rag dolls for physics requires the bone hierarchy to be correct. Other engines have the same requirement. If you want to add IK to your Blender rig, duplicate the desired bone (for instance the hand bone), then disconnect that bone from the hierarchy (making it a root bone), disable Deformation on that bone, and then use that as the IK target bone. Since Deformation is disabled, this bone won't be exported to GLTF either, which is what you want. It will only be a control bone. If you want your actual hand bone to fully follow your IK target bone, add a Copy Rotation Constraint to it to have it follow both position and rotation animations that you add to the IK target bone. You may also want to hide the original hand bone, such that you don't accidentally pick and animate it, when instead you want to animate the IK target bone (which will most of the time be in the exact same location). Be aware though, that once a bone is hidden, it is quite a pain to make any modifications to it, because Blender won't allow you to select it anymore, not even from the outliner tree view. Instead you must unhide the bone first. Either unhide everything (using ALT+H) or unhide only the desired bone through its context menu in the outliner pane. For IK bones, make sure your pole targets really work correctly. Most tutorials mention that you need to use a rotation offset of +90, -90 or 180 degree, but I have also observed the need for 45 degrees (and consequently 135 degrees) etc. The best way to check is to toggle between Edit Mode and Pose Mode (with the rest pose active) and check that bones with IK don't have extreme twist in pose mode. The bones should only slightly move to fulfill their IK configuration, but if for example arm bones twist by a large amount, then your pole target configuration isn't correct. If you seem to not get the pole target configuration correct, first make sure the target joint has a slight bend (for instance an elbow shouldn't be fully straight). Then remove the IK constraint on the bone entirely and set it up from scratch. Blender seems to have internal state that can't be fixed differently. See Also Skeletal Animations"
  },
  "pages/docs/animation/skeletal-animation/joint-attachment-component.html": {
    "href": "pages/docs/animation/skeletal-animation/joint-attachment-component.html",
    "title": "Joint Attachment Component | ezEngine",
    "keywords": "Joint Attachment Component The joint attachment component is used to expose the animated position of a bone, such that you can attach objects there. In the video above a joint attachment component was used to expose the position of the right hand as a game object. This was then used as the parent object for a gun object. Component Properties JointName: The name of the joint/bone of which you want to use the position as an attachment point. You can look up the bone names in the respective skeleton asset. PositionOffset, RotationOffset: Additional local position and rotation offsets added to the bone location. The same could be achieved by adding another child game object with an offset, but using these properties is more efficient. How To Use Whenever an animated mesh receives a new pose, it passes that pose on to all interested components that are attached to the same object or any child object. The joint attachment component listens to this message and positions its owner game object at the relative position of the selected bone. To attach an object to a certain bone, follow these steps: Create an empty game object as a child of the animated mesh. Add a joint attachment component to it. Set its JointName property to the desired bone name. You can look up the bone name on the skeleton asset that is used by the animated mesh asset on the animated mesh component. Add the desired object or component to the joint attachment object. The local transform of the attachment object will be overwritten by the component when it receives an animation pose. Thus setting any values here doesn't have any useful effect during simulation. To see where your attachment ends up, you need to simulate the scene and an animation has to actively play on the animated mesh. While the scene is simulating, you can use the position and rotation offset properties to tweak the exact location of the joint attachment. Note: The position and rotation offset properties are useful for minor tweaks. The same can also be achieved with another child object. However, it can be difficult to position an attachment perfectly and it might turn out that the attachment position needs tweaking depending on the animation as well. In such cases it is better to add a dedicated bone to the skeleton instead, such that you have full control over the attachment in your modeling tool. See Also Skeletal Animations Joint Override Component"
  },
  "pages/docs/animation/skeletal-animation/joint-override-component.html": {
    "href": "pages/docs/animation/skeletal-animation/joint-override-component.html",
    "title": "Joint Override Component | ezEngine",
    "keywords": "Joint Override Component The joint override component enables you to take control over the local transform of a specific bone. Any transform for that bone that comes from an animation is discarded and replaced with the local transform of the object on which this component is attached. This component can be very useful if you have an object that is typically driven by animations, but a specific bone is meant to be controlled procedurally through game code. An example would be a turret which has a shoot animation for recoil etc, but you want to control the direction into which the barrel points from game code. The joint override component works in the reverse way that the joint attachment component works. Every time an animation pose becomes available, this component will overwrite the transform of the selected bone with its own local transform. Thus you can control the bone transform simply by moving and rotating the game object on which the joint override component is attached. Component Properties JointName: The name of the joint/bone whose transform you want to take control over. You can look up the bone names in the respective skeleton asset. OverridePosition, OverrideRotation, OverrideScale: Whether the component shall override the bone position, rotation and/or scale. See Also Skeletal Animations Joint Attachment Component"
  },
  "pages/docs/animation/skeletal-animation/root-motion.html": {
    "href": "pages/docs/animation/skeletal-animation/root-motion.html",
    "title": "Root Motion | ezEngine",
    "keywords": "Root Motion By default a skeletal animation has its origin at the position of the game object on which it is played. Relative to that location animations will move the bones and the skinned mesh will move accordingly. The game object itself stays fixed at that location, though. This is sufficient if either the game object shouldn't change its location anyway, or when any change in position is controlled through other means. For example a player character might be moved around the world through custom code and a walking animation is only played to visualize the action. This approach can be the right solution, depending on the type of game. Such a method is, however, very prone to foot sliding, meaning an artifact where the feet move, but don't stick to the ground. If the movement of a game object should generally be determined by the exact blend of animation clips, it is better to have the motion be part of each animation clip. For example a walk animation would contain the information into which direction and at what speed a game object should be moved to fit the animation. When a forward and walk right animation get mixed together, their root motion information is equally mixed and the object would be moved diagonally. Defining Root Motion There are multiple ways how root motion could be defined for a clip. It could come from a dedicated bone for overall motion, or it could be extracted from how the feet touch the ground, etc. For the time being EZ only implements the most simple method. An animation clip either has no root motion at all, or it has a constant motion that is used for the entire clip. This is sufficient to build basic locomotion animations. Finally, for now only positional root motion is available. That means an animation can change the position of a game object, but not its rotation. It is planned to add more sophisticated methods for root motion in the future. Applying Root Motion The simple animation component and the animation controller component get the root motion data from the played animation clips. There are these modes to apply it to their owner game object: Ignore: No root motion is applied, the game object will not be moved by the animation. ApplyToOwner: Any available root motion is directly applied to the game object and thus moves it without restriction. This mode is useful for objects that have to follow a fixed path. For example moving platforms (which are kinematic physics actors), or objects that don't physically interact with the player. This mode is not suited for characters that should obey physical restrictions. SendMoveCharacterMsg: If this mode is used, root motion is not applied to any object, instead the message ezMsgMoveCharacterController is sent to the top most game object in the hierarchy. This way, if there is also a character controller or other component that accepts this type of message, it can apply the root motion as it sees fit. See Also Skeletal Animations Simple Animation Component Animation Controller Component"
  },
  "pages/docs/animation/skeletal-animation/simple-animation-component.html": {
    "href": "pages/docs/animation/skeletal-animation/simple-animation-component.html",
    "title": "Simple Animation Component | ezEngine",
    "keywords": "Simple Animation Component The simple animation component is used to play a single animation clip on an animated mesh. The component has to be attached on a game object that also has an animated mesh component. The selected animation clip has to be compatible with the mesh's skeleton. For more complex scenarios use an animation graph instead. Component Properties AnimationClip: The animation clip to play. AnimationMode: How to play the animation: Once: The animation is played exactly once and then stops. Loop: The animation is played in an endless loop. BackAndForth: The animation is played to its end, then it reverses and plays back to the start. Then the cycle repeats. Speed: The playback speed. A speed of zero pauses playback. A negative speed makes the animation play backwards. The speed can be changed at any time. RootMotionMode: Selects how root motion is applied to the owning game object. InvisibleUpdateRate: How often to update the animation when the object is not visible. For performance reasons the update rate should be very low or even paused when an object isn't visible. However, since animations may have an important impact on gameplay, it can be undesirable to have a lower update rate even when the object is not visible. Note that this affects the update rate of objects that are not visible by the main camera, but by a shadow casting light. Objects whose shadow can be seen generally get updated, but at a low rate, unless this setting forces a higher update rate. Animation Events The component will broadcast the event ezMsgGenericEvent every time it encounters an animation event in the animation clip. Custom code can listen for these events and trigger relevant game mechanics. See Also Skeletal Animations Animation Clip Asset Animated Mesh Component Animation Graph"
  },
  "pages/docs/animation/skeletal-animation/skeletal-animation-overview.html": {
    "href": "pages/docs/animation/skeletal-animation/skeletal-animation-overview.html",
    "title": "Skeletal Animations | ezEngine",
    "keywords": "Skeletal Animations Skeletal animations are used to animated meshes. This is typically used for game characters and robots, but is equally useful for other complex moving objects. Describing how skeletal animation works in general is out of scope for the EZ documentation, but there are a vast amount of resources about this topic online. It is assumed here, that you are familiar with the concepts and know the basics about modelling, rigging and animating a mesh with a tool such as Blender. The rest of this document gives a high-level overview, how to get started with getting animated meshes into EZ. For more in-depth descriptions of each feature, please consult the respective documentation pages. Sample Scene For a sample scene to look at, open the Animation scene from the Testing Chambers project. Additionally, the Monster Attack Sample may also be a useful resource. The Animation System Pieces The following elements are involved to make an animated mesh: Animated Mesh Asset The animated mesh asset represents the mesh of the animated object. This is a special version of the mesh asset. It works mostly the same way, except that it adds the necessary skinning information. Consequently, only these kinds of meshes can be used for skeletal animation. An animated mesh asset requires you to specify a default skeleton asset, otherwise it won't even transform the data. Skeleton Asset The skeleton asset stores the bone hierarchy of the animated object. This is also where you configure the overall scaling and which direction should be the forward, right and up vector of the imported model. The skeleton asset is also where you set up physics collision shapes for hit detection and ragdolls. Animation Clip Asset The animation clip asset represents a single animation, such as a walk or a jump animation. You may have multiple animations stored in a single .fbx or .glb file, but you need to create one animation clip asset for each animation that you want to import into EZ. Just reference the same source file each time. The animation clip asset has a property UseAnimationClip through which you can choose which animation to extract from the source file. Currently you have to type the name of the animation. The Available Clips list just below it shows you which animations have been found in the file. Additionally, in case all animations are in one large clip, you can use the FirstFrame and NumFrames properties to extract only a subset of the animation. This allows you to import the same source file many times, each time extracting only a specific range as a single clip. The root motion properties are meant for enabling an animation clip to move an animated character (ie. the actual character controller that sits on top of the animated mesh). The event track property allows you to add markers to the clip, that indicate what happens at what time in the clip. This can be used to indicate when a foot touches the ground, or at what point in the animation a weapon fires. Using this information, the game logic could react by, for example, spawning an effect. Animation events are sent as event messages and therefore can only be captured by event handler components such as visual scripts or TypeScript components. Simple Animation Playback Once you have an animated mesh asset, a skeleton asset and an animation clip asset all set up, you can create an animated object in a scene by attaching an animated mesh component and a simple animation component to a game object like so: Here we have also attached a visual script to react to events in the animation clip, but that is entirely optional. When you press play, the editor will play back the animation on the mesh. Advanced Animation Playback To create a playable character, you need multiple animations for all the actions that the character should be able to do. A big part revolves around locomotion, ie making the character walk around. Here it is not sufficient anymore to just play one animation, you will need to have multiple animations and blend them together in a convincing way. This is where the animation graph comes into play. This asset allows you to define how animations should be combined to make a character move fluidly and react to various inputs. Once you have basic animation playback working, getting familiar with animation controllers is the next step to make the most out of your animated characters. How to Import Animation Data To import an animated object, you need to set up multiple assets (the mesh, the skeleton, the animation clips). The easiest way is to press CTRL+I to open the asset import dialog. Select your .fbx or .glb file and choose to import it as multiple asset types like so: This will create all three necessary assets. To import additional animation clips, the easiest method is to just use File > Save As... in the existing animation clip asset to duplicate it for the next clip. Animation Utility Components The following components are currently available: Joint Attachment Component If you put a child object under an object with an animated mesh component, and attach a joint attachment component, the animation system will take control over that object's position. With the JointName property you specify a bone that you are interested in (see the skeleton asset, if you need to know what bones are available). Every game tick, the animation system will then move that game object to the position of the animated bone. This allows you to have game objects move in sync with the animation. This can be used to put an object into the hand of a creature, or to have an effect follow the animation. In the sample scene this is used to place an object at the (animated) front of the turret's barrel, such that the projectile will be spawned at the proper position. Joint Override Component The joint override component is the opposite to the joint attachment component. It works similar in that it has to be attached to a child object of an animated mesh and you have to type in the name of a bone. However, it will then override the specified bone's local transform with its own local transform. The idea here is, that game code can use this game object as a controller to steer the animated object. So for example your turret might be one complex animated object, with many bones but somewhere in the bone hierarchy there is one bone that controls the turrets aim (up/down or left/right or both). You want cool animations to \"unfold\" the turret when it is built and all sorts of other animations that are only possible with proper skeletal animation, but once the game runs you also want to procedurally control where the turret is aiming. Using this component you can take control of certain bones and drive their animation yourself. In the sample scene there are two objects for each turret that override two bones: one for left/right rotation and one for pointing up/down. You can manually modify these values from the property grid, while the editor is in play mode, to control the turrets aim. Note how the recoil animation continues to play properly relative to the turret's main direction. See Also Animated Mesh Asset Skeleton Asset Animation Clip Asset Animation Graph"
  },
  "pages/docs/animation/skeletal-animation/skeleton-asset.html": {
    "href": "pages/docs/animation/skeletal-animation/skeleton-asset.html",
    "title": "Skeleton Asset | ezEngine",
    "keywords": "Skeleton Asset The main function of the Skeleton asset is to store information about the bone hierarchy in an animated mesh. It is a central aspect for all skeletal animations. The skeleton is used to adjust the overall scale and rotation of animated meshes. It is also used to define collision shapes for bones, which is needed for hit detection and when additionally joint types and joint limits are configured, the skeleton can be used for ragdolls. User Interface The left hand side shows a tree view with the bone hierarchy. The top most element is always the ezEditableSkeleton node, which represents the skeleton in general. When you select any node in this tree, the Skeleton Properties panel on the right displays the properties of the selected node. Select the root node (ezEditableSkeleton) to edit the overall properties, such as which file to import. Once a file is imported, the bones should show up as child nodes. Select any of them, to configure the properties of a bone. Asset Properties Select the ezEditableSkeleton node to edit the overall asset properties. File: The file from which to import the skeleton information. This is typically the same file as in the animated mesh asset. RightDir, UpDir, FlipForwardDir: These properties are the same as on the mesh asset. Depending on how the mesh was exported, you may need to adjust these to have the skeleton (and every mesh that uses this skeleton) stand upright and look into the correct direction. In Blender it is common to model meshes such that they face the user when the front view is active (Numpad 1): With such an orientation the right side of the model points into the -X direction. The +Z axis corresponds to the up direction and the model looks into the +Y direction. When you export such a model to GLTF/GLB, you can keep this convention (Y up disabled in the export settings) or you can export it in the more common convention of using +Y as the up axis (Y up enabled). Both conventions can be mapped to EZ's preferred convention like so: For GLB files exported from Blender with Y up use: RightDir = Negative X UpDir = Positive Y FlipForward = off For GLB files exported from Blender with Z up use: RightDir = Negative X UpDir = Positive Z FlipForward = off Note that in EZ the convention is that models look along the +X axis. Every component (such as AI) assumes that moving along the +X axis will move the mesh forward, moving along +Y moves it to the right and moving along +Z moves it upwards. It is therefore best to import all meshes this way right away. UniformScale: The overall size of the skeleton. Use this if you need to adjust from centimeters to meters. BoneDirection: This setting only affects the visualization of the skeleton. It has no effect on the actual mesh skinning. It is used to tell the visualizer which cardinal direction the bones should point into. You only need to change this setting, if the skeleton visualization looks all wrong (all lines point into weird directions). You need to transform the asset to apply the change. Just try all options until it looks right. PreviewMesh: An animated mesh to render transparently over the skeleton to serve as a preview. This is especially useful when setting up collider shapes, to know how large they should be to fit nicely. CollisionLayer, Surface: The default collision layer and surface to use for all bone shapes. This can be overridden on each bone. MaxImpulse: When projectiles and other things apply impulses to ragdoll limbs, the forces can quickly add up and fling a ragdoll far away. This value is used to clamp the maximum impulse to apply to prevent that. Bone Properties Select a bone from the hierarchy to edit its properties. Note: All bone properties are physics properties that are used for setting up collision shapes and constraints. Collider shapes are needed for hit detection. Colliders and constraints are only needed for ragdolls. Local Rotation: This property only shows up on bones that have a parent bone with a collision shape. It is used to adjust the orientation of the joint constraint. Joint Type: Selects what kind of constraint should be between this bone and the parent bone. If it is set to None, the two bones are not joined. If the skeleton is not meant to be used for ragdolls, there is no reason to select anything else. If the type is set to Fixed the two bones are going to be joined such that they are perfectly stiff. In Swing Twist mode you can configure the range of motion between these bones to limit how far they can physically move when simulated as a ragdoll. Stiffness: How easily the ragdoll joint bends. Swing Limit Y/Z, Twist Limit, Twist Limit Center Angle: All these are used to set up the swing-twist constraint. Enable the visualization of the constraint limits to see their effect. The swing limit restricts how far a bone may swing away from its parent bone. Be aware that you can only define the swing limit as a half angle, meaning it swings both ways. Adjust the Local Rotation to define what the center of the bone transform is, such that both extremes end up as a natural movement range. The twist limit works similar, it restricts how much the bone may twist around its main axis and the limit goes both ways.Adjust the center angle to be somewhere within the current limit range, otherwise the moment ragdoll mode gets activated, the bones will immediately twist to be within the limit, which ends up looking very weird. Override Surface / Override Collision Layer: If either of these is activated, you may specify a different surface or collision layer to use just for this bone. Note that the default is configured on the root node. Bone Shapes: An array of shapes to use for this bone. Use this to configure the approximate physical shape of the mesh, which will be used for hit detection or ragdolls. Colliders: A mesh may contain custom collider shapes, which can be extracted automatically. For this to work a mesh must use the name prefix UCX_ to indicate that it is a convex mesh. It must then continue with the bone name to which it should be attached to. So for example, if the bone is called Bone.001, and the custom convex mesh is called UCX_Bone.001 then the mesh will be used as a convex collider shape for that bone. As an example, see the BreakableCube.glb file in the tTesting Chambers project and the corresponding BreakableCube skeleton asset. See Also Skeletal Animations Animated Mesh Asset Skeleton Component"
  },
  "pages/docs/animation/skeletal-animation/skeleton-component.html": {
    "href": "pages/docs/animation/skeletal-animation/skeleton-component.html",
    "title": "Skeleton Component | ezEngine",
    "keywords": "Skeleton Component The skeleton component can be used to visualize the current pose of an animated mesh. This component is only meant for debugging purposes. Component Properties Skeleton: The skeleton asset to visualize. You can set a skeleton asset manually, or keep it empty. In the latter case the skeleton will automatically be taken from a sibling animated mesh component, but only when the pose of that mesh is changed. VisualizeSkeleton: Whether the component should visualize the skeleton at this time. VisualizeColliders: Whether the collision shapes of the skeleton should be visualized. VisualizeJoints: Whether the joints between the bones shall be visualized. VisualizeSwingLimits, VisualizeTwistLimits: Whether the swing and twist limits of the joints shall be visualized. BonesToHighlight: A semicolon-separated list of bone names (case sensitive). Every bone whose name appears in the list will be rendered with a highlight. To know the available bone names, inspect the bone hierarchy in the respective skeleton asset. See Also Skeletal Animations Animated Mesh Component"
  },
  "pages/docs/animation/skeletal-animation/skeleton-pose-component.html": {
    "href": "pages/docs/animation/skeletal-animation/skeleton-pose-component.html",
    "title": "Skeleton Pose Component | ezEngine",
    "keywords": "Skeleton Pose Component The skeleton pose component allows you to assign a custom, static pose to an animated mesh. This can be used for decorative purposes, for instance to place an animated mesh in different poses in your scenes, but it can also be used as a start pose for a mesh that is further animated, for example through ragdoll physics. Component Properties Skeleton: The skeleton for posing. Mode: Selects the source for the custom pose. Bones: The array of bones and their current values. Through the UI you can modify bone rotations or reset them to their default by clicking the 'x' button. However, it is more convenient to edit the bones in the 3D viewport. Click the blue property to enable the bone manipulator, then click on one of the joints to select it for editing, as seen in the image above. Using Ragdolls for Posing When a pose component and a ragdoll component are both present on an animated mesh, the pose component can be used to define the starting pose of the ragdoll (make sure to configure the ragdoll to wait for a pose before it starts simulating). Additionally, when you simulate a scene in editor, you can save the result of a ragdoll simulation in a pose component: Place your ragdoll, make sure a pose component is attached, simulate the scene and then, while the simulation is still running, make sure your ragdoll is selected and press K (or Scene > Utilities > Keep Simulation Changes). Once you stop the simulation, the pose component gets updated to mimic the pose that the ragdoll was in. You can then adjust individual limbs and repeat the process with a new starting pose for the ragdoll, until you are happy with the result. You can also remove the ragdoll component afterwards. See Also Skeletal Animations Animated Mesh Component Ragdoll Component"
  },
  "pages/docs/api-docs.html": {
    "href": "pages/docs/api-docs.html",
    "title": "API Docs | ezEngine",
    "keywords": "API Docs Online The C++ API documentation is available online at ezEngine.GitHub.io/api-docs. Please be aware that the search functionality on the website is very limited, it only finds items when their name starts with what you searched for. So for example to find the documentation for the ezGameState class, you have to type ezgamest to show any results, typing gamest without the ez prefix will show no results. Generating the Docs locally If you need the latest source documentation, you can generate the Doxygen documentation locally. A Doxyfile is available in the Documentation folder of the repository. See Also Doxygen"
  },
  "pages/docs/appendix/coding-guidelines.html": {
    "href": "pages/docs/appendix/coding-guidelines.html",
    "title": "Coding Guidelines | ezEngine",
    "keywords": "Coding Guidelines Our coding guidelines are documented here. See Also Library Structure String Usage Guidelines Container Usage Guidelines"
  },
  "pages/docs/appendix/color-spaces.html": {
    "href": "pages/docs/appendix/color-spaces.html",
    "title": "Color Spaces | ezEngine",
    "keywords": "Color Spaces See Also"
  },
  "pages/docs/appendix/container-usage.html": {
    "href": "pages/docs/appendix/container-usage.html",
    "title": "Container Usage Guidelines | ezEngine",
    "keywords": "Container Usage Guidelines ezEngine has the following container classes: ezStaticArray ezHybridArray ezDynamicArray ezStaticRingBuffer ezDeque ezList ezMap ezSet ezHashTable ezArrayMap The following containers store their data as contiguous arrays: ezStaticArray ezHybridArray ezDynamicArray ezStaticRingBuffer ezArrayMap The following containers are built on top of ezDeque and thus share some performance characteristics: ezList ezMap ezSet When to use which Container Type Arrays General Guidelines For arrays, ezHybridArray, ezDynamicArray and ezDeque are the most important containers. If you know or have a guess how much data you will need, always use this information in a call to Reserve to ensure that the containers can allocate data once (or at least much less), and do not need to reallocate several times. Never remove an element in between (using RemoveAt), unless there is really no other way (and hopefully your array is small). Prefer RemoveAtAndSwap to replace the removed element by the last element in the array instead (this will destroy the order though). Similarly, never insert elements anywhere else than at the end. The only exception is ezDeque, which is very efficient at insertion and removal of elements at both ends. ezHybridArray ezHybridArray uses an internal fixed size cache (which you can specify as a template argument). When you create an ezHybridArray on the stack, that data is also allocated on the stack. This is the most important container for writing performance critical yet safe code. ezHybridArray allows to implement many algorithms without the need to use dynamic allocations. Prefer ezHybridArray when you have a use case where you typically have a small set of elements, but it might be larger in some situations. ezHybridArray will give you the performance of an immediately available array on the stack, combined with the safety of a dynamically allocated array on the heap, as it will reallocate data dynamically, whenever its internal storage is insufficient. However, be careful not to make the internal buffer too large. When creating ezHybridArrays on the stack, you should not use more than a few KB for the internal cache, as you increase the risk for stack overflows. You should usually try to stay below (2KB / sizeof(Type)) for the number of elements in the ezHybridArray cache. Note: ezHybridArray is derived from ezDynamicArray, that means every function that takes an ezDynamicArray (even for writing output to), can be given an ezHybridArray. ezDynamicArray ezDynamicArray always allocates its data on the heap. The upside is, that it has a very low memory overhead, as long as it is empty, and it can handle any number of elements. Prefer ezDynamicArray if your working set is generally larger and when you know how many elements it needs to hold before you fill it up. Use Reserve or SetCount BEFORE you start adding data to it, to prevent unnecessary reallocation later, as those are very costly. Also prefer ezDynamicArray if the memory overhead in the empty state is of concern. ezDeque ezDeque stores its data as several chunks of contiguous arrays. An additional \"redirection array\" is used to know how to index into these chunks. ezDeque requires one pointer indirection to make a lookup into its data. The deque is the only array container that does not store its data in one contiguous block of memory. Be aware of that, you cannot memcpy or memcmp data in a deque with more than one element, as the next element might be stored somewhere completely different. However a deque NEVER relocates an existing element in memory, either. That means once an element is inserted into a deque, you can safely store pointers to it, as it will not move around in memory (unless it is deleted, of course). Deques allocate their data in chunks. So whenever the memory is insufficient, a new chunk that can hold a fixed number of elements is allocated. One chunk is typically 4 KB in size. That means if you store floats in a deque, one chunk can hold 1000 values. Thus this is the 'minimum' memory usage of a deque, unless it is completely empty still. As such ezDeques are very efficient for iteration (most of the data is contiguous) and they are very dynamic, as their size can grow dynamically without the need to reallocate and copy previous data, as the other array types would need to do. Therefore prefer ezDeques whenever you need to have nearly the performance of an array, but have very dynamic data sets, that are difficult to predict in size, or that vary all the time. As deques are optimized for insertion and removal at both ends, ezDeques are perfectly suited as fifo queues and dynamic ring-buffers. The memory overhead of ezDeques is rather high, so do not use it for small data-sets (here ezHybridArray is typically the best container). ezDeques are also very well suited, whenever you have large objects to store that are very costly to construct or copy around, and you want to prevent those operations by all means, such that you do not want a container reallocation to trigger that. ezStaticArray This is a container that only stores a static array internally and cannot resize itself to be larger than that. Use this in code that has a definite upper limit of elements and whenever you must prevent the usage of any allocator by all means (such as for global variables). Typically there should be no need for this container, as ezHybridArray delivers the same performance advantages and the safety of reallocating to the required size dynamically. ezStaticRingBuffer Use this when you need a ring-buffer that shall have a fixed size. Use ezDeque if you need a dynamically resizing ring-buffer. Lists There is only one implementation of a doubly linked list: ezList ezList is internally built on top of a ezDeque. As such the memory requirements are the same. ezList is optimized for inserting and removing objects frequently. Iterating over its elements might be slow due to excessive cache misses. You should typically not use ezList in code that is performance critical. Use it when you have data-sets of unpredictable size that need to be sorted or rearranged very dynamically. For example when you read a complex data set which then needs to be processed and sorted by different criteria, which might delete and insert elements at random positions, then prefer an ezList. However, once that step is finished, you should copy your sorted data into some array container for faster access. Nodes in an ezList are never relocated in memory, as such iterators stay valid as long as the element is still alive. Associative Containers ezMap and ezSet Both containers are basically the same, except that ezMap stores a 'value' for each 'key', whereas a set only stores 'keys'. Use ezMap whenever you need to be able to look up an entry with a key. Use ezSet whenever you simply need to know whether some element is present or to merge data-sets down to all the unique elements. ezMap and ezSet are built on top of ezDeque, similar to ezList, that means they are quite heavy-weight in their memory consumption, however they can grow in size efficiently. As with ezList, iterators (and pointers) to elements stay valid, as long as the element is alive, i.e. the data is never relocated. Insertion, lookup and removal are all O(log n) operations, since they are red-black trees internally and thus always perfectly balanced. ezMap and ezSet are well suited for very dynamic data sets (where a lot of insertions and removals are done, while also using it for lookup). If you have a use-case where you insert once and then lookup often, a sorted array, such as ezArrayMap, or an ezHashTable might be more efficient. ezMap and ezSet only require a simple comparer to be able to sort elements in a strictly weak ordering. As such they are well suited to handle objects that can be difficult to be hashed. Note that the nodes in the Map/Set each contain one element of their key/value type and those are stored in an ezDeque. As such, when you put an ezHybridArray (or an ezString) into an ezMap, only one allocation is needed to allocate all the memory for a chunk (in the ezDeque) of data, which holds a large number of nodes, which already embed the data of their keys/values (e.g. ezHybridArray). Thus you can get away with very few memory allocations. If however you store an ezDynamicArray in an ezMap, each element still needs to allocate its own internal storage, which means you will get one additional allocation per element. ezArrayMap This container provides similar functionality as ezMap but should be more efficient in scenarios where elements are looked up more often than they are inserted or removed. The implementation simply uses an array that is kept sorted, such that lookups can be done in a more cache friendly manner. If all you need is an associative container and your use case consists of changing the container infrequently, but looking up elements frequently (which is very often the case), you should prefer this container. Note, however, that this container will rearrange elements in memory whenever it needs to be sorted. In contrast an ezMap guarantees that elements never move in memory, allowing to store pointers to the memory locations. Likewise the iterators of an ezMap stay valid as long as an elements resides in the map. For ezArrayMap this is not true, the index at which an element is stored can change whenever any element is added or removed. ezHashTable The ezHashTable is optimized for very fast lookup, which should typically be a O(1) operation. Prefer the hash table whenever you will have a data set that is modified infrequently, but lookups will be done often and need to be as fast as possible. ezHashTable may relocate its internal memory, which means iterators to its elements may not stay valid when the table is modified. Make sure to Reserve how much elements you probably will put into the ezHashTable, to ensure fewer reallocations, but also to avoid hash-collisions and thus to improve overall performance. See Also String Usage Guidelines"
  },
  "pages/docs/appendix/library-structure.html": {
    "href": "pages/docs/appendix/library-structure.html",
    "title": "Library Structure | ezEngine",
    "keywords": "Library Structure This document gives an overview of the functionality that ezEngine provides and how the most important libraries are structured. ezEngine is divided into multiple libraries that provide different functionality. The most basic and also largest library is Foundation. It contains all the basic functionality on which the rest of the engine is built. Foundation is meant to be application agnostic. All its features can be used in any kind of application. Most of the platform abstractions are implemented here. Foundation itself only depends on various third-party libraries. The Core library is built on top of Foundation. This library contains engine specific features, such as the Game Object system. Core is where most of the actual engine infrastructure is implemented. System is the library that is supposed to contain all the high-level platform specific code that might be difficult to abstract. Currently this mostly contains window creation code. The TestFramework library implements code to manage our unit-tests. You can ignore this entirely. GameEngine builds on top of all the other libraries, including the rendering code. It contains the most high-level code for a game engine, such as AI and Animation, ezGameApplication (TODO) and ezGameState, which are the basis for any game application built with EZ, the prefab system, the visual scripting and much more. In general the most interesting libraries to look at are Foundation, Core and GameEngine. Library Overview: Foundation The Foundation library contains all the 'low-level' code that is used throughout the engine. In Foundation\\Containers you will find different types of container classes. These are heavily influenced by the STL, so a lot might look familiar. The most interesting class one should have a look at is ezHybridArray. In Foundation\\Strings you will find all the string classes and utilities. ezEngine works with Utf-8 strings everywhere, which makes some things a bit more complicated, but these string classes make it pretty easy. There are utility classes to work with raw C strings and higher-level string classes to create and store strings efficiently. In Foundation\\Math you will find lots of math classes, e.g. classes to do vector arithmetic (ezVec3, ezMat4, ezQuat, ezPlane, etc.), classes to work with colors (ezColor, ezColorGammaUB, ezColorLinear16f), classes to work with bounding volumes (ezBoundingBox, ezBoundingSphere) and do culling (ezFrustum), utility functions for intersection tests (ezIntersectionUtils) and a class to work with angles efficiently (ezAngle). There is even an implementation for a fixed point type (ezFixedPoint). In Foundation\\Time you will find ezTime, which handles simple time values. Using ezTime::Now() you can access the current application time. To handle game time (e.g. for slow-motion, etc.) use ezClock. For profiling or timing ezStopwatch is available. And finally for system-independent timestamps, which might be useful when working with file modification times, ezTimestamp and ezDateTime are provided. In Foundation\\Threading you will find functionality that is useful for threading. ezAtomicInteger provides lock-free integer values. ezLock and ezMutex implement the standard mechanisms for working with critical sections. ezThread is a platform independent implementation for threads and ezThreadSignal allows to send signals to other threads. ezThreadUtils provides some of the low-level threading functions, such as ezThreadUtils::Sleep(). However, before you go ahead and create your own threads, you should have a look at ezTaskSystem, which is a thread-pool implementation that efficiently distributes tasks across multiple worker threads. It supports dependencies across tasks, different priorities, waiting/blocking for unfinished tasks, task canceling and load-balancing when tasks run over multiple frames. There should be only very few situations where a task is not good enough and a custom thread is necessary. In Foundation\\Logging you will find ezLog, the central class to output log information. There are various ways logging information can be output. ezLogWriter::HTML allows to write the data to an HTML file, ezLogWriter::Console and ezLogWriter::VisualStudio output the data to different console windows. Additionally the ezInspector tool will display all logged data as well. The logging system is extremely useful to get an insight into what your application is doing and what might be going wrong, so we suggest setting this up early and using it to log most of what your application is doing. In Foundation\\Algorithm you will find some useful algorithms, mostly for sorting, searching and hashing. In Foundation\\Basics you can find a lot of platform specific code, most of which might not be very interesting. You will also find EZ_ASSERT which you should be using frequently. In Foundation\\Basics\\Types you will find some fundamental types that are used frequently in EZ. ezDelegate is often used for callbacks. ezEnum is used for type-safe enum types and ezBitflags is used for type-safe and easy to use bitflags. ezArrayPtr is a 'fat pointer' that stores the start and length of an array. Finally ezVariant is a type that can store different types of data (float, int, string, vector, etc.) and knows which type it has stored. It can do conversions between related types and is often used in message passing. In Foundation\\CodeUtils you can find utilities to work with code or text, such as ezTokenizer. You will also find a full implementation of a C preprocessor (ezPreprocessor). In Foundation\\Communication you can find functionality to communicate with other code. ezEvent is a frequently used class to raise events and thus inform other code of changes. ezMessage is used for message passing, which is quite often used with the game object system (which you will find in the Core library). ezTelemetry is a system to broadcast information from the running application to other applications, usually tools for introspection, such as ezInspector. In Foundation\\Configuration you will find tools to configure the application. ezStartup is a system to (de-)initialize different subsystems in your code in the right order. ezPlugin is used when you want to extend your application using plugin DLLs dynamically at runtime. ezStartup helps with this as well. ezCVar is a 'configuration variable' that allows to easily change the state of the running application. Its state can be stored on disk and it can be modified either through an ezConsole or remotely through ezInspector. This allows to have lots of 'debug modes' that can be enabled on demand without recompilation or complicated integration into the input handling. In Foundation\\Utilities you will find some utility functionality, such as ezCommandLineUtils for command-line argument parsing, ezConversionUtils for string / number conversions and ezStats for broadcasting the state of some internal code, which is useful for debugging game code. In Foundation\\IO you will find lots of functionality for reading and writing data. ezStreamReader and ezStreamWriter are the interfaces for all IO operations. Derived from these classes you will find ezMemoryStreamReader / ezMemoryStreamWriter for working with data in-memory. ezCompressedStreamReaderZstd and ezCompressedStreamWriterZstd allow to zip and unzip data and ezChunkStreamWriter / ezChunkStreamReader implement a 'chunked' format that can be used for building file formats that another application may not fully understand. ezOSFile is the low-level file abstraction, in most cases you should not need to work with this. Instead prefer ezFileSystem which adds functionality for virtual file systems through mount points. For example a compressed file or a remote folder may be mounted as a read-only directory. ezFileSystem is the central class to manage file accesses, but to actually read or write a file, use ezFileReader and ezFileWriter, which also implement the ezStream* interface. To store data in a structured way, ezJSONWriter and ezOpenDdlWriter are provided. For convenient retrieval ezJSONReader and ezOpenDdlReader are available. For less convenient but more flexible and efficient JSON/OpenDDL reading you can also use ezJSONParser or ezOpenDdlParser. In Foundation\\Reflection you will find the reflection system of ezEngine. This is used by the game objects and some other high-level code for object type identification and properties. This may be used for scripting, for setting up objects from configuration files and for editors. Most notable classes are ezRTTI and ezReflectedClass. Library Overview: Core In Core you will find the core engine infrastructure. In Core\\Application you can find code to more easily set up your application loop in a platform independent way. In Core\\Graphics you will find code commonly needed for doing graphics, such as ezCamera for camera controls and ezGeometry to create geometric objects. In Core\\Input you will find ezInputManager which can be used for retrieving input from various different devices, e.g. mouse, keyboard, gamepad or virtual thumbstick. The system is easily extensible to include custom devices. In Core\\ResourceManager you will find the static class ezResourceManager which is the central class for resource loading (e.g. textures, shaders, etc.). For implementing custom resource types you need to derive from ezResource and for customizing the loading procedure you may need to implement a custom ezResourceTypeLoader. All resources are referenced through ezResourceHandle types, which implement reference counting. In Core\\Scripting you can find ezLuaWrapper that allows to easily work with Lua scripts. In Core\\World you will find the game object system. ezGameObject is the class to use to manage entities, ezComponent is the base component class that allows to implement and attach components to your entities. All entities belong to an instance of ezWorld, which represents your scene graph. Library Overview: GameEngine In GameEngine you will find all the high-level code needed in a game engine. GameEngine\\Console contains code for a Quake-like in-game console that can be used for changing the game configuration (through ezCVar or custom functions) and to see the ezLog output. GameEngine\\GameApplication contains ezGameApplication (TODO), which extends ezApplication with higher-level, more game specific functionality. This is one of the most important high-level classes to look at and extend when writing your own, stand-alone game application (assuming you can't do so with ezGameState alone). In GameEngine\\GameState you find ezGameState, which is the most important class to extend when writing your own game code, especially if you want to be able to run your code within the editor. GameEngine\\Interfaces contains various interface definitions for instance for basic interactions with physics and audio engines. GameEngine\\Prefabs contains the code to work with prefabs. Library Overview: Texture In Texture you will find various things related to working with images and textures. In Texture\\Image you will find ezImage which can be used to read, write and convert images from various formats. Library Overview: Utilities In Utilities you will find various different things that may be useful, but are not used by the general engine runtime. They may be used by some tool or by a sample game, though. Utilities\\DataStructures contains data structures that are too engine specific. Here you will find things such as octree and quadtree implementations (ezDynamicOctree). Utilities\\GridAlgorithms contains functionality to rasterize circles and lines into grids. This can be extremely useful for 2D top down games, like strategy games, to do line-of-sight computations and such. Utilities\\PathFinding contains functionality to do path searches on graphs and simple nav-mesh generation for 2D grids. See Also Samples"
  },
  "pages/docs/appendix/string-formatting.html": {
    "href": "pages/docs/appendix/string-formatting.html",
    "title": "String Formatting | ezEngine",
    "keywords": "String Formatting For formatting strings there are a couple of different options: ezConversionUtils provides various ToString functions. These are useful for generic cases, where only individual variables need to be converted to a string representation and no control over the exact formatting is needed. ezConversionUtils also provides functions to parse strings for numbers. ezStringUtils::snprintf is a custom implementation of the infamous C printf function, with better security against buffer overruns and consistent behavior across platforms. It is available, but should generally be avoided, as it cannot provide any type safety. ezFormatString is the preferred way to do string formatting. It is easy to use, fully type safe, extensible with custom formatters and optimized for performance by doing only very few allocation and delaying the formatting until it is needed, which enables functions to not pay the price for formatting an incoming string, if the function doesn't actually use the result. Using ezFormatString as an Argument ezFormatString is a class that can be easily used as a function parameter to accept formatted strings: void Print(const ezFormatString& text); A function that takes just an ezFormatString has to be called with the ezFmt wrapper: Print(ezFmt(\"Hello {}\", \"World\")); The ezFmt function is a variadic template, that can take up to 10 arguments and wraps them all up into an ezFormatString object. If you want your Print function to be a little bit more convenient, and not require the use of ezFmt, you can add an overload that provides the variadic template functionality directly. template <typename... ARGS> void Print(const char* szFormat, ARGS&&... args) { Print(ezFormatStringImpl<ARGS...>(szFormat, std::forward<ARGS>(args)...)); } Now Print can be called like this: Print(\"Hello {}\", \"World\"); Inside Print, all you need to do to get the formatted string is to call ezFormatString::GetText(): void Print(const ezFormatString& text) { ezStringBuilder tmp; const char* szResult = text.GetText(tmp); // do something with szResult, do not use tmp, as it is not guaranteed to hold the result (meaning, it may not have been needed) } Using ezFormatString Once a function takes an ezFormatString (see above), you can use {} notation to indicate where an argument shall be inserted. Positional Arguments If a formatting string contains {}, every instance will use the next argument, as given to the function. You can also use {n} with n being 0 to 9 to insert the n-th argument. This allows you to skip, rearrange, or repeat arguments: Print(\"Arg1: {1}, Arg2: {2}, Arg1: {1}\", \"zero\", \"one\", \"two\"); Formatting Options Most types that can be formatted, can be passed in directly: Print(\"int value is {}\", 23); However, often you may want to specify exactly how to display the value. To do so, you need to wrap the incoming argument in an option struct. Each option struct can have arbitrary parameters to configure how it works. Print(\"HEX: 0x{}\", ezArgU(value, 8, true, 16, true)); ezArgU is an option struct for unsigned int values. Here we specify that the output should have a fixed width of 8 characters, should pad the output with zeros if necessary, use base 16 (hex) and upper case letters. There are many such option structs available, each with their own parameters. By convention, all formatting option structs are named ezArgXYZ. Available Option Structs This is a not necessarily complete list of available option structs: ezArgC - for single characters ezArgF - for floating point values ezArgI - for int values ezArgU - for unsigned int values ezArgP - for pointers ezArgErrorCode - for Windows error codes ezArgDateTime - for ezDateTime ezArgHumanReadable - for shortening numbers with suffixes (such as K (kilo) and M (mega)) ezArgFileSize - for shortening file sizes and use suffixes (such as KB and MB) Custom Argument Formatters You can easily write your own formatter. The formatting work is done by a free function called BuildString, overloaded for the type that it shall format. If you search for BuildString functions, you will find many overloads, each handling a different type. Please look at those functions to see how to write your own formatter. For it to work, all that is necessary, is that your code is #include'd when it is used in a format string. If you try to use a type (such as MyType) in a format string and your custom formatter is not known (#include'd) in that cpp file, you will get a compile time error with a very long message telling you that no overload of of BuildString is available to handle this type. The ezArgXYZ structs are just used to wrap a type and store additional options. This is not required, for BuildString to work, but it does allow you to wrap the same type multiple times to select different formatters. For example, unsigned int is wrapped by ezArgU for regular int formatting options, by ezArgFileSize to print a value with B, KB, MB or GB suffixes and by ezArgErrorCode to interpret it as a Windows error code and translate it to a readable string. If you have a custom type MyType and you do not need any formatting options, you can write a BuildString overload, that takes a MyType directly. See Also String Usage Guidelines"
  },
  "pages/docs/appendix/string-usage.html": {
    "href": "pages/docs/appendix/string-usage.html",
    "title": "String Usage Guidelines | ezEngine",
    "keywords": "String Usage Guidelines All strings in the ezEngine are Utf8 encoded. As such they are accessible as simple const char* arrays. However, one has to be very careful to distinguish between the \"Element Count\" (the number of bytes in the string) and the \"Character Count\". Utf8 is a variable length encoding, which means that all ASCII characters (the first 127 characters in the ANSI set) are encoded with one byte. Everything else can take between two and four bytes per character. Therefore it is very dangerous to apply an algorithm to every byte in a string, as it might actually need to be aware of the individual characters. Therefore all the string classes provide lots of high-level functionality for modifying the strings, which take care of these things internally. On the other hand, direct access to the char array is not granted, to ensure the encoding does not get corrupted. The following classes provide functionality to work on char* arrays: ezUnicodeUtils ezStringUtils ezPathUtils These generally implement 'read-only' functionality. The actual string classes provide the same functions, but with more comfortable interfaces. Note that many of these functions take an 'end' pointer. This can generally be ignored, if you are working on zero terminated strings. It can however also be used to work on non zero-terminated sub-strings. How to access C strings Working on Utf8 strings is difficult, as you usually want to work on a per-character basis, which does not have a 1:1 mapping to bytes, when the data is Utf8 encoded. To make this possible, use ezStringIterator. ezStringIterator provides an interface on top of a char array, that allows to iterate forwards and backwards through a string on a per-character basis. You can access the current character and get it in Utf32 encoding. This can easily be compared with standard C++ char-constants like 'a', 'b', '\\', '\\n', '\\0' etc. If you want to work on a per-character basis on a char that might be Utf8 encoded, you should always wrap it inside an ezStringIterator and then use the iterator instead of accessing the array directly. Note that it only provides read-access though, as you cannot modify a Utf8 string in place in all circumstances. Additionally there is ezStringView, which allows to only work on sub-strings of other strings, which can be used to implement parsing functions very efficiently. How to create and modify strings When you need to modify or build strings, use ezStringBuilder. ezStringBuilder provides a large set of functions to easily modify strings. It includes a set of dedicated 'path functions' for working with paths (to prevent duplicate slashes, extract certain information etc.). ezStringBuilder uses an internal cache of 128 bytes, which is allocated on the stack. That means working with local ezStringBuilders is very efficient, as usually no memory allocations are required, unless you build very long strings. That in turn means, that you should never use ezStringBuilders for storage, as they will waste a lot of memory. ezStringBuilders should nearly always be local variables with a short life span. How to store strings For storing strings, use ezString. ezString is a typedef for ezHybridString<32>. That means it has an internal cache for storing up to 32 bytes without the need for memory allocations. That usually covers more than 90% of all strings, which means that memory allocations are rare, but the amount of wasted memory is also relatively low. ezString does not provide functions to modify the string (other than completely replacing it), use ezStringBuilder in such cases. If you know that you are storing strings of certain lengths, e.g. filename extensions, you can use other predefined ezStrings, such as ezString8 or ezString48 to tune the internal cache size to be more fitting. How to convert strings between Utf8, Utf16, Utf32 and wchar_t ezEngine provides a few classes to enable converting to and from all the Utf encodings and wchar_t encoding (which is simply Utf16 one some platforms and Utf32 on others). The following classes take strings encoded in any encoding and convert them into their target encoding: ezStringWChar ezStringUtf8 ezStringUtf16 ezStringUtf32 Ie. an instance of ezStringWChar will always encode a string to (the platform specific) wchar_t encoding and instances of ezStringUtf8 will convert strings to Utf8 encoding, etc. Use it like this: ezStringUtf8 MyUtf8 = L\"My wchar_t string\"; // The 'L' makes it a 'wide-string', ie. a wchar_t array printf(\"Output Utf8: %s\", MyUtf8.GetData()); or for interacting with win32 functions: ezStringWChar FileNameW = MyUtf8FileName.GetData(); DeleteFileW(FileNameW.GetData()); This allows to quickly and easily convert back and forth between the different encodings. Just make sure that you convert your data to Utf8 when you store something inside the Engine, and to the platform-specific encoding (typically wchar_t) when interacting with OS functions (unless they support Utf8 as well). See Also Container Usage Guidelines String Formatting"
  },
  "pages/docs/appendix/third-party-code.html": {
    "href": "pages/docs/appendix/third-party-code.html",
    "title": "ThirdParty Code and Data | ezEngine",
    "keywords": "ThirdParty Code and Data This page lists which third party code and data is used by EZ. Important: Before you distribute any project, please check the licensing conditions for all used components. The list below tries to be exhaustive and up-to-date for components directly used by EZ, but this is only provided for your convenience and we give no guarantee for correctness. It is still your responsibility to make absolutely certain that your project doesn't violate any licensing conditions from third-party components used directly or indirectly in your project. Assimp Link: http://www.assimp.org Compile switch: None (hard dependency for the asset processing) Open Asset Import Library, a portable Open Source library to import various well-known 3D model formats in a uniform manner. cc0Textures Link: https://cc0textures.com CC0 Textures offers a library containing hundreds of detailed PBR materials with displacement-, normal- and roughness maps for photorealistic rendering. All assets are available for free and without any restrictions. cgbookcase Link: https://cgbookcase.com cgbookcase provides hundreds of high quality, PBR textures. All textures on cgbookcase.com are licensed as CC0. Dear Imgui Link: https://github.com/ocornut/imgui Compile switch: EZ_3RDPARTY_IMGUI_SUPPORT A nice library for easily creating ingame GUIs. DirectXTex Link: https://github.com/Microsoft/DirectXTex Compile switch: Currently none Used by ezImage and the ezTexConv tool for GPU-enabled block compression. Duktape Link: https://duktape.org Compile switch: EZ_3RDPARTY_DUKTAPE_SUPPORT Duktape is an embeddable Javascript engine, with a focus on portability and compact footprint. It can be used directly or through ezDuktapeWrapper. Non-essential for the engine, but scripting functionality (using TypeScript) is built on top of it. Enet Link: http://enet.bespin.org Compile switch: EZ_3RDPARTY_ENET_SUPPORT An efficient and easy to use networking library, built on top of the UDP protocol. It is used by ezTelemetry to interact with the ezInspector, and it is also used to implement the file serving functionality. FleetOps Link: https://www.fleetops.net Some assets were kindly provided with permission to use and redistribute, by the awesome team behind the FleetOps project. Thanks so much guys! FMOD 2.x Link: https://www.fmod.com Compile switch: EZ_BUILD_FMOD EZ has an integration for the FMOD sound system. Important: FMOD is a commercial product and you may need to buy a license to use it in your project. FreeSound Link: https://freesound.org Freesound is a collaborative database of Creative Commons Licensed sounds. jc_voronoi Link: https://github.com/JCash/voronoi/blob/dev/src/jc_voronoi.h Compile switch: None A fast single file 2D voronoi diagram generator. Used by ezBreakableSheetComponent. Jolt Physics Link: https://github.com/jrouwe/JoltPhysics Compile switch: EZ_3RDPARTY_JOLT_SUPPORT Jolt Physics is a free and open source physics engine. It is used to provide collision detection, physics simulation, character controllers and other interactions. Kenney.nl Link: kenney.nl Kenney provides thousands of textures, 3D models and sound effects under a generous public domain license. Some of them are used in our sample projects. Lua Link: (http://www.lua.org Compile switch: EZ_3RDPARTY_LUA_SUPPORT The Lua scripting language. Can be used directly or through ezLuaWrapper for easier access to common functionality. Non-essential for EZ, only the ingame console interpreter would stop working without it. Mikktspace Link: http://mmikkelsen3d.blogspot.ie Compile switch: None (hard dependency for the asset processing) Tangent space generation code by Morten S. Mikkelsen. See https://wiki.blender.org/index.php/Dev:Shading/Tangent_Space_Normal_Maps for more information. It is used by ezGeometry. Ozz Link: http://guillaumeblanc.github.io/ozz-animation Compile switch: None Used as the basis for skeletal animations. Both during asset import (to build an optimized skeleton structure) and at runtime for animation playback. PhysX 4.1.1 Important: NVIDIA PhysX in EZ is no longer maintained. The code is currently still there, but most likely doesn't compile without errors anymore. Link: https://github.com/NVIDIAGameWorks/PhysX Compile switch: EZ_BUILD_PHYSX NVIDIA PhysX is used to provide collision detection, physics simulation, character controllers and other interactions. To build PhysX yourself: Checkout https://github.com/NVIDIAGameWorks/PhysX.git Goto physx/buildtools/presets/public and open all presets that you want to build and change or add <cmakeSwitch name=\"NV_USE_STATIC_WINCRT\" value=\"False\" comment=\"Use the statically linked windows CRT\" /> <cmakeSwitch name=\"NV_USE_DEBUG_WINCRT\" value=\"True\" comment=\"Use the debug version of the CRT\" /> Run physx/generate_projects.bat for every configuration you want to build Open physx/compiler/.../PhysXSDK.sln and compile CMakePredefinedTargets/INSTALL for both debug and release Uwp installs are missing two include folders: PhysX/include/cudamanager and PhysX/include/gpu so copy those from physx/include to physx/install/.../PhysX/include The content of the built configuration in physx/install is now ready to be consumed by ezEngine by pointing the advanced cmake var EZ_PHYSX_SDK to it. Important: Depending on how you use PhysX, you may need to acquire (buy) a license for it from NVIDIA. PolyHaven Link: PolyHaven.com Poly Haven is a small company based in South Africa, working with artists around the world, with the goal to create a constantly growing community-funded resource of open content (CC0 license). Some of PolyHaven's assets are used in our sample projects. Qt 5 Link: https://www.qt.io Compile switch: EZ_ENABLE_QT_SUPPORT Used for all desktop GUI code in the editor and tools. Important: Depending on how you use Qt, you may need to acquire (buy) a license for it. See https://www.qt.io/terms-conditions/. Qt Advanced Docking System Link https://github.com/githubuser0xFFFF/Qt-Advanced-Docking-System Compile switch EZ_3RDPARTY_ADS_SUPPORT A docking system for Qt similiar to the one in visual studio. Used by the editor and inspector applications. Recast Compile switch: EZ_3RDPARTY_RECAST_SUPPORT Link: https://github.com/recastnavigation/recastnavigation A library to generate navigation meshes from arbitrary 3D geometry. RenderDoc Link: https://renderdoc.org Compile switch: EZ_3RDPARTY_RENDERDOC_SUPPORT RenderDoc is a free MIT licensed stand-alone graphics debugger. The ezRenderDocPlugin enables full control over taking RenderDoc snapshots from within the engine. RmlUi Link: https://mikke89.github.io/RmlUiDoc/ RmlUi is the C++ user interface package based on the HTML and CSS standards, designed as a complete solution for any project's interface needs. It is a fork of the libRocket project, introducing new features, bug fixes, and performance improvements. SFML Link: http://www.sfml-dev.org Compile switch: currently none (TODO) This library provides a simple and portable interface for window creation, input handling and more. Used by ezWindow and ezStandardInputDevice on non-Windows platforms (Mac, Linux). Silk Icons Link: http://www.famfamfam.com/lab/icons/silk Icons from this set were extensively used in the past and may still be used by some of the tools. Sonniss Link: https://sonniss.com Sonniss is a premium distribution platform for high-quality sound effects libraries. Sounds distributed from Sonnis are taken from the GameAudioGDC bundles. See Data\\Content\\Sound\\FmodProject\\Assets\\Sonnis\\Licensing.pdf for details. stb Link: https://github.com/nothings/stb Compile switch: None Public domain licensed code by Sean Barrett. Used by ezImage to read and write some of the supported formats like PNG and JPEG. SVG Repo Link: https://www.svgrepo.com Most of the SVG icons used have been taken from this website. All icons taken are free for commercial use, but some require attribution. The following is a list of all icon sets with and without attribution licenses that we know are used by our tools. If you notice that we forgot to mention a set, please contact us. https://www.svgrepo.com/collection/zwicon-line-icons https://www.svgrepo.com/collection/atomicons-interface-line-icons https://www.svgrepo.com/collection/bigmug-interface-icons https://www.svgrepo.com/collection/rpg-game-filled-icons https://www.svgrepo.com/collection/colour-creative-oval-line-icons https://www.svgrepo.com/collection/dazzle-line-icons https://www.svgrepo.com/collection/game-skills-vectors https://www.svgrepo.com/collection/gentlecons-interface-icons https://www.svgrepo.com/collection/solar-broken-line-icons https://www.svgrepo.com/collection/untitled-ui-oval-interface-icons https://www.svgrepo.com/collection/science-bold-flat-vectors https://www.svgrepo.com/collection/tetrisly-interface-icons https://www.svgrepo.com/collection/smoothie-line-icons https://www.svgrepo.com/collection/lightning-design-utility-icons https://www.svgrepo.com/collection/iconsax-line-oval-icons https://www.svgrepo.com/collection/simple-line-icons https://www.svgrepo.com/collection/web-design-development-6 https://www.svgrepo.com/collection/codicons-coding-icons https://www.svgrepo.com/collection/neuicons-oval-line-icons https://www.svgrepo.com/collection/metrize-circled-icons https://www.svgrepo.com/collection/maki-filled-ui-icons https://www.svgrepo.com/collection/framework7-line-icons https://www.svgrepo.com/collection/nature-and-animals-infographic-icons https://www.svgrepo.com/collection/nonicons-programming-icons https://www.svgrepo.com/collection/kalai-oval-interface-icons https://www.svgrepo.com/collection/line-awesome https://www.svgrepo.com/collection/transportation-icooon-mono-vectors https://www.svgrepo.com/collection/iconcino-interface-icons https://www.svgrepo.com/collection/minimal-ui-icons https://www.svgrepo.com/collection/design-20 https://www.svgrepo.com/collection/location-compilation https://www.svgrepo.com/collection/variety-flat-bordered-icons https://www.svgrepo.com/collection/wellness-line-craft https://www.svgrepo.com/collection/zest-interface-icons/ https://www.svgrepo.com/collection/mingcute-tiny-bold-line-icons https://www.svgrepo.com/collection/fluent-ui-icons-filled https://www.svgrepo.com/collection/iconsax-bold-oval-icons https://www.svgrepo.com/collection/variety-duotone-filled-icons https://www.svgrepo.com/collection/vaadin-flat-vectors https://www.svgrepo.com/collection/calcite-sharp-line-icons https://www.svgrepo.com/collection/design-collection https://www.svgrepo.com/collection/baby-19 https://www.svgrepo.com/collection/ecology-elements-line https://www.svgrepo.com/collection/adverse-phenomena https://www.svgrepo.com/collection/startup https://www.svgrepo.com/collection/nature-icon-collection https://www.svgrepo.com/collection/sign-and-symbols-icooon-mono-vectors https://www.svgrepo.com/collection/industrial-sharp-ui-icons https://www.svgrepo.com/collection/carbon-design-line-icons https://www.svgrepo.com/collection/nuiverse-sharp-interface-icons https://www.svgrepo.com/collection/vscode-icons https://www.svgrepo.com/collection/hashicorp-line-interface-icons https://www.svgrepo.com/collection/wave-oval-interface-icons UTF8-CPP Link: https://github.com/nemtrif/utfcpp Compile switch: None A library that provides Unicode related functionality. Integrated directly into ezFoundation. V-HACD Link: https://github.com/kmammou/v-hacd Compile switch: EZ_3RDPARTY_VHACD_SUPPORT The \"Volumetric Hierarchical Approximate Convex Decomposition\" library is used to decompose a concave triangle mesh into multiple convex pieces. This allows you to generate complex collision meshes which can be used as the shapes of dynamic actors. xxHash Link: https://github.com/Cyan4973/xxHash Compile switch: None A very fast hash algorithm. Integrated directly into ezFoundation. zLib Link: http://www.zlib.net Compile switch: EZ_3RDPARTY_ZLIB_SUPPORT Provides algorithms for zip compression and decompression. It is used by ezCompressedStreamReaderZlib and ezCompressedStreamWriterZlib in ezFoundation. zstd Link: https://facebook.github.io/zstd Compile switch: EZ_3RDPARTY_ZSTD_SUPPORT A very fast lossless compression library. It is used by ezCompressedStreamReaderZstd and ezCompressedStreamWriterZstd and also by ezArchive."
  },
  "pages/docs/assets/asset-browser.html": {
    "href": "pages/docs/assets/asset-browser.html",
    "title": "Asset Browser | ezEngine",
    "keywords": "Asset Browser The asset browser is used for selecting and opening asset documents. Assets can be filtered by type and path or searched for by name. If the asset browser panel is not visible, select Panels > Asset Browser. You will notice that the asset browser will show up in two modes. The panel is constantly visible and can be interacted with at all times, to search for assets and open them via double click, or instantiate them with drag and drop. Additionally, when an object has an asset reference property, choosing Select Asset from the button menu right next to it will open another asset browser in file picker mode. Several other options are available, for example, Open Asset allows you to open the referenced asset document, which can be useful to follow a chain of asset references. File Display The asset browser can show all files that are on disk in the selected data directories. It distinguishes between assets, importable files and non-importable files. Assets are typically shown with a thumbnail or dedicated icon. These are the files you work with most. Importable files are files that can be imported as an asset. Right click such a file and select Import... to do so. If an importable file is not referenced by any asset, it is color-coded orange. Otherwise it shows up white and its context menu contains the sub-menu Imported via which lists all the assets that reference this file. Non-Importable files are all other files. The editor can't do anything with them, other than to open the default application associated with this file type. You can also move and delete files and assets. Note that deleting a file moves it to the trash folder, so you can restore the file through the operating system functionality, if necessary. Search Field The Search field in the top-left corner allows you to search for assets by name, path and GUID. The search by path or name is case insensitive. For paths, both slashes and backslashes are allowed. You can also input the GUID of an asset (for example { 1c47ee4c-0379-4280-85f5-b8cda61941d2 }). Advanced Search The Search field supports special keywords to find assets by certain criteria. Find Asset References (ref and ref-all) It can be very useful to know in which assets one particular asset is used. By typing ref:{asset-guid} into the search field, the asset browser will display all assets that directly reference that particular asset themselves. Using ref-all:{asset-guid} will display also all assets that are indirectly dependent on that asset. However, it is much more convenient to just right-click an asset and select Find all references to this asset. This will fill out the search field accordingly: Filter by Asset Type Below the search field is a combo box containing all asset types. Select one to only show assets of this type. This is also where you select to show all files or importable file types. Filter by Folder On the left the asset browser displays all data directories. When you select a folder in this tree view, the asset browser only displays assets located below that folder. Show Assets in Sub-Folders Right click on a folder and toggle Show items in sub-folders. When enabled (the default) the asset browser shows all assets that are anywhere below a selected folder. When disabled, only the assets that are directly inside the a selected folder are shown. Assets that are in a sub-folder are not displayed. This behavior is more like a typical file explorer. Filter to this Path You can right-click on any asset and select Filter to this Path to set the folder filter to the path in which the selected asset resides. This is useful when you already see an asset in the browser but are interested in an asset that you know is located next to that asset (same folder or sub-folder). Hidden Folders By default when an asset is located in a folder with a _data suffix (e.g. 'MyAssets_data'), it is not displayed in the asset browser, unless you select exactly that folder in the asset browser. This way you can easily hide assets that are rarely needed. Hidden folders are indicated with a grey font. A common use case is to put materials and textures that are very specific for a mesh or prefab into a xyz_data subfolder to group the data together, but prevent it from cluttering the asset browser. You can toggle this feature by right clicking any folder and selecting Show items in hidden folders. Create Asset Documents You can create new asset documents by right clicking a folder on the left or the empty area of a folder on the right or an existing asset on the right and then selecting New > Asset Type. The advantage over creating a document via File > Create... is that the create file dialog opens directly in the location of the selected asset or folder, which makes it easier to create a new asset next to an existing asset. Display Assets in Recently Used Order The editor remembers which assets you used recently. The asset browser can list recently used assets at the top. This option can be toggled from the context menu on the right hand side of the asset browser. Note that the state of this option is remembered separately for the asset browser panel and for the asset browser when used as a file picker. In panel mode, it is typically disabled and all assets are sorted alphabetically, in file picker mode, it typically sorts by recently used time. Copy Asset Guid In rare cases you may need the internal GUID (Globally Unique Identifier) of an asset. You can easily copy it to the clipboard by right clicking an asset and selecting Copy Asset Guid. Transform Assets The asset browser allows you to quickly transform assets in multiple ways: Transform All: In the toolbar of the asset browser there is a button of a white box with a red arrow. When you press this button all assets that are not up-to-date get transformed (ie. their runtime data is created from the source input data). Transform Selected: Select one or multiple assets in the asset browser, right click and choose Transform to update only the selected assets. Transform Single: You can also quickly transform just a single asset by clicking the icon overlay at the bottom right of an asset's thumbnail (usually a checkmark or a gear). Resave all Asset Documents The rightmost button in the asset browser's toolbar triggers an action to open each and every document, save it and close it again. This can be used to migrate all documents to the very latest version. Since document versioning is very robust, there is little practical use for this operation, though. Check Filesystem The leftmost button in the asset browser's toolbar makes the editor check the filesystem for changes that were missed by the automatic filesystem watcher. This may be useful when you added or removed assets on disk and the changes are not reflected in the editor. Background Processing and Transform State At the bottom right of the asset browser there is a widget with a colored progressbar and a play button. The play/pause button is for switching background processing on and off. If enabled, outdated assets are automatically transformed in the background. The progressbar displays how many assets need updating and whether there were any errors. For details about processed assets and potential errors, check out the asset curator. If you do not want assets to be transformed automatically, disable background processing with the pause button. Drag and Drop You can drag assets from the asset browser into other documents, such as scenes. For mesh and prefab assets this will instantiate the asset (ie. create a new node that references the asset). For materials this may assign the material to the object that you drag it onto. Not all asset types support drag and drop. Also dragging an asset into the 3D viewport can have a different effect than dragging it into the scene tree. Video See Also Asset Curator Editor Documents Asset Import"
  },
  "pages/docs/assets/asset-curator.html": {
    "href": "pages/docs/assets/asset-curator.html",
    "title": "Asset Curator | ezEngine",
    "keywords": "Asset Curator The asset curator panel is a tool to help find and fix problems with assets. If the panel is not visible, use Panels > Asset Curator to open it. Activity At its top the asset curator panel displays an activity log. If background transform is active, this shows which assets have been transformed recently. Whether background transform is enabled can be seen from the progress bar at the bottom: Transform Issues If an asset fails to transform for some reason, it will be listed in the view to the bottom left. The most common issue is a missing file reference. For example when a texture source file has been moved or renamed, the texture asset can't find it anymore and thus fails to transform. When you select an asset from that list, the log at the bottom right will display any error message from the failed transform. Double click the asset to directly open the document. If Show Indirect Issues is disabled (the default), only assets that have problems finding their source files are displayed. Otherwise all assets which failed to transform are displayed, however, most of them will be follow-up issues due to other assets being incomplete. Once you fix an asset and make sure it is transformed, the asset curator will no longer show it in the issues list. Video See Also Asset Browser Assets"
  },
  "pages/docs/assets/asset-profiles.html": {
    "href": "pages/docs/assets/asset-profiles.html",
    "title": "Asset Profiles | ezEngine",
    "keywords": "Asset Profiles Asset profiles are used to set platform-specific, project-wide configurations. For example, on PC a different rendering pipeline may need to be used than on Android. Under Project > Project Settings > Asset Profiles you can open the Asset Profiles dialog. The list on the left hand side lists all the asset profiles that are defined for this project. You can add as many as you like, which allows you to switch between different configurations even on the same platform. For the selected profile, the right hand side shows the available options. In the top right you can select for which target platform this profile is meant. Asset Profile Configs Below that the available asset profile configs are listed. These can affect various aspect of the engine: Render Pipelines Here you can specify what the default render pipeline (TODO) should be. Different platforms have different rendering capabilities, so you may want to use a different pipeline to target that hardware best. Additionally you can add render pipelines to be used for certain use cases. For example in the screenshot above an extra Camera pipeline was added, which uses a simpler render pipeline. This can then be selected on a camera component, in case it is used for render to texture (TODO). Again, you can define per platform, which pipeline to use. Texture Assets This configuration specifies how to transform texture assets. At the moment it only allows you to clamp the maximum resolution. XR Config On platforms where XR is supported, this configuration allows you to enable and configure XR support. Switching the Active Profile In the editor there is always one profile active. Thus systems that can be configured through asset profiles will read their settings from this active profile and change their behavior. Additionally, some assets produce platform specific output. For example textures may use different file formats or different transform options. The active profile determines which output to use. For example if you add a PC profile with a very low maximum texture size (say 32) and then activate that profile, the editor will switch to those assets. See Also Assets Render Pipeline (TODO) Supported Platforms"
  },
  "pages/docs/assets/assets-overview.html": {
    "href": "pages/docs/assets/assets-overview.html",
    "title": "Assets | ezEngine",
    "keywords": "Assets Assets are the most common type of documents in the editor. Assets represent data that usually comes from some source file (like an image or a model) and must be processed into an optimized format for the engine to use at runtime. This processing step is called asset transformation. A good example are textures. Textures come in many different source formats, such as TGA, JPG, PNG, and so on. Texture data in these formats is not suited to be loaded directly by the engine. Instead, it must be encoded and compressed in formats that GPUs can decode efficiently. This step can be very time consuming and should therefore be done up front. Additionally, textures should contain mipmaps and may need to be downscaled for different platforms. Exactly how a texture should be transformed is something that you may want to have full control over, so you need some way to configure this. Therefore, instead of loading a texture directly into the engine, you need to create a texture asset in the editor. This document will reference one or multiple source files and allow you to configure the asset transform. When the asset gets transformed it will output an optimized file that the engine can then load and use efficiently. Types of Assets One can distinguish between two types of assets: Assets that mostly exist to transform existing data from a source format into an optimized format, and assets that represent entirely new data, authored in the editor. Examples for the former are textures, meshes, collision meshes, sounds and so on. Their purpose is to ensure that the engine does not need to handle all sorts of source formats, but only optimized runtime formats. Instead, the editor and other tools will deal with the source formats, and allow the user to configure this conversion step. Examples for the latter are scenes, prefabs, materials, property animations (TODO), curves and so on. Their data does not come from some other file on disk, but is instead built entirely in the editor. However, they still need to be transformed to provide the engine with an optimized format. Creating Assets The straight forward way to create an asset document is through the menu File > Create.... This gives you a blank asset and you can (and must) fill out all properties manually. For common types of assets there is a more convenient way to quickly fill out the common properties. See the asset import documentation for details. Asset GUID The editor references assets not by file path, but by GUID (Globally Unique IDentifier). Each asset is assigned a GUID upon creation and the GUID never changes. That means an asset document can be renamed and moved to a different location on disk, and the editor will continue to find it. Similarly, the engine runtime will also locate the transformed asset files through the asset GUID (the file system takes care of translating a GUID to an actual path). This makes reorganizing the file structure easy and resilient to errors. For the rare case that you need to know the actual GUID of an asset, you can right click any asset document and select Copy Asset GUID. The only caveat is, that you should never duplicate an asset by actually copy and pasting an asset file, as this would result in two assets with the same GUID. The editor will try to detect and fix such cases, but it may not work out the way you planned. Instead use the File > Save as... functionality to create a copy of an asset with a different name. This will assign a new asset GUID to the new asset document. Video: How to copy asset documents Asset Browser All assets are listed in the asset browser. Asset Transform Asset documents must be transformed to produce the actual runtime data that the engine uses. In an open asset document you can either press CTRL+E or click the green arrow button in the toolbar to export (transform) the asset. To transform all assets in a project, open the asset browser and click the Transform All button there. Optionally, you can also enable background asset processing. Asset Errors An asset can be in an error state. The most common reason for this is, that it references files or other assets that don't exist (anymore). In this case the asset cannot be transformed correctly and will therefore not produce any new output. All erroneous assets are listed in the asset curator. The curator panel will also show error log output for those assets. A common problem is, when you moved an asset document to a new location, you may also need to adjust the path to input files, such as the source texture or model data. Another problem are deleted assets, or missing assets because of a different data directory setup. Asset Profiles Assets can produce different, platform specific output, depending on which platform they are being transformed for. That means a texture may, for example, generate a runtime file that contains full resolution 4K textures for PC, but only limited 1K resolution textures for mobile devices. Such platform specific options can be configured through asset profiles. For some types of assets, such platform specific settings may also be handled externally, for example FMOD already deals with platform specific audio encoding on its own. Assets and Resources The term asset mostly refers to the editor side and the editor documents. When an asset gets transformed, it generates the data representation for the runtime side. Inside the engine this data will be read into a resource by the resource manager (TODO). Assets and resources are conceptually two different things. Assets always live on the editor side, resources always on the runtime side. Their code is strictly separate. Resources can be loaded from files or procedurally generated at runtime. The files that they load can come from anywhere and there is no requirement that those files are created through assets. However, assets are the most common and most convenient way to generate the runtime data. You could replace the entire asset management system with a custom system, though. The editor may be the most convenient way to transform assets from source format to runtime format for most scenarios, but if you have a special use case, you could built a completely custom asset processing pipeline and ignore the editor entirely, there is no 'secret sauce' in the editor that is required to make the runtime work. Consequently, when the documentation mentions 'assets', it always refers the data and behavior in the editor, and when it mentions 'resources' it always refers to data that is used by the runtime side (the renderer, the physics engine, the game logic, etc). When you work with the editor, the two code paths are even separated into different processes: Editor.exe and EditorEngineProcess.exe Video See Also Editor Documents Asset Browser Asset Curator Asset Profiles Asset Import"
  },
  "pages/docs/assets/import-assets.html": {
    "href": "pages/docs/assets/import-assets.html",
    "title": "Asset Import | ezEngine",
    "keywords": "Asset Import All assets are represented by documents. That means to get a texture into the engine, you need a texture document which describes which source files (png, jpg, etc) are used to create the texture and how they shall be imported. This is where you configure such things as, whether to use compression, whether an alpha channel should be present and so on. Other asset types of course have other options for importing. To create these documents you have two options: Manual or automatic import. Create Documents Manually You can manually create documents via File > Create.... In the file creation dialog, choose the file extension for the desired asset type and specify where to store the document. The newly created document will be in a blank state. You will need to fill out all the properties, including where the source files are located. This method always works for all asset types and for some types it is the only way. Since this method always involves multiple, mostly simple steps, it can become tedious. Therefore, some asset types provide a way to automate much of this process. Create Documents Automatically For asset types that are mostly defined by a single source file (e.g. textures and meshes), the editor often provides an importing method that automates most of the trivial setup. The most convenient way is to find the source asset file in the asset browser (ensure it shows importable files), right-click on it and select Import As. This sub-menu makes it possible to quickly import one or multiple assets as a specific asset type. It does so fully automatically. If you want to have a few more options, select Import... instead. Another method is to select Project > Import Assets... or press CTRL+I to open a file browse dialog. Navigate to the file(s) that you want to import and select them. If you want to know which asset types are currently supported for automatic import, you can open the dropdown with the allowed file extensions here. Afterwards, you will be presented with a table of options how to import the selected files: Here we selected four files for import. One .obj file and three .jpg files. The automatic import uses heuristics to make an educated guess how to import certain source files. Here it already suggests to import the \"_col.jpg\" file as a diffuse texture, the \"_nrm.jpg\" file as a normal map and so on. If the heuristic is incorrect, you can use the dropdown on the left to fix it. It also suggests the target document file name. You can either click Browse or double click into the text field to change the target file name. Some source files can be imported in multiple ways. For example the .obj file could be imported as a mesh for rendering or as a mesh for physics. Often you want to import the same mesh for both, so you want two asset documents (a Mesh and a Collision Mesh) which reference the same input file. Therefore this table lists the .obj file twice but with different import options in the dropdown box. If, for example, you do not want a mesh to be imported as collision mesh, at all, you can just select No Import from the respective dropdown. Once you click Import the asset documents are generated and you can then open them. If background asset processing is enabled, the editor will already start transforming the asset data. Otherwise open each document and click the button of a white box with the green arrow to manually trigger the asset transform. The automatic import creates the documents using a set of rules to fill out its properties, depending on the template that you selected for it. So for example an image imported as a \"diffuse texture\" and one imported as a \"normal map\" are mostly the same, except that a few options are already configured in a certain way for you. You should review all options for correctness afterwards. Video: How to import textures Video: How to import meshes See Also Assets Asset Browser"
  },
  "pages/docs/build/build-android.html": {
    "href": "pages/docs/build/build-android.html",
    "title": "Building for Android | ezEngine",
    "keywords": "Building for Android Prerequisites You need the following to build for Android: Android SDK Platform 6.0 (Marshmallow) API-Level 23 Android NDK 21 or higher Android SDK Tools Android SDK Platform-Tools Java (JRE) Ninja Android Emulator (optional) Visual Studio 2019 Ninja is a build generator used by CMake and needs to be added to the PATH environment variable. The easiest way to install the Android components is to download Android Studio and then to select these from the SDK Manager. Once installed, the following environment variables need to be set: Change the version to reflect the one you are using. ANDROID_NDK_HOME needs to point to your installed version, by default this is: C:\\Users\\[USERNAME]\\AppData\\Local\\Android\\Sdk\\ndk\\[VERSION] ANDROID_HOME needs to point to your installed version, by default this is: C:\\Users\\[USERNAME]\\AppData\\Local\\Android\\Sdk JAVA_HOME needs to point to a java runtime. Android Studio has its own version so there is no need to download it separately: C:\\Program Files\\Android\\Android Studio\\jre Visual Studio Open Folder While you can manually run CMake to use the ninja generator, a more convenient solution is to use Visual Studio's open folder functionality. Go to File>Open>Folder... and select the root of the repository. The CMake settings used by this feature are stored inside the CMakeSettings.json file in the root folder (CppProperties.json contains additional information for VS). Note that if a different API level is used, the ANDROID_PLATFORM parameter in the CMakeSettings.json has to be changed for all configurations. If all environment variables were set correctly VS should automatically configure CMake. Once done, a drop down appears in the VS toolbar, allowing you to select the configuration, e.g. Android-x86-Debug. Once changed, VS will start to configure CMake again for the new configuration. Next, select a build target, e.g. libFoundationTest.so which are the foundation unit tests. Note that you can only select applications, not all libraries here. Setting up an Emulator Open Android Studio, go to Configure>AVD Manager and select Create Virtual Device. Download the Pixel 2 x86 image (or any other compatible one). Next, select Pie (API 28) (29 is broken as of writing). In Advanced Settings go to Emulated Performance and select Cold boot as the other options tend to hang and the only option then is to reset the image to factory defaults. Debugging Code Before debugging it should be ensured that you have a emulator setup or a device connected. There should only be one device or emulator. Otherwise debugging is going to fail because it's unknown which target to use. $ adb devices List of devices attached ce11171b5298cc120c device If adb is not available in the command line %ANDROID_HOME%\\platform-tools needs to be added to the PATH environment variable. To debug & deploy select one of the launch targets which are prefixed with '>'. These have been configured in the launch.vs.json file for debugging. By hitting the \"start debugging\" button in visual studio, as usual, the app will be installed on the connected device / emulator, started and the debugger will be attached. The app can then be debugged by using the Visual Studio UI the same way as for an windows based C++ project. Troubleshooting Debugging / Command Line Debugging If debugging doesn't work or debugging from the command line is preferred, the command line debugger can be started. It gives detailed output. The debugging script is located in Utilities\\DbgAndroid.ps1 The following parameters are present: parameter meaning PrintCmds Prints all commands that are run. Useful for debugging issues. packageName The name of the android package. For example \"com.ezengine.FoundationTest\". All ezEngine package names start with \"com.ezengine.\". If the package name is not known the .apk file can be opened with a zip tool. Then inspect the AndroidManifest.xml. originalSoDir The location where the shared object (.so) that contains all the binary code is located: Output\\Lib\\AndroidNinjaClang(Debug|RelWithDebInfo|Release)(arm32|arm64|x86|x64) arch The architecture of the app you want to debug usually \"arm\",\"arm64\",\"x86\" or \"x86_64\" apk The apk to install on the device before starting debugging. This parameter is optional. If not given no apk will be installed and it is assumed that the apk was already installed on the device For example command line debugging the FoundationTest: Utilities\\DbgAndroid.ps1 -packageName \"com.ezengine.FoundationTest\" -arch arm -apk \"Output\\Lib\\AndroidNinjaClangDebugArm32\\FoundationTest.apk\" -originalSoDir \"Output\\Lib\\AndroidNinjaClangDebugArm32\" See ezEngine log output To see the ezEngine log output the following logcat filter can be used. adb logcat ezEngine:D *:S See Also Building ezEngine"
  },
  "pages/docs/build/build-linux.html": {
    "href": "pages/docs/build/build-linux.html",
    "title": "Building for Linux | ezEngine",
    "keywords": "Building for Linux Linux support for ezEngine is currently in development and still to be considered experimental and incomplete. You can try it, but don't expect to be able to work productively with it. For rendering the new Vulkan backend is used, which itself is also very much in development yet. We welcome help finding and fixing issues. Supported Compilers / Make Systems The ezEngine CMake scripts support the following compilers when building for Linux: GCC Clang C++17 support is required, so make sure that your respective compiler supports it. These generators are currently supported for Linux: Unix Makefiles Ninja Automatic Setup The RunCMake.sh script in the root folder of ezEngine can be used to automatically install all required packages and run CMake, so that you can start building right away. This script currently supports these distributions: Ubuntu 22 Linux Mint 21 We welcome contributions to add support for more distributions.  If the scripts prints a warning about Qt 6.3.0 or newer not being present in your package manager, you will have to install Qt 6.3.0 or newer manually. See Installing Qt 6 Manually GCC When running the script the first time, execute: ./RunCMake.sh --setup This will install all required packages for your distribution and then generate the make files required to build the Dev version of ezEngine. To build the Dev build, execute: ninja -C build-Dev This build command is also given by RunCMake.sh as the final output. If you change any CMake files or add new source files it is sufficient to run: ./RunCMake.sh This only invokes CMake, without checking for missing packages. To build a different build type then Dev, pass the additional --build-type argument: ./RunCMake.sh --build-type Debug Clang If you would like to use Clang instead of GCC, simply add --clang to all invocations of RunCMake.sh: ./RunCMake.sh --setup --clang ./RunCMake.sh --clang ./RunCMake.sh --build-type Debug --clang Installing Qt 6 Manually Some distributions provide quite outdated versions of Qt 6 and the ezEngine Editor requires at least Qt 6.3.0 due to a bug that exists in previous versions of Qt and prevents the 3D viewport in the Editor from working correctly. You have the following options: Install through aqtinstall Install Qt 6 through the official installer Build from source Once you have obtained a recent version of Qt, you have two options so that the ezEngine cmake scripts find it: Add the install location permantently to your PATH environment variable Specify the install location when calling RunCMake.sh like this: > PATH=/path/to/qt6/install:$PATH ./RunCMake.sh Manual Setup If you want to setup things manually or your distribution is not supported by the RunCMake.sh script, you will most likely need all of the following packages: C++17 compliant compiler (GCC or Clang) CMake 3.20 or newer uuid-dev Qt6 (version 6.3 or newer) ninja or gnu-make libxrandr libxinerama libxcursor libxi libfreetype libtinfo5 Then invoke CMake with the following arguments: Option Explanation -B build Path to the build directory. -S . Path to the ezEngine root. -G Ninja Choose to generate Ninja makefiles. Optional, if not provided gnu-make will be used. -DCMAKE_CXX_COMPILER=g++-12 Specify the C++ compiler to use. Optional, if not provided the system default will be used. -DCMAKE_C_COMPILER=gcc-12 Specify the C compiler to use. Optional, if not provided the system default will be used. -DEZ_EXPERIMENTAL_EDITOR_ON_LINUX=ON Build the ezEngine editor on Linux. This is currently experimental and might have significant bugs. -DEZ_BUILD_EXPERIMENTAL_VULKAN=ON Build the Vulkan renderer. This is currently experimental and might have significant bugs. -DCMAKE_BUILD_TYPE=Dev Specify the build type to use. -DCMAKE_EXPORT_COMPILE_COMMANDS=ON Generate a compile_commands.json file to be used for code completion in editors like Visual Studio Code. -DEZ_QT_DIR=/path/to/qt6 Manually specify the path cmake should look for Qt 6 in. -DEZ_ENABLE_FOLDER_UNITY_FILES=OFF Disable unity builds. This increases compile times but might help certain editors to provide better code completion. Example usage: mkdir build cmake -B build -S . -G Ninja -DCMAKE_CXX_COMPILER=g++-12 -DCMAKE_C_COMPILER=gcc-12 -DEZ_EXPERIMENTAL_EDITOR_ON_LINUX=ON -DEZ_BUILD_EXPERIMENTAL_VULKAN=ON -DCMAKE_BUILD_TYPE=Dev -DCMAKE_EXPORT_COMPILE_COMMANDS=ON Using Qt Creator The root of the repository can also be opened in Qt Creator, which will generally do a good job at finding the Qt location on its own. See Also Building ezEngine"
  },
  "pages/docs/build/build-macos.html": {
    "href": "pages/docs/build/build-macos.html",
    "title": "Building for MacOS | ezEngine",
    "keywords": "Building for MacOS Prerequisites Supported Compilers You can compile EZ through one of these methods: XCode 5.1.1 or higher (GCC / Clang) 64 Bit Makefiles 64 Bit Dependencies You need to install these libraries: XQuartz 2.7.5 SFML-2.5.1 Qt 5.11 (optional) A good way to do so is via homebrew: brew update brew install Caskroom/cask/xquartz brew install qt6 brew install sfml Using the command line Run CMake with CMAKE_PREFIX_PATH pointing to the dependencies listed above. In this example, a build folder is created under the root of the repo and cmake is executed in it: cmake -DCMAKE_PREFIX_PATH=/usr/local/Cellar/qt/5.13.1/;/usr/local/Cellar/sfml/2.5.1/ -DEZ_ENABLE_QT_SUPPORT=1 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DEZ_ENABLE_FOLDER_UNITY_FILES=$(unityfiles) -G \"Xcode\" ../ Afterwards the generated solution can be opened in XCode. See Also Building ezEngine"
  },
  "pages/docs/build/build-uwp.html": {
    "href": "pages/docs/build/build-uwp.html",
    "title": "Building for Windows UWP | ezEngine",
    "keywords": "Building for Windows UWP This page describes how to build EZ for the Universal Windows Platform (UWP). For desktop builds, see this page. Note that only a subset of EZ's functionality is officially maintained and supported on UWP. In general UWP support is not a priority for us. Prerequisites Install the desktop Windows prerequisites. Microsoft Visual Studio In Visual Studio, install these additional workloads: Universal Windows Platform Development CMake CMake is used as the build system. For UWP you need to have a custom installation. Generate the Solution To generate a solution for UWP, you need to pass a toolchain file to CMake. The file is located in the EZ repository under Code/BuildSystem/CMake/toolchain-winstore.cmake. Using the CMake GUI Start the CMake GUI application. Create a new solution by pointing Where to build the binaries to a new location. Press Configure once, a dialog will show up to choose the generator. Choose the desired Visual Studio generator at the top. Depending on your target device, choose the platform. For instance, for HoloLens 1 select Win32. At the bottom select Specify toolchain file for cross-compiling. On the next screen set the toolchain file PathToEzRepository/Code/BuildSystem/CMake/toolchain-winstore.cmake Using the command line Run CMake with this argument: -DCMAKE_TOOLCHAIN_FILE=PathToEzRepository/Code/BuildSystem/CMake/toolchain-winstore.cmake Building the Code Open the generated solution with Visual Studio and build everything. See Also Windows Builds"
  },
  "pages/docs/build/build-windows.html": {
    "href": "pages/docs/build/build-windows.html",
    "title": "Building for Windows | ezEngine",
    "keywords": "Building for Windows This page describes how to build EZ for desktop Windows. For UWP builds, see this page. Prerequisites This software has to be installed manually. Microsoft Visual Studio Visual Studio is the only supported IDE on Windows. It is sufficient to install the free Community edition. These versions are currently supported: Visual Studio 2022 64 Bit These workloads have to be installed: Desktop Development with C++ Game Development with C++ .Net Desktop Development You can also use the file Utilities/VS2022-ezEngine.vsconfig to import the necessary configuration into the Visual Studio installer. CMake (optional) CMake is used as the build system. On Windows you only need to install CMake if you want to use the CMake GUI to choose custom CMake configurations. If you only use the provided GenerateXYZ.bat scripts, those will use a cmake.exe that comes with the EZ repository. Generate the Solution Using the Generate Scripts In the root folder of the EZ repository you will find a couple of .bat files, such as: GenerateWin64vs2022.bat Run one of them to generate a Visual Studio solution for your preferred compiler. If these scripts fail, you most likely don't have all the prerequisites installed. They also sometimes fail, if Visual Studio recently installed an update and you haven't rebooted your PC since. Usually when this script fails it is due to common issues with CMake or the MSVC installation. Read the error messages carefully and search the internet, you'll usually find a solution quickly. Once the script finished successfully, there will be a Workspace folder in the EZ root folder. You fill find a ezEngine_***.sln file in the respective folder for the Visual Studio version that you chose. Manually Running CMake This step requires that you install CMake yourself. Run the CMake GUI and configure the build options. Building the Code Open the generated solution with Visual Studio and build everything. Run the Editor project afterwards. See Also Building ezEngine Building with Clang on Windows"
  },
  "pages/docs/build/building-ez.html": {
    "href": "pages/docs/build/building-ez.html",
    "title": "Building ezEngine | ezEngine",
    "keywords": "Building ezEngine To try out ezEngine, you can download a precompiled binary package. This article describes how you can build the engine yourself, which enables you to extend the engine with custom functionality. Getting the Code and Data Clone a branch from the GitHub repository. If you need a good git GUI, have a look at Fork. Check out the 'dev' branch. Unless your git client already checks out git sub-modules for you, also run git submodule update --initin your local clone. The EZ project uses submodules to deliver additional data such as sample content and precompiled tools. Platform Builds Windows Builds UWP Builds Linux Builds MacOS Builds Android Builds Building with Clang on Windows Build Types EZ sets up three build configuration types: Debug Dev Shipping While developing your game you should either use a Debug or a Dev build. The Debug build is best when you want to use a C++ debugger to investigate problems. It includes the necessary debug symbols and has many optimizations disabled, which makes it much easier to step through the code. Debug builds are significantly slower than the other build types. The Dev build has most of the optimizations enabled, yet still includes debug symbols. The Dev build is 3x to 10x faster than a Debug build in most cases and is very close to the speed of a Shipping build. Stepping through the C++ code with a debugger is possible, though it often behaves erratic due to the optimizations (inlining and such). For most developers the Dev build should be the main configuration to use. The Shipping build has all optimizations enabled. It does not include debug symbols anymore and it also has all the developer features disabled. That means things like ezInspector won't work here. Similarly, features like allocation tracking (for detecting memory leaks) and profiling features are disabled as well. See Also Supported Platforms ezEngine as a Submodule"
  },
  "pages/docs/build/clang-on-windows.html": {
    "href": "pages/docs/build/clang-on-windows.html",
    "title": "Building with Clang on Windows | ezEngine",
    "keywords": "Building with Clang on Windows You can build ezEngine using Clang on Windows. This can be useful to find and fix compilation errors and warnings, that do not happen with MSVC. However, as Clang support on Windows is still experimental, you may not be able to build a working executable. Using Clang/LLVM with the CMake GUI Prerequisites Install CMake (or locate cmake-gui.exe in the ez repository). Get a recent Clang Windows distribution: https://releases.llvm.org/download.html (the 64-bit version is recommended) Note: The binary should be called something like LLVM-<version>-win64.exe A Windows binary may not be available for the latest version, use an older version, if necessary. Get ninja from https://ninja-build.org and put it in your PATH environment variable. If you had to edit your PATH variable, restart your PC. Generate a Solution Using cmake-gui.exe, create a new solution for a Clang build by pointing Where to build the binaries to a new location. Press Configure once, a dialog will show up. Choose Ninja as the generator. Choose Specify native compilers then hit Finish. Specify the C and C++ compiler. When using the default paths they are located at: C: C:/Program Files/LLVM/bin/clang.exe C++: C:/Program Files/LLVM/bin/clang++.exe Click Finish If CMake can't find your ninja.exe even though it is in your PATH set the CMAKE_MAKE_PROGRAM manually to point to ninja.exe and click Configure again. You will now get an error from CMake No CMAKE_RC_COMPILER could be found. Check the Advanced checkbox to show additional options. Point CMAKE_RC_COMPILER to C:\\Program Files (x86)\\Windows Kits\\10\\bin\\<windows-sdk-version>\\x64\\rc.exe (for example C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64\\rc.exe). Also set CMAKE_RC_COMPILER_INIT to rc (if it even shows up). Click Configure Click Generate Open a Terminal and cd into the build location Run ninja to build. Using the Clang frontend for Visual Studio with the CMake GUI The clang frontend for the Visual Studio Compiler is no longer in development. Use official LLVM Clang as described above. See Also Windows Builds"
  },
  "pages/docs/build/cmake-config.html": {
    "href": "pages/docs/build/cmake-config.html",
    "title": "CMake Configuration | ezEngine",
    "keywords": "CMake Configuration Note: Using CMake directly is only needed, if you want to choose advanced build options. This is rarely needed. Prefer to use the provided build scripts for Windows or Linux. To generate a solution, run the CMake GUI. Specify Where is the source code and Where to build the binaries, then run Configure. As a generator, pick Visual Studio 2022 x64 (or one of the other supported platforms). The screenshot above shows a common setup. Noteworthy are the following points: EZ_ENABLE_QT_SUPPORT Disable this setting, if you want to compile EZ without Qt. This will remove all editor code and several tools from the final solution. The default is on. When possible the EZ CMake scripts will automatically download Qt libraries and set everything up for you. On configurations for which we do not support fully automatic setup, you need to install Qt manually and then set set EZ_QT_DIR to its installation folder. EZ_BUILD_FMOD Enable this, if you want to FMOD sound support in your build. On Windows and Linux the default is on. EZ_BUILD_RMLUI Enable this, if you want to add support for RmlUi to your build. The default is on. Once you have configured everything, run Generate and then Open Project. Adding a Custom Project The easiest way to get started with a custom project, is to use the C++ project generation. Another method is to copy an existing sample, such as the Sample Game Plugin. For starters, just create it in the same location, within the EZ source tree. If you want to move it into your own repository, you can then reference its location as an external project (see below). External Projects The options EZ_EXTERNAL_PROJECT_1-3 allow you to specify folders outside the EZ repository, which will be integrated into the solution. This is the most practical way to store your own code in a separate repository, yet have it all compiled in the same solution. This makes building, linking and debugging code as convenient as if it was stored inside the EZ file structure. Build Filter The option EZ_BUILD_FILTER allows you to strip down the code that is included in the solution. This is mainly meant for use cases where EZ is integrated as a submodule and you only need parts of its functionality. Advanced CMake Options Checking Advanced in the CMake GUI will show additional options to configure the EZ build. These are mostly used to remove specific 3rd party code (and all dependent features). This is particularly helpful, if you want to build EZ on a platform on which one of the dependencies may not compile. See Also Supported Platforms C++ Project Generation ezEngine as a Submodule"
  },
  "pages/docs/build/sdk-root.html": {
    "href": "pages/docs/build/sdk-root.html",
    "title": "SDK Root Folder | ezEngine",
    "keywords": "SDK Root Folder When the engine launches, one of the first things it usually does, is to detect the exact path of the SDK root folder. This folder is a so called special directory and is mostly used when mounting data directories. Special directories are referenced with a \">\" at the beginning, and are only allowed in few places, such as when adding data directories. For example, the 'base' data directory is mounted like this: ezFileSystem::AddDataDirectory(\">sdk/Data/Base\"); This adds the folder \"Data/Base\" that is located inside the folder where the SDK (ezEngine) is stored. Default Strategy for Locating SDK Root The default strategy by which the engine detects the SDK root folder, is to start at the location of the application binary, and walk the file structure up, until it finds a folder, which contains the sub-folders \"Data/Base\". So for instance, if the running application is located in C:/ezEngine/Bin/MyGame.exe, the search will start in C:/ezEngine/Bin, where no such folder is found. Then it will continue in C:/ezEngine. That folder does have the sub-structure C:/ezEngine/Data/Base, so the SDK root is determined to be C:/ezEngine. This strategy works, as long as the application binary is located somewhere inside the ezEngine SDK. Redirecting to SDK Root If you use a different file structure, the default strategy won't work. This is commonly the case when integrating ezEngine as a Submodule. For example your file structure may look like this: C:/MyRepo C:/MyRepo/ezEngine-module/ ... C:/MyRepo/Bin/MyGame.exe C:/MyRepo/OtherData/ ... Here ezEngine is integrated into another repository. The 'Bin' folder is top level, just as the 'ezEngine-module' folder. To enable such a pattern, you can place a 'redirection file', which points to the SDK root folder. The file has to be called ezSdkRoot.txt and it must be located somewhere along the path that the default strategy searches. In this case it would be put into C:/MyRepo/ezSdkRoot.txt and it would contain the string ezEngine-module. This way, when the engine searches for the folder that contains 'Data/Base', it will reach C:/MyRepo, see the ezSdkRoot.txt file, read its content, append the relative path inside to its current path (C:/MyRepo/ezEngine-module) and find C:/MyRepo/ezEngine-module/Data/Base, which means it determines C:/MyRepo/ezEngine-module to be the SDK root folder. Using a redirection file is the least invasive method and it works for all EZ applications, e.g. the editor, samples and tools. Custom SDK Root You can fully control where the SDK root should be and how it is found, if you write your own application (TODO). During early startup you can simply set the path of the SDK root folder with ezFileSystem::SetSdkRootDirectory(). This can be preferable when you use a very different structure. Note that this method will only work for applications that you control. Tools such as ezInspector or the editor expect to find the SDK root through the default search strategy (or through a redirection file). When to Redirect At All The SDK root folder doesn't need to point to the folder where ezEngine is stored. This is only necessary, when you really need the data that is stored in Data/Base. If you only use a fraction of EZ, for example only the Foundation library, or not the editor and rendering code, then you can also use a very different folder as your root (for example C:/MyRepo in the example above). In such cases your application would almost certainly specify its custom SDK root in its startup code directly. See Also ezEngine as a Submodule CMake Setup Building ezEngine"
  },
  "pages/docs/build/submodule.html": {
    "href": "pages/docs/build/submodule.html",
    "title": "ezEngine as a Submodule | ezEngine",
    "keywords": "ezEngine as a Submodule When using git and CMake for a project, ezEngine can be integrated as a submodule into the git repository and referenced from CMake. First ezEngine needs to be added as a submodule to git: git submodule add https://github.com/ezEngine/ezEngine.git Additionally, if you want to use the precompiled tools and the sample content from EZ, you also need to pull in its submodules as well: cd ezEngine git submodule init git submodule update Next, add the ezEngine folder in your root CMakeLists.txt: # Set the build filter, if you only want to integrate parts of EZ into your build. # set(EZ_BUILD_FILTER \"FoundationOnly\") add_subdirectory(ezEngine) The ezEngine language detection can be reused by including the ezEngine submodule utility file: # include the EZ submodule utility CMake functions include(\"ezEngine/Code/BuildSystem/CMake/ezUtilsSubmodule.cmake\") ez_detect_languages() project(\"MyProject\" LANGUAGES ${EZ_LANGUAGES}) For a full example see: https://github.com/ezEngine/submodule-example Important: This kind of integration is useful, if you want to integrate EZ code into your project, for instance, if you want to use ezFoundation as your base library. Since the EZ folder isn't top-level in this setup, using the full engine and all the data located in the data directories won't work out of the box. For additional options, see the CMake setup page. Strip Unnecessary Code When integrating EZ this way, you may only want a subset of the available functionality. For instance, you may only need the ezFoundation base library (and 3rd party dependencies). You can achieve this by configuring the build filter SDK Root Folder When integrating EZ as a submodule, it is common for the binaries to be located outside of the ezEngine sub-folder, which means the engine won't be able to find the SDK root folder anymore. See this article for ways to fix this. See Also CMake Setup"
  },
  "pages/docs/build/supported-platforms.html": {
    "href": "pages/docs/build/supported-platforms.html",
    "title": "Supported Platforms | ezEngine",
    "keywords": "Supported Platforms ezEngine is developed primarily on Windows 11, using Visual Studio 2022 in 64 Bit builds. Therefore this platform has the largest feature set and is the best tested one. The code uses C++ 11, 14 and 17 features, but only where broad compiler support is available. The renderer currently uses DX11 on Windows and a Vulkan implementation is in progress. The editor is currently available on Windows, and being ported to Linux. On Mac, Android and Linux only the base libraries are fully functional. Once the Vulkan renderer is more mature, the goal is to have most features available everywhere. List of Officially Supported Platforms Windows 10/11 desktop (details) Windows 10/11 UWP (details) OS X 10.9 (Mavericks) (details) Linux (details) Android 6.0 Marshmallow (API 23) or newer (details) Consoles (Unofficial Ports) The ezEngine team does not have access to console developer kits and thus cannot provide support for those platforms. WDStudios has ported ezEngine to various consoles. If you are a registered developer with Sony, Microsoft or Nintendo, you can contact them to get access to their ports. Send an e-mail to contact@wdstudios.tech with the title [XBox / PlayStation / Nintendo] Platform Access for ezEngine to inquire for details. Be aware that this service may not be provided for free. Important: The ezEngine project is in no way associated with WDStudios. If you become a paying customer of WDStudios, all contractual obligations are only between you and WDStudios. ezEngine itself is a free and open-source project built by people in their spare-time and the software is provided as-is. See Also Building ezEngine"
  },
  "pages/docs/custom-code/cpp/cpp-code-reload.html": {
    "href": "pages/docs/custom-code/cpp/cpp-code-reload.html",
    "title": "Hot Reloading C++ Game Plugins in the Editor | ezEngine",
    "keywords": "Hot Reloading C++ Game Plugins in the Editor When writing game code in C++, the most annoying aspect are the iteration times. Due to how C++ works, it is nearly impossible to swap out code and replace it with a newer version. Some engines try to do this, but there are always limitations, and the effort to get this working and keep it from breaking is quite big. ezEngine is no different here, reloading code at runtime is not possible. However, the editor is split into two processes: The actual editor process which displays the UI, and manages the scene state, and the engine process which does the scene rendering and executes the actual game code. This separation makes the editor more resilient to crashes. If the engine process crashes, the editor will typically just display this: When you click the button, the editor launches a new engine process, synchronizes the latest scene state over and continues as if nothing happened. That of course means, that when the new engine process launches, it also loads the latest state of all plugins. So if any of the plugins was modified, we would now see these modifications. So by simply nuking and restarting the entire engine process, we can get some form of C++ code hot reloading. Unfortunately, you can't compile a plugin while a process is using it, because the process prevents other applications from writing the DLL. We can solve this problem, by making a copy of our game plugin DLL, and loading that instead. That means that the original DLL is not actually used, and our IDE can modify it further. This trick works quite well if we only do it for plugins that are loaded fully dynamic, meaning that no other plugin tries to link against it. If that were the case, both the original plugin, as well as the copied plugin would get loaded, which is not good. Therefore only select game plugins may use this copy mechanic, which is why you need to set this up manually. How to Enable Plugin Hot Reloading In the plugin selection dialog, select your custom plugin and check Enable Reload. Note: If you used the C++ Project Generation feature to create your project, the reference to your C++ plugin is automatically set up this way. Now you can modify the code of your plugin and compile it, while the editor is open. Of course, you can't do that while being attached with a debugger. When a plugin is marked as Enable Reload, and it gets modified, the editor already automatically restarts the engine process, once no scene is being simulated. Terminate and Restart the Engine You can always manually restart the engine process through Tools > Reload Engine or the hotkey Ctrl+Shift+F4. Restrictions The Enable Reload option should only be used for select game plugins. Enabling this feature can have unintended side-effects. If any code links against a plugin, that plugin cannot be loaded as a copy. Therefore, if you want to put shared code into a separate library that other users of your plugins link against, you can't load that shared library as a copy. You can't compile code while debugging a process. To compile your code, you first have to detach your debugger. In Visual Studio that can be done via Debug > Detach All. Consequently, if you want to continue debugging after you restarted the engine process, you need to manually re-attach your debugger to EditorEngineProcess.exe. In Visual Studio this is done via Debug > Attach to Process... or even better Debug > Reattach to Process (SHIFT+ALT+P) when you want to repeat the same thing a second time. See Also Debugging C++ Code Engine Plugins C++ Project Generation"
  },
  "pages/docs/custom-code/cpp/cpp-overview.html": {
    "href": "pages/docs/custom-code/cpp/cpp-overview.html",
    "title": "Custom Code with C++ | ezEngine",
    "keywords": "Custom Code with C++ To extend the engine with C++ code, you should put your code into an engine plugin. This enables both the editor as well as ezPlayer to load and execute your code. Video: How to add custom C++ code to your game Build Setup The first thing you need to set up is your build system, meaning you need to have a project for your plugin. The easiest way to do so, is to use the C++ Project Generation functionality in the editor. Another option is to add your own project into the ezEngine source tree and just use EZ's CMake build setup and generated solution. A cleaner approach is to do basically the same, but to put your code outside the EZ source tree, and add a reference to that folder: This will integrate your source into the ezEngine solution. Make sure to use the same CMakeLists.txt files as the engine plugins in EZ do. With this option, you can have your code in a separate repository. Another way is to use your own CMake based build setup, and integrate ezEngine as a Submodule. All three solutions give you tight integration of your code and the ezEngine code, which makes debugging more convenient. You can, of course, also build ezEngine once and just link against its libraries. Custom Game Code For an example how to set up a game plugin, see the Sample Game Plugin. It is best to start with writing a custom component. If you need higher level game logic, have a look at game states. Also have a look at input. Once you add more complex systems, you may need the startup system to properly initialize them and shut them down. Debugging & Profiling The chapter Debugging C++ Code gives some useful tips. Things you should also be aware of are the console and CVars, as well as stats and debug rendering. Finally, have a look at profiling, to know where your code spends most of its time. Standalone App If you want to write your own stand-alone application, have a look at the application (TODO) chapter. However, we encourage you to have your entire game code in a plugin, otherwise you can't use any of it in the editor. You would mostly need your own application for the finishing touches of how you present the main menu, etc, and a good starting point is to just copy the ezPlayer and make adjustments. See Also Custom Code Sample Game Plugin Engine Plugins The World / Scenegraph System"
  },
  "pages/docs/custom-code/cpp/cpp-project-generation.html": {
    "href": "pages/docs/custom-code/cpp/cpp-project-generation.html",
    "title": "C++ Project Generation | ezEngine",
    "keywords": "C++ Project Generation There are multiple ways to get custom C++ code into EZ. The best way is to create an engine plugin, because this way the code can be executed directly inside the editor. Additionally such plugins can be hot reloaded to some degree. EZ uses CMake for its build infrastructure. See this chapter about different ways to integrate your own project into the EZ build. The most convenient way, however, is to let the editor create a stand-alone project for you. This way you get a CMake based project that only contains your code, but has all the necessary references set up to link against EZ and output its DLLs to the right folder. Note: One downside with this approach is that your C++ IDE will not contain the EZ engine sources. That makes it less convenient to search for existing functionality, look up code API docs and stepping through EZ code while debugging. However, if you want that, you can include the generated plugin as an external project into the EZ build system. Video: How to add custom C++ code to your game How to Generate a New C++ Project Note: Generating C++ projects requires a one time setup. Go to the preferences Tools > Preferences... and configure the C++ compiler to use for your C++ projects. See Editor Settings for details. Select Project > C++ Project > Setup C++ Plugin.... The following dialog will appear: Currently the locations for where the C++ source is stored and where the project will be built are both hard-coded to be within the project directory. Choose the desired name for the plugin or leave the default. Press the CMake Generate button and wait for it to finish. The CMake output is written into the Output window. In case of errors, please have a look here. If everything went fine, you can open the project and compile the code. Attention: The code has to be built for the very same build type that the editor is running in (Debug, Dev or Shipping), otherwise the editor won't be able to load the DLL. If the build fails because certain EZ DLLs are missing, you are most likely building the wrong build type. The same may be true if you do code changes, but running the game from the editor doesn't reflect those changes. The editor makes sure to generate the solution only for the necessary build type and will update the solution automatically when needed. Opening an Existing C++ Project If you have generated a C++ project before, the Open in IDE button will be active right away when you open this dialog. In this case you don't need to generate the C++ project again. You can also skip this dialog and use Project > C++ Project > Open in IDE instead. Regenerating a C++ Project If you have added or removed source files on disk, you might need to regenerate the C++ project for those changes to show up in your IDE. You have three options to do so: Use Project > C++ Project > Regenerate C++ Project. Run CMake yourself. For example you can use the CMake GUI app, point it to the plugin's build directory, and then Configure and Generate the C++ project at any time you like. Rerun CMake Generate from the dialog above. This will clear the CMake cache and fully regenerate the C++ project. Be aware that this resets all CMake options to their default values and often takes longer than strictly necessary. See Also Custom Code with C++ Hot Reloading C++ Game Plugins in the Editor Engine Plugins Sample Game Plugin"
  },
  "pages/docs/custom-code/cpp/custom-cpp-component.html": {
    "href": "pages/docs/custom-code/cpp/custom-cpp-component.html",
    "title": "Custom Components with C++ | ezEngine",
    "keywords": "Custom Components with C++ To write a custom C++ component, the first thing you need is a custom engine plugin. Once you have that, and have it enabled in your project settings, any custom component that you define in that plugin will show up in the editor and can be attached to game objects. The Sample Game Plugin shows all the pieces that you need, including multiple components to get inspiration from. This article describes the steps to create a simple custom component. Before you continue, please read the components chapter, as it already covers most things that you need to know. Component Manager Declaration For every type of C++ component there is a corresponding component manager. The component manager is responsible for allocating and deallocating components and for updating them. Each component manager is tied to a single world, so if you have multiple worlds, each world will hold its own instance of each component manager. A component manager is a world module, so it can register functions to be called during specific update phases of the world. For the vast majority of components we only need a component manager that calls Update() on our component type once a frame. We can declare such a simple manager like this in the header file for our component: using DemoComponentManager = ezComponentManagerSimple<class DemoComponent, ezComponentUpdateType::WhenSimulating>; Component Class Declaration Next, we declare our component class. All components must derive (at least indirectly) from ezComponent. Also vital is to insert the EZ_DECLARE_COMPONENT_TYPE macro, where you pass in the own component class name, the base class, and the component manager class. class DemoComponent : public ezComponent { EZ_DECLARE_COMPONENT_TYPE(DemoComponent, ezComponent, DemoComponentManager); ////////////////////////////////////////////////////////////////////////// // ezComponent public: virtual void SerializeComponent(ezWorldWriter& inout_stream) const override; virtual void DeserializeComponent(ezWorldReader& inout_stream) override; protected: virtual void OnSimulationStarted() override; ////////////////////////////////////////////////////////////////////////// // DemoComponent public: DemoComponent(); ~DemoComponent(); private: void Update(); float m_fAmplitude = 1.0f; // [ property ] ezAngle m_Speed = ezAngle::MakeFromDegree(90); // [ property ] }; Here we override a couple of functions from ezComponent. For the binary serialization we must implement ezComponent::SerializeComponent(). As long as you test your component only inside the editor, you don't yet need to implement these functions, as the editor stores reflected properties automatically. However, once you want to export your scene, these functions are used, and if you forgot to properly serialize something, the exported scene will not work correctly. Note that our sample component has a (non-virtual) function called Update(). This is necessary because we use the ezComponentManagerSimple here, which expects to find such a function. If you write your own component manager, you can do this differently. Reflection Block In our cpp file we need to insert a reflection block for our component type. This tells the engine all the details about our component, for instance which properties it has. // clang-format off EZ_BEGIN_COMPONENT_TYPE(DemoComponent, 3 /* version */, ezComponentMode::Dynamic) { EZ_BEGIN_PROPERTIES { EZ_MEMBER_PROPERTY(\"Amplitude\", m_fAmplitude)->AddAttributes(new ezDefaultValueAttribute(1), new ezClampValueAttribute(0, 10)), EZ_MEMBER_PROPERTY(\"Speed\", m_Speed)->AddAttributes(new ezDefaultValueAttribute(ezAngle::MakeFromDegree(90))), } EZ_END_PROPERTIES; EZ_BEGIN_ATTRIBUTES { new ezCategoryAttribute(\"SampleGamePlugin\"), } EZ_END_ATTRIBUTES; } EZ_END_COMPONENT_TYPE // clang-format on This information is used in various ways. The editor uses it for the UI. Attributes on each property allow you to configure what default values the editor should use, and whether it should clamp the range for values, etc. Bindings to other languages also use this information to generate the necessary code. Everything that is not mentioned in this block, is internal to the C++ code and hidden from the tools. Initialization and Update Next up, we implement our basic component code: DemoComponent::DemoComponent() = default; DemoComponent::~DemoComponent() = default; void DemoComponent::OnSimulationStarted() { SUPER::OnSimulationStarted(); // this component doesn't need to anything for initialization } void DemoComponent::Update() { const ezTime curTime = GetWorld()->GetClock().GetAccumulatedTime(); const ezAngle curAngle = curTime.AsFloatInSeconds() * m_Speed; const float curHeight = ezMath::Sin(curAngle) * m_fAmplitude; GetOwner()->SetLocalPosition(ezVec3(0, 0, curHeight)); } Components rarely need to do much in their constructor and destructor. Most setup should be done in ezComponent::OnSimulationStarted(). For components that should already have functionality in the editor, while the simulation is not yet running, you should do your setup in ezComponent::OnActivated() instead. There is no OnSimulationStopped(), as this would always be the same as ezComponent::OnDeactivated(). As you can see, this component modifies the position of its owner object during its update. This is why we had to use ezComponentMode::Dynamic in the reflection block, to tell the engine that objects with this component attached may change their position. Serialization Finally, to make our component also work in exported scenes, we need to implement serialization: void DemoComponent::SerializeComponent(ezWorldWriter& inout_stream) const { SUPER::SerializeComponent(inout_stream); auto& s = inout_stream.GetStream(); s << m_fAmplitude; s << m_Speed; } This writes out the data in the latest format. If you change the format, you should increase the version number of your component in the reflection block at the very top. Obviously, at runtime we also need to deserialize our component. This is where we implement backwards compatibility for older exported scenes: void DemoComponent::DeserializeComponent(ezWorldReader& inout_stream) { SUPER::DeserializeComponent(inout_stream); const ezUInt32 uiVersion = inout_stream.GetComponentTypeVersion(GetStaticRTTI()); auto& s = inout_stream.GetStream(); s >> m_fAmplitude; if (uiVersion <= 2) { // up to version 2 the angle was stored as a float in degree // convert this to ezAngle float fDegree; s >> fDegree; m_Speed = ezAngle::MakeFromDegree(fDegree); } else { s >> m_Speed; } } Conclusion Adding a custom component in C++ is not hard. Use the Sample Game Plugin as a playground to get started. Of course with C++ you have the typical restriction that you can't hot reload code, you have to close the editor, compile your plugin and reopen the editor. Hot Reloading C++ Game Plugins in the Editor describes a mechanism that can basically do all that for you with a single button press, though. Armed with these basics, you should have a look at existing components to see how to solve specific issues. See Also Components Custom Code Sample Game Plugin Hot Reloading C++ Game Plugins in the Editor"
  },
  "pages/docs/custom-code/cpp/engine-plugins.html": {
    "href": "pages/docs/custom-code/cpp/engine-plugins.html",
    "title": "Engine Plugins | ezEngine",
    "keywords": "Engine Plugins Engine plugins are the best way to get your custom code into the engine, such that it is accessible by the editor and also ezPlayer. Contrary to using a plugin, you could also build your own application (TODO), which may link to static libraries that contain your code. However, that approach means that your code cannot be loaded into the editor process and therefore you won't be able to leverage those tools to their full extent. We strongly advise against that. Creating a Plugin The easiest way to create a custom plugin is to use the C++ Project Generation. This will create a template project for you, set up the CMake files for your build and configure the plugin selection. Creating a Plugin Manually In case you want more control, you can also set up your game plugin manually. The Sample Game Plugin is a good reference for how to do that. To just create a plugin, at all, you only need very little setup. The only files you need to look at are: CMakeLists.txt SampleGamePluginDLL.h SampleGamePlugin.cpp Build System Setup The file CMakeLists.txt is only of interest here in case you want to reuse the EZ build infrastructure to generate your library. If you use ezEngine as a Submodule then you probably have your own CMake scripts. Either way, you need to add a project that generates a DLL. DLL Symbol Import/Export The file SampleGamePluginDLL.h only contains #defines for DLL import/export macros. // Configure the DLL Import/Export Define #if EZ_ENABLED(EZ_COMPILE_ENGINE_AS_DLL) # ifdef BUILDSYSTEM_BUILDING_SAMPLEGAMEPLUGIN_LIB # define EZ_SAMPLEGAMEPLUGIN_DLL EZ_DECL_EXPORT # else # define EZ_SAMPLEGAMEPLUGIN_DLL EZ_DECL_IMPORT # endif #else # define EZ_SAMPLEGAMEPLUGIN_DLL #endif If your plugin will be entirely on its own, you don't even need this. However, if you want to use multiple plugins and some of them should contain shared code, then others need to be able to link against the shared libraries and access classes and functions from that library. By tagging classes with these macros you can export symbols from a DLL and thus make those things available to other code. For examples how to use this, just search the sample plugin. Plugin Callbacks EZ provides additional hooks for initialization when a plugin gets loaded or unloaded. You can find these in SampleGamePlugin.cpp: EZ_PLUGIN_ON_LOADED() { // you could do something here, though this is rare } EZ_PLUGIN_ON_UNLOADED() { // you could do something here, though this is rare } These callbacks are optional, though in some cases you may want to register and unregister things here. However, it is way more common to rather use the startup system instead. Loading a Plugin If you want to load a plugin from code, you would use ezPlugin::LoadPlugin() and provide only the name (no path) of your plugin. Make sure that the DLL is stored in the same directory as all other DLLs and EXEs. The more convenient way to load your game plugin, though, is to enable it in the project settings. Then it will be automatically loaded by every application (TODO). Add Custom Code Once you have your basic plugin set up and can load it into your project, you can start adding custom code. The easiest way to get started is to write a custom component. Once you need control over higher level game logic, you can add your own game state. And if you need to initialize and shut down certain systems, you should utilize the startup system. Utility Plugins If you want to write a plugin that provides some functionality for shared access, like some integration of a third-party library, the process is exactly the same. The only difference is, that such libraries would never contain a game state. Also, have a look at singletons if your plugin is supposed to provide an implementation of some abstract interface. See Also Sample Game Plugin Game States Custom Components with C++ Startup System Singleton Interfaces"
  },
  "pages/docs/custom-code/custom-code-overview.html": {
    "href": "pages/docs/custom-code/custom-code-overview.html",
    "title": "Custom Code | ezEngine",
    "keywords": "Custom Code There are currently three ways to write custom code: Custom Code with C++ Custom Code with TypeScript Custom Code with Visual Scripts C++ Extending the engine with C++ is the most versatile and efficient. With C++ you have full access to the entire engine, giving you all the power you need. Any serious project will have to use C++ code for some parts. For things like game logic you can start with script code instead, and migrate critical parts to C++ on demand. Extending the renderer and access to third party integrations is mostly only possible from C++ code. Using the C++ Project Generation feature, it is very quick and easy to set up a custom C++ plugin. C++ obviously has the downsides of longer compilation times, and live updating code is not possible. There is a way for game plugins to be modifiable while the editor runs, and the editor can fully shut down and reload its engine process, which does enable a form of live reloading of C++ code. TypeScript The TypeScript integration allows you to write custom components. The integration provides access to the most important aspects that are needed for game code. C++ components can be accessed, as long as they expose their functionality through reflection. TypeScript is very useful for game logic and allows to quickly create complex prefabs. TypeScript is transpiled to JavaScript code, which is interpreted in a VM. Its performance is therefore far worse than C++. However, migrating a critical component from TypeScript to C++ at a later stage is possible. TypeScript code is updated every time you run a scene, which allows for quick iteration times. Visual Scripting Visual script code is the most limited both in functionality and tooling, but it is quite convenient to use for small scripts that act as the glue code, for example between C++ and state machines, or for handling simple logic."
  },
  "pages/docs/custom-code/game-logic/forward-events-to-game-state-component.html": {
    "href": "pages/docs/custom-code/game-logic/forward-events-to-game-state-component.html",
    "title": "Forward Events To Game State Component | ezEngine",
    "keywords": "Forward Events To Game State Component This component forwards any message that it receives to the active game state. Game states can have message handlers just like any other reflected type. However, since they are not part of the world, messages are not delivered to them. By attaching this component to a game object, all event messages that arrive here are forwarded to the active game state. This way, a game state can receive information, such as when a trigger gets activated. Multiple of these components can exist in a scene, gathering and forwarding messages from many different game objects, so that the game state can react to all of them. See Also Game States Messaging"
  },
  "pages/docs/custom-code/game-logic/state-machine-asset.html": {
    "href": "pages/docs/custom-code/game-logic/state-machine-asset.html",
    "title": "State Machine Asset | ezEngine",
    "keywords": "State Machine Asset A state machine asset is used to create a state machine that is commonly used to keep track of an entity's state. State machines are very useful to give objects functionality. For example a door might have the states closed, opening, open and closing and in each state it has to do certain things (apply a rotation) and allow or disallow interaction. State machines are also often used for AI (creatures or NPCs), to give them behavior. Since many AI types are rather simple, this is a viable option. For more complex AI logic, state machines quickly become too limited. State Machine Concept At any time exactly one state is active in a state machine. The active state determines what actions an entity will do and what other states it can transition into. For example in the image above you can see that this state machine can only transition from the Idle state into the Alert state (and back), but it can't directly transition into the Attacking state. What an entity does, when a certain state is active, is usually up to other code. On the state node in the state machine asset you select what type of state this is. Different state types can be implemented in C++ or visual scripts. State machines are updated regularly and during every update they may transition into another state. Possible transitions are represented by arrows between states. Just as with states, there are also different types of transitions. As with states, custom transition types can be implemented with C++ or visual scripts. During the state machine update, each transition on the active state is queried, whether its condition is met. If so, the transition is taken, and the state that it points to becomes the new active state. What it means that a transition's condition is met, is up to the transition type's implementation. For example the blackboard transition inspects values from a nearby blackboard, allowing you to set up logical rules. Another type of transition may simply wait for a second and then allow to transition further, acting as a timer. In many state machines there are states that can be reached from pretty much every other state. For example the Dead state in the image above is simply reached whenever the health of a creature reaches zero, no matter which state it currently is in. In a pure state machine, one would add transitions from every node to that final state. However, since this is cumbersome, EZ also allows to set up transitions that can transition from any active state to their target state. In the state machine asset, such transitions are represented as an Any State (taking the place of any other state) from which they are drawn to the target state. Configuring State Machines Right click into the window and choose New State or New Any State to add a node. To add a transition between states, click the + on a node and drag it to another state. Select state nodes in the main window to see their properties. Here you can give the state a name and select the state type. Most state types have custom properties that also need to be configured here. Select transition arrows to see their properties. Here you can select the transition type. Typically you also then need to configure the transition's properties. If you don't select a state type, a state simply does nothing. If you have a transition without a transition type, the transition will be taken immediately. This can be used to simply chain states. For example the start state is always active, even while the editor is not simulating. It is sometimes undesireable to have this state do anything. Therefore, a second state can be used as the \"real\" start state, and a transition without a type can be added between the two. This transition is only going to be taken once the scene is being simulated. Default Initial State Right click on a state and select Set as Initial State to make a state the default initial state. That means, when this state machine is used in a state machine component or as a nested state machine, and no initial state is selected by the user, this state is used. State Types You have to select a state type for every node in your state machine. Send Message State When the Send Message State gets activated (transitioned into) or deactivated (transitioned out of), it sends the ezMsgStateMachineStateChanged event message to the game object on which the executing state machine component is attached. Thus other components, such as TypeScript components can listen for this message and react accordingly. The message can be sent with a delay. Also, if this state type is configured to send neither a message on enter, nor on exit, it effectively does nothing and can be used for states that don't require further action. You can also turn on logging on enter or exit, for better debugging. Nested State Machine A Nested State Machine state references another state machine. When the state is entered, it starts executing an instance of that state machine. Messages sent from the nested state machine will be delivered to the same owner game object. As long as the surrounding state stays active, the nested state machine gets updated. Once the surrounding state is exited, execution of the nested state machine is suspended. You can choose whether it gets reset to the initial state, or stay in the last active state. This way, once the nested state machine gets activated again later, it may either start from the beginning, or resume where it left off. Nested state machines can be very useful to reuse state machines and to make editing easier. Also the fact that a state machine can be suspended and resumed at its last active state allows for more complex behavior. If no initial state is specified, the default initial state is used. Compound State A Compound State has no functionality by itself, rather it holds an array of other state types. All events (state entered/left) are forwarded to all sub-states equally. Thus it can be used to trigger multiple reactions at the same time. Script State The Script State runs a visual scripts. It executes the OnEnter, OnExit and Update event handlers for state machine states. Switch Object State This state sets the enabled flag on a game object and disables all other objects in the same group. This state allows to easily switch the representation of a game object. For instance you may have two objects states: normal and burning. You can basically just build two objects, one in the normal state, and one with all the effects needed for the fire. Then you group both objects under a shared parent (e.g. with name 'visuals'), give both of them a name ('normal', 'burning') and disable one of them. When the state machine transitions from the normal state to the burning state, you can then use this type of state to say that from the 'visuals' group you want to activate the 'burning' object and deactivate all other objects in the same group. Because the state activates one object and deactivates all others, you can have many different visuals and switch between them. You can also only activate an object and keep the rest in the group as they are (e.g. to enable more and more effects). If you only give a group path, but no object name, you can also use it to just disable all objects in a group. If multiple objects in the same group have the same name, they will all get activated simultaneously. Make sure that essential other objects (like the physics representation or other scripts) are located on other objects, that don't get deactivated. State Properties PathToGroup: A list of object names, separated with slashes, that forms a relative path starting at the game object on which the state machine component is located, to a game object which acts as a group and contains multiple sub-objects. ObjectToEnable: The name of the sub-object in the group that should get enabled. DeactivateOthers: If true, all other objects within the group get disabled. Otherwise they stay as they are. Transition Types You have to select a transition type for every transition in your state machine. Blackboard Conditions Blackboard conditions query the blackboard. You can configure the transition condition to be fulfilled either when at least one or all the specified blackboard entries have the desired values. Timeout Transition This transition is fulfilled after a certain amount of time has passed. This can be used to automatically transition to the next state after a fixed amount of time. Together with a compound transition it can also be used to prevent transitioning before a minimum of time has passed. Compound Transition The Compound Transition allows you to create complex logical conditions. It holds an array of other transition types. You can select whether either at least one of them, or all of these conditions need to be met, for the compound transition to be fulfilled. For example, you can add a blackboard condition and a timeout transition. If you set the compound transition to AND, both of these must be true, meaning the transition can only be taken after the blackboard values are correct and the minimum time has passed. If, however, you set it to OR, the transition is taken once either the timeout or the blackboard state is fulfilled. Note that you can use nested compound transitions to create even more complex logical conditions. Transition Event Transition This type of transition is taken when a transition event with the expected name has been raised. Transition events can be raised from visual scripts or C++ through the state machine component. This allows script code to report back, that a certain condition was met, and that the state machine may leave its current state. Executing State Machines In theory state machines could be used in many contexts. In custom C++ code you are free to instantiate state machines directly via ezStateMachineResource. In a scene, you can instantiate a state machine through the state machine component. See Also State Machine Component Custom Code"
  },
  "pages/docs/custom-code/game-logic/state-machine-component.html": {
    "href": "pages/docs/custom-code/game-logic/state-machine-component.html",
    "title": "State Machine Component | ezEngine",
    "keywords": "State Machine Component The state machine component creates an instance of a state machine asset and updates that every frame. The InitialState property determines the starting state of the state machine. This component also enables a state machine to send messages to components attached to the same game object. For example the ezMsgStateMachineStateChanged event message will be broadcast on this object, if the state machine contains a corresponding state. Component Properties Resource: The state machine to use. InitialState: In which state the state machine is supposed to start. If left empty, the default initial state is used. See Also State Machine Asset Custom Code"
  },
  "pages/docs/custom-code/game-logic/trigger-delay-modifier-component.html": {
    "href": "pages/docs/custom-code/game-logic/trigger-delay-modifier-component.html",
    "title": "Trigger Delay Modifier Component | ezEngine",
    "keywords": "Trigger Delay Modifier Component Handles ezMsgTriggerTriggered events and sends new messages after a delay. This is typically used to activate something not right away, but when something is present inside a trigger volume for a minimum duration. Similarly, it can also be used from deactivating something too early. The enter and leave messages are sent only when an empty trigger is entered or when the last object leaves the trigger. While any object is already inside the trigger, no change event is sent. Therefore this component can't be used to keep track of all the objects inside a trigger. The enter and leave events can be sent with a delay. The enter event is only sent, if the trigger had at least one object inside it for the full duration of the delay. Which exact object may change, but once the trigger contains no object, the timer is reset. The sent ezMsgTriggerTriggered does not contain a reference to the triggering object, since there may be multiple and they may change randomly. Component Properties ActivationDelay, DeactivationDelay: The time that an object needs to be present inside the trigger, or have fully left the trigger area, before the trigger is considered to be activated or deactivated. Only after this delay is the ezMsgTriggerTriggered event sent to the parent object. See Also Jolt Trigger Component Messaging"
  },
  "pages/docs/custom-code/typescript/custom-ts-components.html": {
    "href": "pages/docs/custom-code/typescript/custom-ts-components.html",
    "title": "Custom Components with TypeScript | ezEngine",
    "keywords": "Custom Components with TypeScript To create a new component type, create a new TypeScript asset. In that asset document, click the toolbar button to edit the script with Visual Studio Code. This will not only open the text editor, but also ensure that the .ts file exists and contains a basic template for your new component. Base Class Your component class must extend one of these base classes: ez.TypescriptComponent ez.TickedTypescriptComponent If it extends ez.TypescriptComponent, it can react to messages, startup/shutdown and activation/deactivation callbacks. However, it will not be updated regularly. Though this can be achieved through messages. If it extends ez.TickedTypescriptComponent, the member function Tick() is executed regularly. The rate at which it shall be called can be modified using SetTickInterval(). Often game components need to do regular checks and update their own state. Use the ticked base class when this is necessary. Choose a tick interval that is as long as possible to reduce their performance cost. You can also dynamically change the tick rate, to e.g. do more updates when the player is close, than when they are far away. Whenever possible, though, prefer to use the non-ticked base class and have no regular update, at all. Such components rely on other machnisms, such as triggers to detect when they need to react, and they can use delayed messages (sent by others or by themselves) to trigger follow up work. Tick Function When extending ez.TickedTypescriptComponent, the component code must contain a function with this signature: Tick(): void { } It will be executed during the game update whenever enough time has passed. Use SetTickInterval() to adjust the frequency. Initialization The template code contains examples for these functions: Initialize() Deinitialize() OnActivated() OnDeactivated() OnSimulationStarted() These functions are called in the same way as for C++ components. See Component Activation for details. Message Handlers TypeScript components can both send and receive messages. The article Messaging in TypeScript Code explains this in more detail. To handle messages, message handler functions must be registered first. This is done on a per-type basis, rather than per instance. Therefore you have to register message handlers from within the static function RegisterMessageHandlers(). Auto Generated Code The editor may insert auto generated code into the .ts file. This is needed for example for variables that are supposed to show up as exposed parameters. Special code comments are used to tag the are where the editor can insert the generated code: /* BEGIN AUTO-GENERATED: VARIABLES */ /* END AUTO-GENERATED: VARIABLES */ Important: Don't remove these comments and don't put any of your code between these two lines. Writing Your Component To initialize things, use the OnSimulationStarted() callback. For regular updates, put your code into the Tick() function. Use messaging to communicate with unknown component types or when a delay is desired. For all known component types, you can call functions or read and write properties directly. For an overview what functionality is available through TypeScript, check out the TypeScript API. See Also TypeScript Asset Messaging in TypeScript Code TypeScript API Custom Code with TypeScript"
  },
  "pages/docs/custom-code/typescript/ts-api.html": {
    "href": "pages/docs/custom-code/typescript/ts-api.html",
    "title": "TypeScript API | ezEngine",
    "keywords": "TypeScript API This page gives an overview over the functionality that EZ exposes through TypeScript. For an introduction to the TypeScript language please refer to the web (for example TypeScript in 5 minutes). Note that you don't need to install anything to use TypeScript in EZ, the required TypeScript transpiler is already included. API Documentation All TypeScript APIs are documented with code comments. In Visual Studio Code you can see the documentation for each function by hovering the mouse cursor over it: You can also jump to a function or class declaration using F12. This is useful to see what functions are available on a given class. Importing Files (require) TypeScript and JavaScript have multiple mechanisms how to make code from other files available. In EZ only the require mechanism will work: import EZ = require(\"TypeScript/ez\") This imports all exported declarations from the file TypeScript/ez.ts into an object called ez in this file. Thus typing ez. grants access to all the exported classes, namespaces and functions from that file. The path given to require must be relative to a data directory. For example, the file above is located in the Plugins data directory. Note: require always returns an object and therefore you must assign its result to a variable. Consequently, there is no way to make the imported names globally accessible, you can only access them through that variable. To import multiple files, you need to store each result in a different variable: import EZ = require(\"TypeScript/ez\") import _ge = require(\"Scripting/GameEnums\") Re-exporting Imported Declarations You can re-export declarations from a .ts file that you imported from somewhere else. For plenty of examples, look at the file ez.ts: import __Log = require(\"TypeScript/ez/Log\") export import Log = __Log.Log; Here, everything from the file Log.ts is imported into the variable __Log. We then selectively re-export the namespace Log from the variable __Log again, under the name Log. We could rename the exported symbol, if we wanted. Unfortunately, there does not seem to be a way to re-export all declarations automatically, you need to name each one individually. Scenegraph ez.Component ez.Component is the base class for all component types, including the C++ components. Your custom components must extend either ez.TypescriptComponent or ez.TickedTypescriptComponent. The functionality exposed through ez.Component is mostly identical to all other components. If you hold a reference to a component for more than a frame, it is vital to use ez.Component.IsValid() to check whether the component is still alive, before accessing it. If IsValid() returns true, the component can be accessed safely for the rest of the frame. ez.GameObject ez.GameObject exposes the game object functionality to TypeScript mostly 1:1. Through this you modify object positions, delete or move child nodes, access attached components and send messages. You can't extend game objects. If you hold a reference to a game object for more than a frame, it is vital to use ez.GameObject.IsValid() to check whether it is still alive, before accessing it. If IsValid() returns true, the game object can be accessed safely for the rest of the frame. ez.World ez.World exposes the world functionality. However, the functionality provided is only a limited subset. Some functionality is simply not needed in the TypeScript binding, and some is exposed differently. Since all TypeScript code is executed in the context of one specific world, you can't access a different world from TypeScript code. Therefore, there is no need to get the world that you operate in (as is common in C++). Therefore ez.World is only a namespace, not a class, and all functionality is always accessible. Additionally, functionality like ez.Clock and ez.Random, which are in C++ bound directly to a world, are similarly just global namespaces in TypeScript and not exposed through ez.World. ez.Message ez.Message and ez.EventMessage are base classes for all messages. The page Messaging in TypeScript Code goes into more detail. Math TypeScript already provides mathematical functions through the Math namespace. Additionally, the EZ API provides these classes: ez.Vec2 and ez.Vec3: 2 and 3 component vectors for 2D and 3D linear algebra. ez.Mat3 and ez.Mat4: A 3x3 and 4x4 matrix. ez.Quat: A quaternion class to handle rotations. ez.Transform: A transform stores a position (ez.Vec3), a rotation (ez.Quat) and a scale factor (ez.Vec3). It is mainly used to represent object transformations, and is more convenient than using 4x4 matrices. ez.Angle: Provides utility functions to work with angles. Mostly to convert between radians and degree. ez.Color: A utility class to work with colors. All colors are treated as HDR colors in linear space, though conversions to and from Gamma space are provided. See color spaces (TODO) for details. Debugging ez.Log The ez.Log namespace contains functions for writing messages to the log. This is a useful tool for debugging. ez.Debug The ez.Debug namespace contains various functionality. There are functions for debug rendering, ie. to insert shapes into the rendered output, which can be helpful in visualizing many aspects. ez.Debug also provides access to CVars and console functions. Utilities ez.Clock The ez.Clock namespace has functions to access the world's clock. The clock represents the game time, meaning it advances at its own pace, which can be adjusted dynamically. When you need to know how much time has passed since the last frame (not the last Tick()), use ez.Clock.GetTimeDiff(). Use ez.Clock.GetAccumulatedTime() it you need to measure longer durations. ez.Time In TypeScript code time should be stored as number types and measured in seconds. This is how all functions expect time values. ez.Time contains utility functions to convert time values to other units and to query the current system time. ez.Random The ez.Random namespace contains functions to get random numbers. Physics In ez.Physics you find functions to query the physics engine. For example to do raycasts or overlap tests. See Also TypeScript Component Messaging in TypeScript Code"
  },
  "pages/docs/custom-code/typescript/ts-asset.html": {
    "href": "pages/docs/custom-code/typescript/ts-asset.html",
    "title": "TypeScript Asset | ezEngine",
    "keywords": "TypeScript Asset Each TypeScript asset manages the script for a single custom TypeScript component. You create a new TypeScript asset using File > Create.... The document displays the script file contents as read-only. The document has no text editing functionality. The actual script code is stored in a separate .ts file, such that you can edit it with a regular text editor. After creating a new TypeScript asset, the ScriptFile property will have a default value for the .ts file. You can change this location, if you wish. Code Editing The ezEditor assumes that you have Visual Studio Code installed. When you click the VSC icon in the toolbar, it will launch Visual Studio Code with a workspace setup that includes all data directories of the project. It also makes sure that the referenced ScriptFile is created and filled out with some template code, if the file doesn't exist yet. From there on, you can write the code in the external editor. Transforming the asset (Ctrl+E) will transpile the script and update the text preview. Script Parameters In the asset properties you can add variables. Each variable has a name and a default value. After changing the variables you have to transform the script (toolbar button or Ctrl+E). This will insert the necessary code into the script. Make sure to not touch the markers for the auto-generated code section. The variables that you add here will become exposed parameters for this script. Meaning, when this asset is used through a TypeScript component, the component can override the values of these variables. Thus you can instantiate the same script many times with different starting parameters. Non-Component TS Files You can split up script code into multiple files, for example to easier share code between TS components. Simply create as many .ts files as you need and 'include' them as needed (using require). Having a TypeScript asset is not required for such files. TypeScript assets are only needed for the main TS files that represent a proper custom component type. The TypeScript assets are needed in the editor to be able to select the desired TS component code on a TypeScript component, and at runtime they are used to know how to instantiate the script. The assets are not used to identify which files to transpile. Instead, the editor will simply transpile all .ts files that it finds in any data directory of the project. See Also Custom Components with TypeScript TypeScript Component"
  },
  "pages/docs/custom-code/typescript/ts-component.html": {
    "href": "pages/docs/custom-code/typescript/ts-component.html",
    "title": "TypeScript Component | ezEngine",
    "keywords": "TypeScript Component The TypeScript Component represents a custom component that was written in TypeScript, rather than in C++. The component itself is a C++ component. It mediates between C++ and TypeScript by forwarding C++ events and messages to the script code and back. Component Properties HandleGlobalEvents: If enabled, this component acts as a Global Event Message Handler. This is useful for scripts that should implement logic for an entire level. Script: The TypeScript asset reference for which script to run. Parameters: In case the referenced script has exposed parameters, they are being listed here and can be modified. When the script gets instantiated, the values of these parameters are passed into the script. Modifying the values after the script was started currently has no effect. See Also TypeScript Asset"
  },
  "pages/docs/custom-code/typescript/ts-messaging.html": {
    "href": "pages/docs/custom-code/typescript/ts-messaging.html",
    "title": "Messaging in TypeScript Code | ezEngine",
    "keywords": "Messaging in TypeScript Code TypeScript components can both send and receive messages. The way messages can be sent, posted and received, and how messages are routed is identical to the behavior on the C++ side. Please read the chapter about messaging to familiarize yourself with the general concepts. The main difference in TypeScript is, that messages that have been declared in C++ can be sent and received in TypeScript, but messages that have been declared in TypeScript can only be sent and received by TypeScript code. Sending Messages You can either send a message directly to a specific component (through ez.Component) or to a game object hierarchy (through ez.GameObject). Contrary to the C++ API, there are no functions on ez.World to send messages. The SendMessage() functions on ez.Component and ez.GameObject take an additional boolean parameter expectResultData. If this is set to true, that means that the sender of the message expects the receiver(s) to write back result data into the sent message, and intends to read those results afterwards. If it is set to false, the message sender does either not expect result data in the message, or doesn't intend to read it. This is an optimization, if you need any result data, set the parameter to true, which means additional work is necessary to synchronize the message back to the caller. Otherwise keep this set to the default value (false). Sending Event Messages TypeScript components can raise event messages on themselves through ez.TypescriptComponent.BroadcastEvent(). Note: At the moment TypeScript components can't raise event messages on other components or game objects. Handling Messages To handle messages of a specific type, a component needs a function that takes that message type as its only parameter, and it must register that function as a message handler: static RegisterMessageHandlers() { ez.TypescriptComponent.RegisterMessageHandler(ez.MsgSetColor, \"OnMsgSetColor\"); } OnMsgSetColor(msg: ez.MsgSetColor): void { ez.Log.Info(\"MsgSetColor: \" + msg.Color.r + \", \" + msg.Color.g + \", \" + msg.Color.b + \", \" + msg.Color.a); } The static function RegisterMessageHandlers() is the only place where your code may call ez.TypescriptComponent.RegisterMessageHandler(). It is an error to call this from anywhere else. Declaring a Message in TypeScript You declare a custom message in TypeScript by extending ez.Message: export class MsgShowText extends ez.Message { EZ_DECLARE_MESSAGE_TYPE; text: string; } Important: It is vital to insert EZ_DECLARE_MESSAGE_TYPE; into the body of the message to make it work. If you need to send a message from one component and handle it in other component types, you should put the message declaration into a separate .ts file and import that file from both component files. See Importing Files (require) for details. Declaring Event Messages Note: At the moment it is not supported to declare event messages. See Also TypeScript API Custom Components with TypeScript"
  },
  "pages/docs/custom-code/typescript/typescript-overview.html": {
    "href": "pages/docs/custom-code/typescript/typescript-overview.html",
    "title": "Custom Code with TypeScript | ezEngine",
    "keywords": "Custom Code with TypeScript TypeScript (TS) is a language that is built on top of JavaScript. All JavaScript is valid TypeScript, but additionally TypeScript allows you to use strong typing to improve your coding experience. The TypeScript code itself is never executed, instead it is transpiled from TypeScript into pure JavaScript, and then interpreted by a regular JavaScript interpreter. The benefit of using TypeScript is mostly in ease of use. You can edit TypeScript code with any text editor. However, ezEditor assumes that you have Visual Studio Code (VSC) installed. When you open a TS file from the editor, it will open an entire workspace in VSC, which contains references to all your project's data directories. This enables VSC to give you full code-completion and type-checking functionality. Editing TypeScript Code Providing a decent text editing experience for code is difficult. Therefore ezEditor doesn't even attempt to. All code editing has to be done in an external editor, usually meaning Visual Studio Code. For details see the TypeScript asset. Extending the Engine with TypeScript The TypeScript integration allows you to create custom components. TypeScript components can interact both with each other, as well as with C++ components. The APIs available to TypeScript code are deliberately very similar to their C++ counterparts, to make it easy to migrate a TypeScript component to C++, if the need arises. At the moment you can't use TypeScript to write things like custom game states. Instantiating TypeScript Components TypeScript code is executed through the TypeScript component. This is effectively a C++ component which forwards everything of relevance to script code and back. Therefore you never add a script directly to a game object, instead you attach a TypeScript component, which then references the desired script. Compiling TypeScript Code All functionality that TypeScript provides over JavaScript is technically possible to do with pure JavaScript, it is just very cumbersome. Therefore any piece of TypeScript can be transformed to (more complex) JavaScript code. This step is called transpiling. All script code in a project has to be fully transpiled before a scene can be simulated. Therefore running a scene always triggers the transpilation step. Since the TypeScript transpiler is itself written in JavaScript and therefore executed in a JavaScript virtual machine, this step is unfortunately quite slow, especially for larger files. All results are cached, though, so only modified files ever need to be transpiled again. Note that C++ reflection information is used to expose C++ components, enums, and messages to TypeScript, meaning that certain changes of C++ code can also trigger re-transpilation of some TypeScript files. Messaging TypeScript code can use messages to communicate both with other TS components as well as with C++ components. TypeScript code can handle any message, and it can send (or post) any message. To communicate with another TS component, you can define custom message types directly in script code. To communicate with a C++ component, only C++ messages can be used, as the C++ code has no means to know and handle a TypeScript message. If necessary to do so, the custom message type has to be defined in C++. See Messaging in TypeScript Code for details. Functionality Available in TypeScript The TypeScript binding is a mixture of auto-generated code and hand-crafted APIs specifically tailored to provide a smooth experience. Auto-Generated Code Where possible reflection information is used to automatically generate the necessary TypeScript code to give Visual Studio Code the needed information for code-completion and error-checking. This is, for example, used to expose all C++ components, enums, flags, and messages. The generated TS code is stored in each project in the folder TypeScript/ez and you will notice that for instance the file AllComponents.ts is re-transpiled when a C++ component is added. Consequently, for things like messages and components, only reflected parts can be available to TypeScript. For components this is obvious, as only reflected parts will show up in the editor UI as well, but for messages you may come across a C++ message for which members are missing in the TS version, as reflecting message properties is technically not necessary for the message to work. If you do need that message on the TypeScript side, you need to add the proper reflection information. Additionally, not all kinds of reflected properties are currently supported for TypeScript. Array, map and set properties are not available, as well as game object handles and component handles. Such reflected properties are simply not included in the auto-generated TS code. Hand-Crafted Code Although auto-generating code is the most convenient to keep large amounts of code in sync, there are limits what can be done. TypeScript and C++ are often very different and to not compromise the usability or performance of either side, the way some aspects are exposed to TypeScript has to be chosen carefully. Basic types such as the math classes (Vec2, Vec3, Mat4, Quat, ...) have been designed to be very similar to their C++ counterparts. However, a big design goal was to minimize the amount of temporaries ('garbage') produced when using them, as this has a significant impact on performance. Therefore, using those classes you will notice that their functions often work 'in place', instead of returning a modified clone. Additionally, TypeScript doesn't support function overloading, which is why the TypeScript variants of ez.Vec3, etc use more explicit function names for disambiguation. Finally, bindings for larger systems, such as worlds or physics are also built by hand. Here, exposing the C++ API 1:1 to TypeScript would simply not yield a good user experience. Instead, the TypeScript binding is fine tuned to expose useful functionality, and to hide pointless complexity. If you need full control over every aspect, there is no way around using C++ anyway. Consequently, if you decide that scripting is fine for your use case, the binding tries to make this as convenient as possible. Video See Also TypeScript Visual Studio Code Custom Code TypeScript Asset Custom Components with TypeScript"
  },
  "pages/docs/custom-code/visual-script/script-component.html": {
    "href": "pages/docs/custom-code/visual-script/script-component.html",
    "title": "Script Component | ezEngine",
    "keywords": "Script Component The Script Component represents a custom component that was written using visual scripting. The component itself is a C++ component. It mediates between C++ and the visual script by forwarding C++ events and messages to the script and back. Component Properties HandleGlobalEvents: If enabled, this component acts as a Global Event Message Handler. This is useful for scripts that should implement logic for an entire level. Script: The visual script to execute. Parameters: In case the referenced script has exposed parameters, they are listed here and can be modified. When the script gets instantiated, the values of these parameters are passed into the script. See Also Custom Code with Visual Scripts Visual Script Class Asset Custom Code"
  },
  "pages/docs/custom-code/visual-script/visual-script-class-asset.html": {
    "href": "pages/docs/custom-code/visual-script/visual-script-class-asset.html",
    "title": "Visual Script Class Asset | ezEngine",
    "keywords": "Visual Script Class Asset The Visual Script Class asset enables you to define custom logic for components and state machines using a visual programming language. Its intended use is to bridge the gap between what other components provide. For example a trigger component provides an event when something enters an area, and a spawn component can spawn a prefab, however, to have a creature spawn in a room when the player enters it, you need something that connects the two. Visual scripts are a great way to accomplish this. Visual Script Editor The image above shows the visual script editor layout. On the left is the graph editor where you can add and connect nodes. Right-click and drag to pan the view. Use the mouse wheel to zoom. On the right is the property panel which shows the properties of the selected node. Node connections cannot be selected and don't have properties. When nothing is selected, as in this case, the general script properties are displayed. Script Base Class Deselect all nodes to see the script properties. The Base Class property defines in which scenario the script may be used. Base Class: Component When Component is selected as the base class, the script acts like a component. Component specific functions like Component::GetScriptOwner() are only available with this base class. These scripts are used in conjunction with the script component to execute them. Base Class: State Machine State When StateMachineState is selected as the base class, the script acts like a custom state for a state machine. In this case, different functions are available, such as StateMachineState::GetScriptOwner(). These scripts are used in state machines through the script state. Visual Script Variables Through the general script properties you can add variables to your script. These may just be internal variables to keep track of state, but when the Expose flag is enabled, they become exposed parameters. These variables will show up where the script is used (for example on script components) and allow you to pass in different starting values. Editing Visual Scripts To build a visual script, right-click into the graph editor to open the context menu: The menu shows all the available nodes that can be added. Type into the menu's header to search for specific items. Nodes have pins on the left or right side with which they can be connected. The flow of execution and data goes from left to right. You connect pins via drag and drop. Every pin has a color-coded type. Not all pin types are compatible. Once you start dragging from a pin, all incompatible pins are greyed out and connections snap towards compatible pins. Depending on the target pin type, a data conversion may happen. For example, if a number pin is connected with a string pin, the number will be converted to text. Nodes When you select a node, you will see its properties on the right. When a node has input pins (pins on the left), it often also has properties for those pins. This is used for setting an input to a constant value, rather than passing it in through the pin. So when an input pin is unconnected, the script will use the value from the property grid. Some nodes have additional configuration properties, that cannot be passed in through a pin, for example GameObject::TryGetComponentOfBaseType() where you have to select the desired base type. Execution Pins Execution pins are the grey, arrow-shaped pins at the top of nodes. Not all nodes have them. These pins define the order in which nodes are executed. Only very few nodes have only an outgoing execution pin. These are entry points for the script, meaning they are where script execution starts. For instance, in the image at the top, the Update node is an entry point. Whenever the script gets updated (usually once every frame), this node gets executed. After that, the node that is connected to it through an execution pin gets executed. This continue until the last node in a chain was executed. Some nodes have multiple outgoing execution pins, such as the Switch nodes. These nodes are used to conditionally execute one or another code path. When a node has an incoming execution pin, it must be connected to something, otherwise the node can never be executed. However, nodes that do not have execution pins, at all, are executed on demand whenever their output is needed by another node that is being executed. Data Pins All the round pins are data pins meaning they represent some kind of data. Many different types of data are supported. Numbers are generally convertible into other number types, and nearly everything can be converted to the string type, but most other types mostly exist to be passed unmodified from one node to another. Coroutines The flow of execution starts at an entry point node and follows the execution pins to the right until the last node is reached. Usually this will happen within on script update and thus all nodes along the path are executed within the same game tick. Coroutines allow you to pause execution at any point in the graph, and have the script continue there at a later time. Consequently, a script may have multiple threads of execution, meaning that there might be several execution paths active over a longer period of time. Have a look at this script: There are two entry points, the OnEnter node and the Update node. OnEnter is only executed once when the state machine state got activated. However, Update is executed once every game tick (every frame). The OnEnter code path uses the Wait function. This turns the entire execution path into a coroutine. What this means is that when Wait is encountered, this execution path pauses for a second. After the wait is over, it continues from that point until it runs into the next Wait call. Tip: When an execution path uses coroutine functionality and thus may execute over a longer duration, the entry point node of that path shows an extra icon of two crossing arrows in its top left corner. See the OnEnter node in the image above and compare it to the Update node, which is not a coroutine. In the mean time, the Update node is executed every frame. Thus when it runs, you already have two threads of execution, the one starting in the Update node which finishes right away, and the one that started from the OnEnter which is dormant for a time, but continues after a while. Coroutines are a powerful and very convenient feature, as they make it possible to write code in a very linear fashion, even though there are complex, temporal dependencies. A common use case for coroutines is to sequence AI tasks or quest objectives. For example one may instruct an NPC to walk to a position and then sit down. The command to walk somewhere is given through a node. But then an AI system has to calculate a path and steer the creature around obstacles to make it reach that point. None of this is part of the visual script, instead the walkTo node would be a coroutine node that pauses the script until the task is fulfilled or failed. On success the script would then run the next node to play the sit down animation. So the script is very simple, even though walking to a spot is a very complex operation. Coroutine Modes When an entry point node gets executed that already spawn a thread of execution before, and that thread is not yet finished, there are three different ways to continue. Stop Other: In this case, the existing coroutine gets canceled without notice. Use this mode when you always only want to react to the latest update. For example, you may have a coroutine that moves a creature to a picked location when the player clicks somewhere. Once the player clicks somewhere else, you would want the creature only to walk to the new target, and want to cancel the previous coroutine. Don't Create New: In this mode you let an existing coroutine fully finish before starting a new one. For example a door would react to a button press by fully opening or closing and only react to another button press when the first action is over. Allow Overlap: In this mode every single event would spawn a new coroutine, which all execute in parallel. For example a timer may fire once a second and every time you want to react to this by doing something complex, then you would use this mode to react to all events equally. The coroutine mode is selectable on every entry point node. Advanced Coroutine Features Every entry point provides a coroutine ID. This can be used to cancel a specific coroutine if needed. Similarly, there are functions to stop all coroutines or start separate ones. You can also use the Yield node to interrupt a script at a specific point and have it continue in the next frame. Loops and Arrays You can execute loops to iterate over data or repeat certain actions. Several different loop nodes are available. They all operate in the same way, that they have two outgoing execution pins. One execution pin is for the loop body. This execution path will be executed repeatedly until the loop is finished. Finally, the completed execution pin is executed to continue with the code that comes after the loop. For example, the following script loops from 0 to 8 (inclusive) and for each iteration it adds the loop index to the counter variable. After the loop has been completed the execution flow continues at the completed pin, so a SetColor message is sent to the owner game object. Also note that the loop body contains a Yield statement so the loop is paused after every iteration and resumed the next frame (see the coroutines section above). Loops are often used together with arrays. Various array operations such as PushBack, GetCount, Contains and Remove are available: Node Types The following broad categories of nodes exist: Event Handlers Event handlers are nodes that get executed when a certain message is sent to any of the objects that this script is responsible for. All event handlers are entry points into the script and most scripts will only execute as a reaction to an event. Blackboards All the nodes for working with blackboards. Clock There are two clocks, the global clock and the world clock. The global one always advances in real-time and should be used for animating things that are independent of the game speed. The world clock should be used for all game-play functionality that should slow down or speed up according to the game's speed, so that they work correctly in slow-motion. Component Here you find all functionality shared by all components such as: GetOwner: Returns the components owner game object. GetWorld: Returns the world that the component belongs to. Additionally, all component specific functionality can be found in the sub-menus. Coroutine Here you find all functionality to work with coroutines, see the coroutines section above. Debug These nodes are for debug rendering. Enums These nodes are for working with enum values. There are two node types for each enum. The value nodes just return a fixed value and can be used to pass along. The switch nodes are used for reading an enum value and then executing a code path depending on the value. Game Object Game objects nodes are for reading and writing object transforms, finding and accessing child objects and components. Log Nodes for logging. Logic This group contains mathematical logic operators as well as conditions and loops. Very important nodes are: Branch: An if condition node with two possible outcomes. Switch: Several variants to map one value to multiple possible outcomes. Compare: Checks whether two values are equal, with a boolean result. Is Valid: Checks whether the incoming value, such as a game object or component, can still be used. Math All sorts of mathematical operations for working with number types. Messages While event handlers react to messages, the script can also send messages to other objects. Messages can be sent directly to a component, or to a game object, in which case they may be broadcast to all components on that object, or even to the whole sub-tree of objects and components. If the Send Mode is set to Event, however, they are delivered not downwards in the hierarchy, but upwards along the parent chain of the target object, to the closest component that handles this type of message. See this chapter for details. Property For reading and writing component properties. StateMachineInstance and StateMachineState For interacting with state machines. This is mainly necessary when the script itself is used as a StateMachineState. Be aware that the state machine instance in which the script is run, is passed into the script through the OnEnter, OnExit and Update nodes. String For working with strings, e.g. to format a string by combining variable values. Time For working with the time data type. Type Conversion These nodes are for converting variables from one type to another. Especially important is the ConvertTo node, which is used for converting a Variant to an expected type. A Variant is a variable that can contain data of many different types. When a node returns a variant, you usually expect that it contains a certain type and using the ConvertTo node, you can get to it. Variable Nodes These nodes operate on visual script variables. The variables have to be declared on the script first. Use these to keep track of state within the script and also to read state that was passed in through exposed parameters. World These nodes provide access to the world, which is used for managing objects. See Also Custom Code with Visual Scripts Script Component"
  },
  "pages/docs/custom-code/visual-script/visual-script-overview.html": {
    "href": "pages/docs/custom-code/visual-script/visual-script-overview.html",
    "title": "Custom Code with Visual Scripts | ezEngine",
    "keywords": "Custom Code with Visual Scripts The engine supports visual scripting as a way to execute custom logic without writing code. Visual scripting aims to be quick and easy to use, while providing a subset of the engine's features that is most useful for the intended use cases. Visual Script Use Cases Visual scripting is meant for small scripts that deal with simple tasks. Often they act as glue code between other systems. For example to wait for an event from one component and then instruct another component to do something. Such logic would be overly cumbersome to set up in C++, and the performance difference is negligible. Visual script code may also be used to quickly prototype behavior to get an idea how something might work. However, dealing with edge-cases and errors typically involves a lot of complex code, and once the stage is reached where a feature should be fully fledged out and polished, it might be better to migrate to C++. In general, visual scripting is no substitue for C++. It only provides a subset of the features. A lot of functionality is deliberately left inaccessible, such as working with resources (TODO) or game states. If you need to access them, you definitely should use C++. Custom Components The Visual Script Class Asset allows you to write custom components to interface with other components within the same object hierarchy. Custom State Machine States The Visual Script Class Asset may also be used to write custom state machine states. In this case the script code is executed by a state machine whenever the state is active. Performance Considerations Most visual script code actually calls existing C++ functions. A custom virtual machine ministrates those calls. The execution is relatively fast and shouldn't be a concern for the intended use cases. It is more likely that you run into the limitations of editing the script, than performance bottlenecks. See Also Custom Code Visual Script Class Asset"
  },
  "pages/docs/debugging/components/debug-text-component.html": {
    "href": "pages/docs/debugging/components/debug-text-component.html",
    "title": "Debug Text Component | ezEngine",
    "keywords": "Debug Text Component The debug text component can be used to display a short piece of text at the location of the object. Important: If this component is attached to an object that has no other rendered component, its text will not show up! This component will only render its text, if the parent object is already being rendered because of other things, such as an attached mesh. Component Properties Text: The text to display. It may contain placeholders for up to four float values, e.g. {0}, {1}, etc. The values of the properties Value0 to Value3 will be embedded into the text. Value0, Value1, Value2, Value3: Four float properties that can be set to arbitrary values. If Text contains appropriate placeholders, these values will be displayed. Color: The color of the text. See Also Debug Rendering"
  },
  "pages/docs/debugging/components/draw-line-component.html": {
    "href": "pages/docs/debugging/components/draw-line-component.html",
    "title": "DrawLineToObject Component | ezEngine",
    "keywords": "DrawLineToObject Component The line-to component is a utility component that simply draws a colored line from its own location towards the position of a referenced object. This can be useful for debugging various aspects. Component Properties Target: The referenced object to which to draw the line. Color: The color of the line. See Also Debug Rendering"
  },
  "pages/docs/debugging/console.html": {
    "href": "pages/docs/debugging/console.html",
    "title": "Console | ezEngine",
    "keywords": "Console The in-game console is a utility for inspecting the log, modifying CVars and calling console functions. Key bindings The default key binding for the console is: F1 - Opens/closes the console. Up / Down - Select a previously entered command from the history. Note that the history is saved to disk so that commands don't need to be typed again after restarting. F2 and F3 - Repeat last and second-to-last commands. This works even when the console is currently closed. ESC - Clears the input line. Page Up / Page Down - Scrolls the log output up / down. TAB - Auto-completes the current input. Also displays all available input options in the output. Ie. lists the names of CVars and console functions and prints their descriptions. Enter - Executes the typed command. If the typed text is only the name of a CVar without an assignment, this will simply print the current value and the description of the CVar. Modify CVars You can modify CVars by typing: CVarName = value See the CVars chapter for details. Binding Keys To bind commands to certain keys you can call: bind f g_showFPS= This would bind the command 'g_showFPS=' (which toggles the display of the FPS counter) to the f-key. You can only bind commands to printable characters (a-z, 0-9) and the casing matters. So you can also bind another command to SHIFT+f by using bind F .... To unbind a key call: unbind f Search You can filter the output of the console (the log messages) to only strings that contain some string by typing a * at the beginning: *some text Now the output window will only show strings that contain 'some text'. Console Functions Console functions are an easy way to expose C++ utility functions through the console. The class ezConsoleFunction is used to wrap any function (static or method function) in a delegate and enable the console to call it. Of course, since the user can only input certain types of variables in the console, the argument types that you can use are very limited: strings, numbers (int / float) and boolean. This code snippet shows how to declare a console function in a class, for example inside a custom game state. void ConFunc_Print(ezString sText); ezConsoleFunction<void(ezString)> m_ConFunc_Print; In the implementation the binding has to be completed. You need to provide a name under which to expose the function, a description (this should include the parameter list) and the actual function to forward the call to. For member functions this has to be an ezDelegate to also bind to the class instance (this). SampleGameState::SampleGameState() : m_ConFunc_Print(\"Print\", \"(string arg1): Prints 'arg1' to the log\", ezMakeDelegate(&SampleGameState::ConFunc_Print, this)) { } void SampleGameState::ConFunc_Print(ezString sText) { ezLog::Info(\"Text: '{}'\", sText); } When you now open the console (F1) in-game and press TAB, the 'Print' function will be among the listed functions. You can then execute it: Print(\"Hello Console\") If you need to call a certain function repeatedly, you can bind the call to a key or use F2 and F3 to repeat it, as long as it is the last or second-to-last command in your history. TypeScript You can also register custom console functions through the TypeScript API. See Also CVars Logging"
  },
  "pages/docs/debugging/cvars.html": {
    "href": "pages/docs/debugging/cvars.html",
    "title": "CVars | ezEngine",
    "keywords": "CVars CVars are global variables used for configuring the runtime. They are used for development to enable or modify hidden features. Types of CVars Only a strictly limited set of CVar types is supported: ezCVarBool ezCVarFloat ezCVarInt ezCVarString Accessing and Modifying CVars CVars are exposed in multiple ways. ezEditor In ezEditor you can open Panels > CVars to show a panel that allows you to modify CVars. Be aware that some CVars only have an effect when simulating the scene, and some even only when using Play-the-Game mode. The latter mostly happens when the effect of a CVar is implemented by a Game State. ezInspector ezInspector allows you to modify CVars of the connected process using the same UI as the editor. In-Game Console In-game a convenient way to modify CVars is the console. Press TAB to list all CVars Type the beginning of a CVar name and press TAB to list CVars with just that prefix name. Type CVarName = value to modify the CVar's value. cvar_bool = true cvar_bool = cvar_int = 3 cvar_string = \"test\" For boolean CVars, typing 'var =' will toggle the variables value, which can be very handy, especially combined with using F2 to repeat the previous console command. You can also do basic arithmetic like so: cvar_bool = not cvar_bool cvar_int = cvar_int + 1 TypeScript CVars can also be accessed through the TypeScript API. Command Line CVars can be set through command line arguments using this syntax: MyGame.exe -CvarName Value For example: MyGame.exe -Game.DebugDisplay true -fmod_MasterVolume 0.1 Values specified through the command line take precedence over stored values. Saving State The value of a CVar is typically discarded when the program closes, however, if the CVar uses ezCVarFlags::Save, it will be saved and restored in the next run. Be careful with this flag, as it can be very confusing when it is used to toggle subtle behavior. Be especially careful keeping this flag in for production code. There is also ezCVarFlags::RequiresRestart which means that modifying that variable will take no effect unless you restart the application. This can be used for things like screen resolutions and other initial values. Callbacks You can subscribe to events for either all CVars or specific ones, to be informed when a CVar is modified. Example Code You create a CVar simply by instantiating it as a global variable somewhere in a cpp file: #include <Foundation/Configuration/CVar.h> ezCVarBool cvar_DebugDisplay(\"Game.DebugDisplay\", false, ezCVarFlags::Default, \"Whether the game should display debug geometry.\"); Then you can just treat it like a regular variable to read or write its value: if (cvar_DebugDisplay) { ezDebugRenderer::DrawLineSphere(m_pMainWorld, ezBoundingSphere::MakeFromCenterAndRadius(ezVec3::MakeZero(), 1.0f), ezColor::Orange); } See Also Console ezInspector"
  },
  "pages/docs/debugging/debug-cpp.html": {
    "href": "pages/docs/debugging/debug-cpp.html",
    "title": "Debugging C++ Code | ezEngine",
    "keywords": "Debugging C++ Code Visual Studio Debug Visualizers To ease debugging the C++ code, we provide a natvis file, located at Code/Engine/Foundation/ezEngine.natvis. The file adds improved inspection for EZ specific code, such as the container and string classes. This file is already referenced by our CMake scripts in the Foundation library and therefore works out of the box. Debugging the Editor Please be aware that the editor uses at least one, but potentially multiple child processes for its operation. Everything that is actually 3D rendered, is done by the EditorEngineProcess.exe, which is spawned when the editor opens a project. This makes the editor more resilient. However, it means that if you launch the editor in a debugger, by default breakpoints inside the engine runtime code cannot be hit, as you are not attached to the right process. You can attach manually to this process. However, it is much easier to use an extension for Visual Studio: Microsoft Child Process Debugging Power Tool 2022 Microsoft Child Process Debugging Power Tool 2019 This tool enables Visual Studio to automatically attach to all child processes spawned by the parent process, which makes debugging the EZ editor much easier. After installing the extension go to Debug > Other Debug Targets > Child Process Debugging Settings.... There you need to enable child process debugging. Additionally, you may want to exclude debugging Git (git.exe) and Visual Studio Code (code.exe), as those may be spawned by the EZ editor (indirectly) and have some annoying behavior when attached to with a debugger: Hot Reloading C++ Game Code You can't reload any C++ code at runtime. You can, however, reload the entire engine process in the editor. See this chapter for details. See Also Debug Rendering Jolt Debug Visualizations Shader Debugging Hot Reloading C++ Game Plugins in the Editor"
  },
  "pages/docs/debugging/debug-rendering.html": {
    "href": "pages/docs/debugging/debug-rendering.html",
    "title": "Debug Rendering | ezEngine",
    "keywords": "Debug Rendering The rendering in EZ is optimized to handle large and complex data efficiently. However, to achieve this you must implement certain patterns, and to get started a non-trivial amount of code is involved. To visualize simple things, the ezDebugRenderer is provided. This class allows you to quickly add debug visualizations to your game, with just a few lines of code. This interface is also (partially) available through TypeScript. Usage The ezDebugRenderer class has a number of static functions that you can call from any thread and at any time. The debug renderer gathers all requests and renders them at a specific point in the frame. Afterwards all requests are cleared and you must resubmit the same calls in the next frame, if you want it to appear again. Render Context Every function in ezDebugRenderer takes an ezDebugRendererContext as its first parameter. This specifies where the debug geometry should be rendered. You can either pass in an ezViewHandle or an ezWorld here. If you pass in a view handle, the geometry will only be rendered in the corresponding ezView, meaning you can have the debug geometry only appear in a specific render target. If you pass in an ezWorld, the geometry appears in all views that render that world. It is more common to bind debug geometry to an entire ezWorld, such that it appears in any view. The TypeScript binding omits the ezDebugRendererContext parameter and always binds it to the world. Debug Geometry Types The debug renderer allows you to render arbitrary lines and triangles. For convenience it also provides functions to render a number of common shapes: 3D lines 2D lines (screenspace) Boxes (wireframe) Boxes (solid) Spheres (wireframe) Capsules (wireframe) Frustums (wireframe) 3D triangles (solid) 3D triangles (textured) 2D rectangles (solid, screenspace) 2D rectangles (textured, screenspace) 2D text (screenspace) 2D info text (screenspace, automatically placed non-overlapping) 3D text (3D location, fixed size and always facing the camera) Shading The debug renderer always uses a fullbright shading model. No lighting is ever applied to debug geometry. In fact, most geometry simply has a color, and only some functions allow you to choose a texture. Example A full example for how to use the debug renderer is given in the Sample Game Plugin. Here, the DebugRenderComponent shows how to utilize the debug renderer. The following code snippet is sufficient to render a wireframe sphere at the location of the component: ezBoundingSphere sphere = ezBoundingSphere::MakeFromCenterAndRadius(ezVec3::MakeZero(), m_fSize); ezDebugRenderer::DrawLineSphere(GetWorld(), sphere, m_Color, ownerTransform); This has to be called in every frame in which it should appear. Therefore this has been added to the DebugRenderComponent::Update() function. Performance Considerations The debug renderer is meant for quickly visualizing data. If the data potentially changes every frame anyway (like visualizing AI raycasts), its performance will be on par with what a 'proper' solution could do. However, if you want to place things in a scene that may stay for a longer duration, it's usually much better to instead build a game object, attach a mesh component and just render a proper asset. This enables culling, static object optimizations, and gives you the option to choose materials. See Also Debugging C++ Code Shader Debugging Jolt Debug Visualizations Custom Code with TypeScript"
  },
  "pages/docs/debugging/logging.html": {
    "href": "pages/docs/debugging/logging.html",
    "title": "Logging | ezEngine",
    "keywords": "Logging Log messages are often very helpful in finding problems. Logging Information The log is accessible through the ezLog class. There are multiple functions to log information of different severity: ezLog::Debug - for verbose output, will be compiled out in non-debug builds ezLog::Dev - for output during development, will typically be silenced (but not compiled out) in non-development builds ezLog::Info - for regular information ezLog::Warning - for important information that may point at problems ezLog::SeriousWarning - for problems that should be fixed but won't crash the system just now ezLog::Error - for errors Log messages can be grouped using the EZ_LOG_BLOCK macro. TypeScript Information can also be logged through the TypeScript API. Inspecting the Log There are multiple ways to see the content of the log: In the ezEditor you can open Panels > Log to see two logs, the one for the editor and the one from the engine process. ezInspector shows the log of the connected process. The in-game console outputs the log messages. By default all EZ applications (TODO) also write the log output to a Log.htm file in the application's appdata folder. Thread-local logging The logging system uses a thread-local variable to store the active logging system, through which to route all messages that originate on that thread. This can be used to easily replace the entire logging backend on a thread and capture all log messages in a custom backend. See ezLogSystemScope and ezLogInterface, if you want to write a custom backend. This can be used to, for example, capture all output from some subsystem and prevent the messages from reaching the regular outputs. Custom Log Writers ezLog is the central class for all messages to be logged. By default, it routes all messages through an instance of ezGlobalLog, though you can redirect this on the calling side if you want. On ezGlobalLog you can register multiple handlers that take the messages and either write them to some output or forward them to another system. This method is used to, for instance, forward log messages from one process to another. The ezInspector integration, for example, registers a custom log writer to gather all log messages, and send them over the network, for display in the external tool. ezEditor does something similar for the messages from the engine process. For an in-depth explanation of how you can configure the system, see ezLog and ezGlobalLog. See Also ezInspector Console"
  },
  "pages/docs/debugging/renderdoc.html": {
    "href": "pages/docs/debugging/renderdoc.html",
    "title": "RenderDoc Integration | ezEngine",
    "keywords": "RenderDoc Integration RenderDoc is a great tool to capture rendering commands for analysis and debugging. Commonly, to analyze a rendering issue, one would launch an application through RenderDoc, such that it can hook into the application and record rendering commands. ezEngine has a dedicated RenderDocPlugin, to integrate RenderDoc support even better. When that plugin is active (see Project Settings) you can trigger a RenderDoc capture at any time, even if your application was not launched through RenderDoc. Taking Captures If you write your own application (TODO) you can hook up RenderDoc in different ways, however, by default these methods are available: Press F11: The F11 key will take a capture of the current frame. Type CaptureFrame() into the game console. All captures are written to a sub-folder of the appdata data directory. On Windows this refers to the %appdata% folder, which you can find by typing %appdata% into Windows Explorer. The exact sub-folder is printed into the log (or see the in-game console). You can then open the capture using RenderDoc. See Also ezInspector Profiling Common Application Features"
  },
  "pages/docs/debugging/stats.html": {
    "href": "pages/docs/debugging/stats.html",
    "title": "Stats | ezEngine",
    "keywords": "Stats Stats are a feature for development and debugging. They are arbitrary key/value pairs that your application can set and update frequently, typically once per frame. Stats can be seen and plotted as a graph in ezInspector. C++ Interface The stats system is available through ezStats. This class allows you to add, modify and remove stats. The ezInspector plugin will listen to all changes and send the updated values over the network to ezInspector for visualization. You can make ezInspector put your stats into a tree structure by using slashes in the stat name. Use Cases Common use cases for stats are to output: Number of things of interest (render polygons, active NPCs, ...) Time spent doing X (script execution, AI updates, ...) Network status (bandwidth usage, ...) Stats are very useful to give an insight into data that is otherwise hidden. They are particularly useful when the way a value behaves is of interest. That means when a value fluctuates and thus may be responsible for inconsistent performance or sudden spikes. When such a value is plotted as a graph in ezInspector and put side-by-side with the frame time, it can become easier to find correlations. ezInspector Stats appear in a tree structure. You can mark certain stats as 'favorites' using the checkmarks. Using the context menu, you can assign a stat to one of ten graphs, where its value will be plotted over time. Make sure to adjust the graph's min and max value range. See Also ezInspector"
  },
  "pages/docs/docs-overview.html": {
    "href": "pages/docs/docs-overview.html",
    "title": "Welcome | ezEngine",
    "keywords": "Welcome Welcome to the ezEngine documentation. In the sidebar on the left you'll find pages documenting the vast majority of features. You can either browse by topic, or you can use the searchbar on the top right to search by keywords. All pages link to related topics at their bottom in the See Also section. Some pages are marked with TODO. For these topics the documentation is not yet written. Generally, all features that have documentation are also in a good enough state to be used. Features that are undocumented are typically not yet good enough to be used productively, unless their documentation page says differently. Those features are subject to breaking changes in the future."
  },
  "pages/docs/editor/dashboard.html": {
    "href": "pages/docs/editor/dashboard.html",
    "title": "Dashboard | ezEngine",
    "keywords": "Dashboard The editor dashboard is a welcome screen that helps you getting started quickly. The buttons on the left-hand switch between the available options. Projects On the projects page you can easily open a recently used project. To create a new project, click the New button. To open an existing project that isn't shown in the list, click the Browse... button. To open a project selected from the list, use the Open Project button or double click the list entry. If you uncheck Always load last project, then the editor won't automatically open the project that you had open last time, when it starts up. Instead, it will always display the dashboard. Samples On the samples page you can see all the available sample projects. Open one via the Open Sample button or double-click. Docs & Community On the community page you find buttons to quickly go to the available online resources, such as the API Docs and contact options. See Also Projects Contact"
  },
  "pages/docs/editor/editor-bg-operations.html": {
    "href": "pages/docs/editor/editor-bg-operations.html",
    "title": "Editor Background Operations | ezEngine",
    "keywords": "Editor Background Operations Most data that takes time to be generated or converted, comes from assets. However, there is also scene specific data, which does not qualify as an asset. However, generating it can take very long and therefore should only be triggered on demand. Examples include precomputed lighting or navmeshes. For complex scenes these processes can take very long. Also, if they become out of date, this is usually not a big problem immediately, which means updating the data can be delayed until it is really necessary. Certain components expose background operations. Once you place such a component in a level, its background operation will show up in the Background Operations panel. You can find this under Panels > Background Operations: This panel lists all background operations for all documents. Click Start to make one run in the background. Double click a row to jump directly to the corresponding component. Important: Background operations are currently never run automatically, it is your responsibility to decide when to execute them. Sharing Generated Data The generated data is written to the project's AssetCache/Generated sub-folder. There it is typically ignored by git and thus won't show up for source control check-in. Therefore, either every user has to regenerate the data locally themselves, or you need to force add the desired files to git. Be aware that this type of data typically changes every time and a different storage solution, for example a shared folder on a network drive, may be a better solution. Currently there is no built-in option to change the location where this data is stored, so you would need to use Windows file junctions to redirect the folder or individual files to a different location. See Also Assets"
  },
  "pages/docs/editor/editor-documents.html": {
    "href": "pages/docs/editor/editor-documents.html",
    "title": "Editor Documents | ezEngine",
    "keywords": "Editor Documents In the editor nearly everything that you work with is a dedicated document. Most documents are backed by a file, which is usually in OpenDDL format. Documents are an editor specific representation of things like scenes, prefab, materials and so on. The document format is not understood by the runtime and therefore cannot be loaded by applications like ezPlayer directly. A document can represent any type of data. However, the most common case is, that it represents data that shall be converted (or transformed) from its source representation into something that the engine runtime can use. For example a texture must be transformed from a source JPG format to a more optimized DDS format. These types of documents are called assets in EZ and they are by far the most common type of document. The only exception to documents in the editor are things like project settings and editor settings, which are not handled by documents. Video See Also Assets"
  },
  "pages/docs/editor/editor-plugins.html": {
    "href": "pages/docs/editor/editor-plugins.html",
    "title": "Editor Plugins | ezEngine",
    "keywords": "Editor Plugins Editor plugins are fully functional, but currently undocumented. See Also Engine Plugins"
  },
  "pages/docs/editor/editor-settings.html": {
    "href": "pages/docs/editor/editor-settings.html",
    "title": "Editor Settings | ezEngine",
    "keywords": "Editor Settings Editor settings are user specific and independent of projects. They are stored in a OS user folder and thus are not checked into source control. Preferences Preferences are user specific editor settings. Preferences may affect the entire editor application, only a certain project, or even just a single document. The list on the left lists all the available preferences. Items prefixed with Application: affect the general editor, no matter which project is open. Items prefixed with Project: are specific to the currently open project and can be configured differently for other projects. Per-document preferences only show up while a document is open. Application: C++ projects C++ IDE: Select which IDE you want to use as default when opening C++ projects. Compiler Preferences: Settings on what compiler to use to compile C++ projects. Compiler Preset: Select one of the auto detected compiler presets available on your system, or use a \"custom\" preset to manually configure a compiler. C++ Compiler: Path to the C++ compiler executable to use. C Compiler: Path to the C compiler executable to use. RC Compiler: Path to the resource compiler executable to use. Application: General Restore Project On Startup: If enabled, the editor will load the project that was open the last time. Show Splashscreen: Disable this to not have a splashscreen show up every time the editor starts. Background Asset Processing: If set, background assets processing will be activated by default, when you open a project. Field Of View: The FOV of the generic editor camera. Gizmo Size: Allows you to change the size of the editor gizmos. Show In Development Features: Some features (mainly components) are hidden by default from menus. That's because those features are not considered ready to be used productively. If you do want to try them out regardless, check this option for them to show up. Use Precompiled Tools: If enabled, the editor will prefer to use the tools under Data\\Tools\\Precompiled, rather than the ones from its own binary directory. The precompiled tools are built with maximum optimizations and are therefore typically faster, however, they only get updated infrequently and may not have all the latest features and bugfixes. Custom Precompiled Tools: If not empty use the given path instead of Data\\Tools\\Precompiled when looking for tools. This is especially usefull on Linux if you want fast tools while using a debug build of the editor. Expand Scene Tree On Selection: If enabled, selecting an object in a scene will automatically expand the corresponding item in the scene tree view. If disabled, the scene tree will not change on selection from the viewport, and jumping to the selected tree item has to be done manually using CTRL+T. Clear Editor Logs On Play: If enabled, the engine log gets cleared every time you run a scene. Highlight Untranslated UI: If enabled, the editor will highlight all texts in the UI which are missing a translation. Engine View Light Settings: Many assets have 3D previews that require lighting. These are the lighting settings to use. Shortcuts Tools > Shortcuts... opens a dialog to configure the shortcuts. Video See Also Project Settings"
  },
  "pages/docs/editor/editor-template-documents.html": {
    "href": "pages/docs/editor/editor-template-documents.html",
    "title": "Template Documents | ezEngine",
    "keywords": "Template Documents When you create a new document, it is typically blank. You can change this, by saving a pre-configured template document in your project's directory. Every time a new document is created, the editor checks whether such a template document is present, and if so, clones that instead. For the editor to find your template document, it has to be stored in the sub-folder Editor/DocumentTemplates and has to have the name Default. The file extension of course has match the document type. So for example if your project is located under C:/MyGame then to create a custom scene template, you would store a scene file under C:/MyGame/Editor/DocumentTemplates/Default.ezScene. This can be done for any document type. See Also Editor Documents"
  },
  "pages/docs/editor/editor-views.html": {
    "href": "pages/docs/editor/editor-views.html",
    "title": "Editing Views | ezEngine",
    "keywords": "Editing Views Most documents come with at least one 3D view. The scene documents allow you to switch between single-view and quad-view mode, using the Toggle Views button in each view toolbar. Using the Perspective menu in the toolbar (the eye icon) you can switch each view to either orthographic or perspective mode. Camera controls and editing gizmos act differently in orthographic and perspective mode. Render Modes Render modes are used to visualize different aspects of the scene. They can be useful for debugging rendering issues, see potential performance hotspots, or easier edit a dark scene. Most 3D viewports allow you to switch the rendering mode through a drop down menu. Default This mode renders the scene as it would appear in the final game. Wireframe In this mode the scene is rendered only as wireframe. Either monochrome or colored. Diffuse Lit Only This mode visualizes only the diffuse lighting contributions. Specular Lit Only This mode visualizes only the specular lighting contributions, including reflections. Decal Count Visualizes how many decals affect each pixel. Yellow and red areas indicate high decal overdraw and will affect performance negatively. Light Count Visualizes how many lights affect each pixel. Yellow and red areas have many contributing lights and will affect performance negatively. Static vs Dynamic This mode visualizes which objects in the scene are static (green) and which ones are dynamic (red). Dynamic objects have a per-frame performance cost, even if they don't move. This mode allows you to find objects that are unnecessarily set to be dynamic. Texture Coordinates There are two modes to visualize the UV0 and UV1 texture coordinates. Normals and Tangents There are multiple modes to visualize normals and tangents, per-vertex and per-pixel. Diffuse Color This mode only shows the diffuse color. It can be very handy for editing a scene that is otherwise very dark. Diffuse Color Range Check In Physically Based Rendering (PBR) the diffuse color values should never be too dark or too bright, as both will not give the best possible results. This mode visualizes which areas may have non-optimal diffuse colors. Emissive Color This mode visualizes which objects use emissive colors. Specular Color Visualizes the specular color. Ambient Occlusion This mode shows ambient occlusion values. These come both from dedicated AO maps, as well as screen space ambient occlusion (SSAO). Depth Visualizes the depth of all objects. Note that depending on the near and far plane settings and the camera distance to the closest object, this mode may appear nearly entirely black or white. For this screenshot the far plane had to be adjusted. Roughness Visualizes the roughness of objects. Video See Also Editor Camera Scene Editing"
  },
  "pages/docs/editor/run-scene.html": {
    "href": "pages/docs/editor/run-scene.html",
    "title": "Running a Scene | ezEngine",
    "keywords": "Running a Scene When you have a scene open in the editor, there are multiple ways how you can test it. Simulate Mode The Simulate Mode is activated with the Play button in the toolbar or by pressing the F5 key. It is deactivated with the Stop button or by pressing Shift+F5. This enables the simulation of all game objects inside the editor. You can still select objects, modify their properties and move and rotate most of them. However, since at that point the simulation actively modifies the state of objects, some things will not work. For example, physically simulated objects and other objects that actively modify their position may not react to transform changes as expected. Similarly, modifying properties may not have an effect, if the components in question don't reevaluate those properties during simulation. Some objects will also be spawned by the simulation, and therefore are unknown to the editor side. Those objects cannot be picked or modified. In general, modifying the scene during simulation will work for some objects, and won't work for others. The only way to find out, is to try it. Once the simulation is stopped, though, all modifications will be applied as expected, to the reset scene. Keep Simulation Changes The simulate mode is useful to quickly check how some object behaves. It can also be used to simulate physical objects (e.g. boxes falling down) and then save that simulated transform to the scene. While the simulation is active, select the objects that you are interested in and press K (or Scene > Utilities > Keep Simulation Changes). Once you stop the simulation, an undoable action is executed that moves the selected objects to the recorded location. You can record multiple keep changes actions during one simulation. Play the Game Mode The Play-the-Game Mode (PTG) is activated with the Controller button in the toolbar or by pressing CTRL+F5. It is deactivated with the Stop button in the editor toolbar, by pressing ESC while the PTG window has focus. This enables the simulation, similar to the Simulate Mode. However, it additionally activates the Game State. Therefore this mode spawns a separate window and also routes all input to the running game. If the scene contains a Player Start Point component the game state may spawn the referenced prefab and thus enable you to properly play the game. You can Alt+Tab out of the PTG window, back to the editor, and modify the scene the same way as in the simulate mode, with the same restrictions. This mode is useful to quickly run the game with full input. Note that, because it is running from the editor process, its framerate is limited. For maximum performance you need to Export and Run the scene. Play From Here If the scene contains a Player Start Point component, then you can right click in the scene and select Play From Here. This starts Play-the-Game and spawns the player prefab at the desired position. This makes it quick and easy to try out a feature in the scene, without having to move the player start position. Export and Run Another way to test the scene is to export it to a binary format and run it in the ezPlayer. You do so using Scene > Export and Run... or by pressing CTRL+R. If you keep Transform all Assets checked, all assets in the project will be transformed first, making sure they are up-to-date. You can uncheck this, to speed up the process, if you know that all assets that you require are up-to-date already. Update Thumbnail will use the scene camera with the Thumbnail usage hint, to create a scene thumbnail. Both Simulate Mode and Play-the-Game Mode run inside the editor process. Compared to a game running in a stand-alone process this has two drawbacks: The editor process limits the maximum framerate, and has some performance overhead of its own, so this is not useful for performance testing. All component properties are initialized from the values as the editor has saved them. This is not the (optimized) binary serialization that is used for a final game. Since the binary serialization code has to be written manually, it can happen that properties that are exposed in the editor and work fine there, have been forgotten to be included in the binary serialization and therefore have no effect in the final game. Therefore, always make sure to test custom components properly with exported scenes. Important: If a scene is composed of multiple scene layers, only the objects from the currently loaded layers are exported. Custom Player Executable In the export and run dialog you can add a custom executable to run instead of the default ezPlayer application. Use this, if you have a custom game application. The command line used is the same as for ezPlayer as shown in the dialog. See Also ezPlayer Game States Player Start Point"
  },
  "pages/docs/effects/beam-component.html": {
    "href": "pages/docs/effects/beam-component.html",
    "title": "Beam Component | ezEngine",
    "keywords": "Beam Component The beam component renders a thick line (a \"beam\") starting at the position of the beam component's owner game object and ending at the position of a selected target object. If the object at either end moves, the beam moves with it. The beam component can be used to implement laser beams, for example from trip mines. The beam component is purely a graphical effect, it has no game play functionality. It also has no logic to decide how long the beam shall be. To adjust the length of the beam, the target object has to be positioned at the desired distance. The raycast placement component works well in conjunction with the beam component, as it uses a raycast to decide where to move a referenced object to. You can attach both components to the same object and let them reference the same (dummy) target object, to get a beam that always stretches towards the closest obstacle. Component Properties TargetObject: The beam geometry starts at the beam component position and goes towards the position of the referenced object. If the target object moves, the beam follows. Material: The material to use to render the beam geometry with. Color: The tint color for the material. Width: The thickness of the beam geometry. UVUnitsPerWorldUnit: How to stretch the material across the geometry. See Also Raycast Placement Component"
  },
  "pages/docs/effects/camera-shake/camera-shake-component.html": {
    "href": "pages/docs/effects/camera-shake/camera-shake-component.html",
    "title": "Camera Shake Component | ezEngine",
    "keywords": "Camera Shake Component The camera shake component is used to apply a shaking effect to the game object that it is attached to. How much shake to apply is controlled through the MinShake and MaxShake properties. Typically MinShake is zero, meaning there is no shake, at all, but you can procedurally raise the value at any time. However, it is more convenient to instead place a camera shake volume. This allows to easily define where in a scene the camera should shake and how much. The shake is applied as a local rotation around the Y and Z axis, assuming the camera is looking along the positive X axis. Insert a dedicated shake object as a parent of your camera like this: The component could theoretically also be used to add a shake effect to other decorative objects. Component Properties MinShake: The minimum amount of shake to apply to the owner object at all times. Measured in angle, because the shake is applied as a rotation. MaxShake: The maximum amount of shake to apply to the owner object. This is used as the reference value when a shake volume has a strength of 1. See Also Camera Shake Volume Components"
  },
  "pages/docs/effects/camera-shake/camera-shake-volume-components.html": {
    "href": "pages/docs/effects/camera-shake/camera-shake-volume-components.html",
    "title": "Camera Shake Volume Components | ezEngine",
    "keywords": "Camera Shake Volume Components Camera shake volumes are used to place regions in a scene where a player's camera should vibrate. This is used in conjunction with camera shake components. The volumes have no effect on their own, instead the camera shake component will check whether it is inside such a volume, and apply its shake effect accordingly. Shared Component Properties All camera shake volumes share these properties: Strength: How much shake to apply when the camera is at the strongest point inside the volume (for a sphere that would be right at the center). This is a factor between 0 and 1, scaling the camera shake between the MinShake and MaxShake value of the camera shake component. BurstDuration: If zero, the shake is indefinite. Otherwise it is active for only a limited duration. This would be used for effects like explosions that should only last a short time. OnFinishedAction: If used as a burst, the component or entire object may get deleted afterwards. Camera Shake Volume Sphere Component This camera shake volume defines a spherical volume. The shake is strongest at its center and gradually fades out towards its edge. Sphere Component Properties Radius: The radius of the sphere volume. See Also Camera Shake Component"
  },
  "pages/docs/effects/cloth-sheet-component.html": {
    "href": "pages/docs/effects/cloth-sheet-component.html",
    "title": "Cloth Sheet Component | ezEngine",
    "keywords": "Cloth Sheet Component The cloth sheet component simulates a square patch of cloth as it hangs and swings in the wind. It is meant for decorative purposes such as flags. Cloth sheets are affected by wind and movement of the owner object. They do not interact with physics objects and they don't collide with scene geometry. Component Properties Size: The physical size of the cloth sheet in the world. Slack: How much slack the cloth has along the X and Y axis. A value of zero means it is hung perfectly straight between its anchors. Positive values make it sag downwards. Segments: How detailed to simulate the cloth. Use as low values as possible, the simulation quickly becomes prohibitively expensive with higher tesselations. Damping: How quickly the cloth loses energy while swinging. Higher values make it come to rest more quickly, low values make it swing for a longer time. Once it comes to rest, it takes significantly less processing power. WindInfluence: How strongly wind should make the cloth swing. Flags: These define at which corners and edges the sheet of cloth is attached to the world. Material: The material used for rendering the cloth. Make sure to set it to two-sided for cloth that can be seen from both sides. Color: An additional tint-color for rendering. See Also Fake Rope Component Wind"
  },
  "pages/docs/effects/decals.html": {
    "href": "pages/docs/effects/decals.html",
    "title": "Decals | ezEngine",
    "keywords": "Decals Decals are textures that are projected onto the underlying geometry. Decals can be used to to apply text and other signs to geometry. The most common use-case, though, is to make scenes look more natural by simulating wear and tear, such as dirt and scratches. Decals can also be used to simulate dynamic surface imperfections like bullet holes, soot and blood spatters. Video: How to make decals Decal Asset Before being able to place a decal component, you must create a decal asset. Decal Asset Properties Mode: The mode specifies which surface properties (color, normal, occlusion/roughness/normal) the decal will affect. If the mode is set to BaseColor only, it will change the geometry's underlying color, but nothing else. If it is set to BaseColor, Normal, it will also modify the surface's normal, etc. BlendModeColorize: If this is disabled, the decal's color texture will be applied 1:1 and the decal's alpha channel specifies the blend factor. If BlendModeColorize is true, the decal's color texture is used to 'change' the color of the underlying geometry, but not 'overwrite' it. A middle-grey value in the decal color texture means the decal will not change the underlying geometry color, at all. A darker value will darken the underlying geometry and a lighter value will lighten up the underlying color. This mode is useful for decals that should always darken or brighten the underlying geometry slightly, instead of overwriting the existing color and thus resulting in some fixed brightness. AlphaMask: An optional separate texture to specify the decal's alpha channel. If this is not specified, the decal's opacity is taken from the alpha channel of the Base Color texture. This can be used to combine a dedicated grey-scale texture to specify the decal's shape, and combine it with some arbitrary texture to specify the color pattern. Note: If an AlphaMask texture is given, all other textures are resized to be no larger than this. Base Color: The texture that defines the decal's color. If no separate AlphaMask texture is given, the alpha channel of this texture will also define the shape of the decal. Normal, ORM, Emissive: If the respective Mode is selected, these settings show up for you to specify which textures to use to modify the normal and/or occlusion/roughness/metalness. If the mode is BaseColor, Emissive, a dedicated Emissive texture can be used to specify which pixels will glow with which color, though in that case you cannot overwrite the normal or ORM values. Decal Component Each decal component represents a single instance of a decal. Its position, rotation and scale define where the decal appears. Decal Component Properties Decals: An array of decal asset references. When the game starts, a random decal from this list is chosen for display. ProjectionAxis: The axis along which to project the decal. Extents: The size of the decal along each axis. SizeVariance: If this value is non-zero, the decal's Extents will be randomized between Extents - Extents*Variance and Extents + Extents*Variance using a Normal Distribution. See Variance Values. Color: A tint color for the decal. EmissiveColor: If set to anything other than black, the decal will glow with this color. SortOrder: A float value to adjust whether this decal will appear before or behind other decals. Default is zero. Decals with higher values will be rendered on top of decals with lower values. In the screenshot above, the left EZ decal uses a positive sort order, the right one uses a negative sort order. WrapAround: If disabled, the decal is simply projected onto the geometry along the selected axis. This can result in visible stretching along orthogonal axes. When WrapAround is enabled, the depth along the projection axis is used to modify the decal's UV coordinates. This trades less stretching, for other distortions. Enabling WrapAround may be useful for 'organic' decals, such as dirt and fluid spatters. For 'mechanical' decals, such as road signs, it should be disabled. In the screenshot above, the left (yellow) decal uses WrapAround, the right (turquoise) one does not. MapNormalToGeometry: If enabled, the normal of the decal is considered to be relative to the normal of the underlying geometry. Thus it will 'adjust' the normal of the geometry and the direction from where the decal is projected has no influence on the final pixel normal. This is useful for decals that should act like a layer on top of some geometry, for example fluids and scratches. If disabled, the decal will completely overwrite the normal of the underlying geometry and thus the direction from where the decal is projected has a significant influence. This is useful for decals that should show exactly from where they were projected, for example bullet holes. InnerFadeAngle, OuterFadeAngle: When a decal is projected onto geometry at an angle (not straight down), the inner fade angle specifies at what angle the decal starts to fade out and the outer fade angle specifies at what angle the decal will be completely invisible. FadeOutDelay, FadeOutDuration: If these are non-zero, the decal will fade out over FadeOutDuration seconds starting after FadeOutDelay seconds. OnFinishedAction: If the decal component is set to fade out, it may delete itself or its entire owner object afterwards. ApplyToDynamic: By default, decals apply to static geometry but not to dynamic geometry. If it is desired for a decal to be projected onto a dynamic object, this property should be used to select exactly to which dynamic object the decal should be applied to. Note that decals can only be applied to a single dynamic game object. If the selected object turns out to be static, though, the decal will be invisible. See Also Particle Effects"
  },
  "pages/docs/effects/fog.html": {
    "href": "pages/docs/effects/fog.html",
    "title": "Fog | ezEngine",
    "keywords": "Fog By default a scene doesn't have any fog. To enable fog, you need to place a game object in the scene and attach a fog component. Fog Component The fog component is used to apply simple depth-fog to the entire scene. The image below shows a scene without fog (left) with depth-fog (middle) and with an additional height-falloff (right): The rotation and scale of the game object has no effect on the fog. The position is mostly irrelevant, except for the z-coordinate, which is used as the threshold if HeightFalloff is used. Color: The overall color of the fog. Density: The density of the fog. The higher this value, the thicker the fog will be and thus it will also become noticeable at a closer distance. HeightFalloff: If set to zero, the fog is applied equally everywhere in the scene. Otherwise, the fog will only be applied to objects below the fog object. Thus, in this case, the position of the fog object defines which objects will be inside the fog and which are outside. The HeightFalloff value defines the distance over which the height fog transitions from foggy ground to clear sky. For example, a value of 1 means the fog changes from fully foggy to non-foggy over one meter and thus gives a relatively sharp transition. A value of 10 results in a much larger and softer transition. See Also Sky Lighting Particle Effects"
  },
  "pages/docs/effects/lensflare-component.html": {
    "href": "pages/docs/effects/lensflare-component.html",
    "title": "Lensflare Component | ezEngine",
    "keywords": "Lensflare Component The lensflare component adds a screen-space effect for simulating lens flares. Lens flares can be added to any light source or even to other objects. Lens flares consists of several textures that are placed along a line in screen-space that rotates around the screen center. One end of the line is at the position of the lens flare object and the other end is mirrored across the screen center. The renderer determines how much the lens flare object is occluded and scales the intensity of the effect accordingly. Thus the effect smoothly fades in when the object becomes visible around a corner. See the video below for an example: Component Properties LinkToLightShape: Links the lens flare to the first light component on the same owner object or any of its parent objects. When a lens flare is linked it will take the light color and intensity to modulate the lens flare color and intensity for elements that have the ModulateByLightColor flag set. For directional lights the lens flare is positioned at far plane and moved with the camera to simulate a light that is at infinite distance, like the sun. For spot lights the lens flare intensity is additionally adjusted so that the it is only visible when the camera is inside the light cone. Intensity: Adjusts the overall intensity of the lens flare. OcclusionSampleRadius: Sets the world space radius in which the depth buffer is sampled to determine how much the lens flare is occluded. Typically this would be the size of the light emitting area, like a light bulb, or slightly larger. OcclusionSampleSpread: Moves the occlusion sample center towards the lens flare corner to introduce a slight gradient when the lens flare is only partially occluded. This value is relative to the sample radius (0..1 range). OcclusionDepthOffset: Adjusts the occlusion sample depth in world space. Negative values will move towards the camera. This can be used to prevent self occlusion with the light source object. ApplyFog: If enabled, fog is added on top of the flare. Elements: Array of lens flare elements. Texture: The texture to use. GreyscaleTexture: Whether the given texture is a greyscale or color texture. If enabled, the texture's alpha channel is ignored entirely and the red channel is used for all colors. Thus if you have a single-color texture, which would usually show up red, this turns it into greyscale. For color textures that are already greyscale, this will only have an effect, if their alpha channel is not entirely white. In this case choose whatever looks best. Color: A tint color. ModulateByLightColor: Modulates the element's color by the light color and intensity if the component is linked to a light component. Size: The world-space size of the element. This determines the size of the flare at varying distances. Note that the final size on screen gets clamped by MaxScreenSize, thus you can exaggerate the size, to have the flare stay visible even at large distances, without having it exceed a maximum size on screen. That is why the default is set to a very large value, because for directional light-sources that are projected to be very far away the size needs to be extremely big. MaxScreenSize: The maximum screen-space size in 0..1 range. AspectRatio: The width-to-height ratio. A ratio other than one will stretch the element along the X or Y axis. ShiftToCenter: Moves the element along the lens flare origin to screen center line. 0 is at the lens flare origin, 1 at the screen center. Values below 0 or above 1 are also possible. InverseTonemap: Applies an inverse tonemapping operation on the final color. This can be useful if the lens flare is not linked to a light or does not use an HDR color since lens flares are rendered before tonemapping and can look washed out in this case. See Also Lighting Fog Sprite Component Post-Processing Component"
  },
  "pages/docs/effects/particle-effects/how-particle-effects-work.html": {
    "href": "pages/docs/effects/particle-effects/how-particle-effects-work.html",
    "title": "How Particle Effects Work | ezEngine",
    "keywords": "How Particle Effects Work This article gives a broad introduction how particle effects work. It is meant for people completely new to this topic. The information here is not very engine specific, as particle effects conceptually work the same in all engines. Particles A particle is the smallest unit that a particle effect is made up of. Each particle has a small number of properties. Every particle has a position, a duration how long it lives, and typically also velocity (speed and direction), size and color. It may have additional properties, when needed, but those are the most common ones. In a particle effect we often have hundreds, sometimes even thousands of particles. Each particle is small, but by having hundreds of particles scattered throughout a volume, the end result is something that looks volumetric and behaves in complex patterns. There a multiple ways a single particle may get rendered. The most common method are so called billboards. A billboard is a quad which always rotates such that it faces the viewer. By using a texture with a circular image (for instance a flare) and making the quad transparent, the particle will appear volumetric (it appears circular from all directions), although it is just rendered with a simple polygon that is a flat plane. The reason this is the preferred method for rendering particles is that it is otherwise quite difficult to render volumetric, transparent objects with triangles (the only thing GPUs can render). Billboards are an effective illusion. Particles can represent other things, as well, for example small meshes, light sources or even sounds, but billboards are by far the most common. A large part of building a particle effect is about configuring how the different properties of all those particles evolve over time. For example, if you configure particles to have a lifespan of two seconds, rise up with medium speed, change their color from red to yellow and fade out shortly before they die, then you get an effect that looks a lot like fire. Particle Systems and Effects You never work or configure individual particles. Instead, you mostly work with particle systems. A particle system represents a large amount of particles that all behave according to the same rules. A complete particle effect often consists of multiple particle systems, but always at least one. Each particle system defines different rules how the particles of that system behave. So in the fire example, you would have one particle system which is configured to spawn five particles every tenth of a second. All these particles are rendered as billboards, use the same flare texture, rise up, and change their color according to some fire gradient over their lifetime. This system represents the flames. To add smoke above the flame, you would add a second particle system, which may only spawn one particle every tenth of a second, use a smoke-like texture, rise up more slowly and start with a zero size at the beginning, slowly growing larger and larger, such that it becomes visible just above the flame. The flame particles and the smoke particles have no relation, whatsoever, but together they form a better illusion of fire. Evolving Properties The code that updates a particle system mostly handles every property of the particles in isolation. And that is also how you need to think about each property, when you want to create an effect. The way a property, such as position, changes, is called the behavior. So for example, a particle may rise up slowly, or it may fall down according to gravity. Whether the position behaves one way or the other results in a drastically different effect. The same is true for all other properties. A particle may have a constant size or it may start small but grow over time. Its color may be just white, or some random blue-ish tone or it may change its color such that it appears to be burning up or fading out. Every property has its own rule, how it behaves. Put it all together and you can build an infinite amount of different effects. Building Blocks What the particle editor presents to you, are a number of building blocks that you choose and configure. For example, there are a few behaviors how the position of a particle should be calculated. There are a few building blocks for determining a particle's color, its size, and how to render it. Many behaviors are mutually exclusive. So if you already chose the \"gravity\" building block, which lets particles fall down, then you can't choose a second behavior that also affects particle positions. Most building blocks expose options for you to tweak. For example the \"gravity\" behavior allows you to tweak the strength of the applied gravity. To create an effect, you create multiple particle systems, and for each one you select and configure the desired behaviors. Spawning and Lifetime We already mentioned that particles have a limited (usually very short) lifespan. Generally you can separate particle effects into two types: short one shot type of effects, and long lasting or even unending continuous effects. Explosions, water splashes and bullet impacts are all of the former type. They typically spawn all of their particles in one big burst. Those particles live for a second or so and then the effect is over. Fire, smoke and mist are of the latter type. Those effects typically spawn particles continuously. Each particle lives for several seconds and by the time it dies, many other particles have already been spawned to take its place. Continuous effects can be configured such that they are not endless, for example a smoke effect may stop by itself after 10 seconds. However, often it is more convenient to build such effects as endless effects, that never stop, and then use custom (script) code to make a particle effect stop spawning new particles at the desired time. This way the desired duration of an effect can be dynamically adjusted. Whether an effect acts one way or the other, is determined by the selected type of emitter. The emitter building block specifies how often and how many new particles get spawned. Emitters can be configured to be smooth or erratic, doing short bursts or long intervals. Most of the time one uses one of two emitter types, but there are also emitters that only spawn particles when some event happens, which can be used for even more complex effects. How long a particle will live is decided randomly when it gets spawned (within a range). How much time a particle has left to live, is often used to look up other properties. For example the color of a particle often depends on its lifetime, such that particles will fade out towards their end. See Also Particle Effects"
  },
  "pages/docs/effects/particle-effects/particle-behaviors.html": {
    "href": "pages/docs/effects/particle-effects/particle-behaviors.html",
    "title": "Particle Behaviors | ezEngine",
    "keywords": "Particle Behaviors This page lists and describes all particle behaviors. Bounds Behavior This behavior can be used for atmospheric effects that should be centered around the player, such as rain, snow or mist. The bounds behavior specifies an area in which particles are allowed. When the player moves, and thus the particle effect is moved to a new location, particles would usually stay behind although not being needed anymore. The bounds behavior can make sure to delete those particles. For some effects it is also vital to fill up the new space quickly. This can be achieved with a very high rate of spawning new particles, though this is often not feasible for atmospheric effects. Instead, the bounds behavior can also just teleport the particles that were left behind, to the new area. PositionOffset, Extents: These values define the size and position of the box, relative to the origin of the particle system. With a position offset of (0, 0, 0), the box will be centered around the system's origin. OutOfBoundsMode: Defines what happens for particles that leave the bounding area. Die: Particles outside the area will be killed right away. Teleport: Particles leaving one side of the bounding box will be teleported to the other end of the box. This allows the effect to keep a constant density of particles and is therefore useful for effects that should happen around a player, without being simulated completely in the local space of the player, which would prevent things like using the raycast behavior. Instead, particles can simulate in global space, and only be teleported on demand. Be aware that this teleportation can still break the effect in various ways, because only the position and last position of each particle is relocated. Behaviors and particle renderers that use additional positional data may not work well with this. For example, the trail renderer's position history is not relocated and therefore trails will suddenly stretch through the entire bounding area after a relocation. Similarly, an effect that uses the raycast behavior to prevent tunneling through geometry, may be able to tunnel through walls, if it is being relocated from an unobstructed area to a position where it should not have been able to get to without the teleportation. Color Gradient Behavior This behavior changes a particle's color during the update step. A color gradient is used as the color source, and a mode specifies how to look up the color from the gradient. Gradient: The color gradient to use as the source. TintColor: An additional color to be multiplied into the gradient, for tweaking the final result. ColorFrom: This mode specifies how the color is looked up from the gradient: Age - In this mode the particle's color depends on its age and remaining lifetime. That means it starts out with the leftmost color from the gradient and transitions towards the rightmost color. Optimally, the color gradient should include alpha values, such that the particles can fade out towards the end. Speed - In this mode the particle's color is determined from its current speed. Slow particles are assigned colors from the left side of the gradient, fast particles that from the right side. This mode only makes sense when either every particle gets a random speed assigned, or when its speed is able to change over time, due to friction, gravity or other factors. MaxSpeed: When using ColorFrom = Speed, this value specifies the maximum expected speed of any particle. That speed is then mapped to the rightmost side of the color gradient. Fade Out Behavior This behavior changes a particle's alpha value to gradually fade out over its lifetime. This behavior can also be achieved using a color gradient behavior, however, the fade out behavior is easier to set up and more efficient at runtime. StartAlpha: The alpha value to begin with when the particle has just spawned. Exponent: How quickly to fade the alpha value from StartAlpha towards 0 over the particle's lifespan. An exponent of 1 results in a linear fade. An exponent of 2 will make it fade out much earlier, a value of 0.5 will make it fade out very slowly at first and then quite abruptly at the end. Flies Behavior This behavior moves particles around the emitter center in erratic patterns, similar to a swarm of flies circling something. FlySpeed: The speed with which the particles move. PathLength: The distance that the particles move into some direction before making another turn. The shorter this is, the more often the particles can change direction and thus the smoother the motion becomes. They will also clump up more and stay within the MaxEmitterDistance, if the particles can correct their course more often. With a long PathLength they may spread out more. MaxEmitterDistance: The maximum distance that the particles will fly away from the effect's center before turning back. If they travel further, they will always steer back towards the emitter. How quickly that is possible though, depends on PathLength and MaxSteeringAngle. MaxSteeringAngle: Every time a particle has traveled a distance of PathLength, it will make a random turn. This value specifies how large that turn may be. A small value results in very slow and wide turns, whereas a large value results in quick and erratic behavior. Gravity Behavior This behavior lets particles fall downwards. GravityFactor: Scales gravity before applying it to the particles' velocity. Pull Along Behavior Typically once a particle has been spawned, its position is unaffected by changes to the particle effect position. That means when an effect moves around quickly, it may leave a trail of particles behind it, but that trail will be very choppy, unless you have an extremely high particle spawn count and frequency. Thus making something like a rocket exhaust look convincing for a fast moving object can be difficult. The pull along behavior helps to solve this problem by keeping track of any position changes of the particle effect node and applying a fraction of those movements to all the particles' positions as well. This way, if the effect moves a meter, all particles may move 0.8 meters as well. One typically only applies a fraction, such that when the effect moves fast, the particles will be stretched long behind it and not move in perfect unison with the effect node, yielding a more convincing effect. Strength: How much of the effect node's movement should be carried over to the particle positions. The video below shows two effects beside each other. The left one does not use the pull along behavior, the right one does. As can be seen, the particles on the right stay closer to the moving emitter position. Raycast Behavior This behavior uses raycasts to detect collisions along the trajectory of a particle. If a particle would collide with geometry, the behavior can either adjust the its velocity, or terminate the particle early, potentially raising an event, which could in turn lead to other effects or being spawned. Reaction: Specifies how the particle should react to a collision. Bounce: The particle's velocity will be adjusted such that it bounces off the hit surface. Die: The particle will be killed early. Stop: The particle's current velocity will be set to zero, thus stopping it in its tracks. If other position affecting behaviors are active, for example the gravity behavior, it will start moving again, but without its previous momentum. BounceFactor: How much of the current speed should be preserved after the bounce. CollisionLayer: The physics collision layer to use. Affects with which geometry the particle will collide and which it will pass through. OnCollideEvent: An optional name of an event to raise. If set, other effects or prefabs can be spawned at the location of impact. Size Curve Behavior This behavior changes a particle's size over the course of its lifetime. SizeCurve: A curve which is used to look up the size of the particle. The current fraction of the particle's lifespan is used for the lookup along the X axis. The absolute X and Y values in the curve don't matter, the curve is normalized to [0; 1] range. BaseSize: The particles will always have at least this size, the rest is added on top. CurveScale: Specifies what value the largest value in the curve maps to. That means at the peak of a curve, the particle's size will be BaseSize + CurveScale. Velocity Behavior This behavior affects particle position and velocity. It can be used to gradually dampen the starting velocity through 'friction' and it may apply a constant upwards movement. If a scene contains wind, this behavior can also apply a fraction of the wind force to the particle's position. RiseSpeed: If non-zero, the particles will move upwards with at least this constant speed. This is added to the particle position independent from its velocity, so if the current velocity points downward, the two may cancel each other out. Friction: This value imitates air friction. If it is non-zero, the particle's velocity will be dampened over time. The value's range is [0; infinity]. To achieve an effect as in the animation below, the particles must have a very large starting velocity (here: 10). The friction here is set to 6. This way the particles will appear to be quite fast, but will also get slowed down almost to a standstill within a fraction of a second. WindInfluence: If the scene has wind, this value specifies how much the wind should be able to push the particles around. See Also Particle Effects Particle Initializers Particle Renderers"
  },
  "pages/docs/effects/particle-effects/particle-effect-component.html": {
    "href": "pages/docs/effects/particle-effects/particle-effect-component.html",
    "title": "Particle Effect Component | ezEngine",
    "keywords": "Particle Effect Component The particle effect component is used to instantiate and control particle effects in a scene. Each component handles one effect. When the owner game object is moved, the particle effect will move accordingly. Particles are emitted along the up axis (positive Z) of the game object. Effect: The particle effect to spawn. SpawnAtStart: If true, the effect will be spawned once the component becomes active. Otherwise, nothing will happen, and the component must be triggered manually via custom code. OnFinishedAction: Specifies what happens when a non-continuous effect finishes: None - The effect stays off, and the particle effect component and its owner object stay as they are. Delete Component - The particle effect component gets automatically deleted to clean up unused components. Delete Object - The game object that the component is attached to is deleted including all attached components and child objects. This can be very useful to clean up entire effect objects, once the effect is finished. Note: This mode can be combined with other components that also have an OnFinishedAction. If multiple such components are set to delete themselves or the owning object, the last one that finishes will delete the object hierarchy. All components that finish earlier will only delete themselves (as if Delete Component was selected on them). This way you can attach for example a particle effect, a decal and a sound source to the same game object, select an OnFinishedAction on all of them, and get the correct behavior, no matter which one finishes first. Restart - The effect will be restarted after an optional restart delay. MinRestartDelay, RestartDelayRange: If OnFinishedAction is set to Restart, a random time between MinRestartDelay and MinRestartDelay + RestartDelayRange has to pass before the effect will be restarted. RandomSeed: If set to zero, the effect will use random values and look slightly different every time. If set to any other value, the effect will look identical every time it is restarted. SpawnDirection: The direction along which the effect should be spawned (in local space). The default is 'positive Z' which means 'up', but to align this with other things, such as decals or lights, it can be useful to use a different axis. Note that interactions with surfaces (e.g. an impact effect that is spawned when a bullet hits a wall) are always spawned such that the spawned prefab's positive X axis aligns with the surface interaction axis (e.g. it's normal). For such cases it therefore makes sense to spawn a particle effect along 'positive X'. IgnoreOwnerRotation: By default the SpawnDirection is local to the owning game object, meaning when the owning object is tipped over, the effect will also spawn sideways. For some effects it can be desireable to ignore the rotation of the owner, and always spawn in global space, though. For instance, when an effect has a strong directionality, such as debris flying away in a cone, it may look best when it is always spawned upwards. SharedInstanceName: If non-empty, this instance will use a shared effect. Parameters: If the chosen effect exposed effect parameters, they will be listed here and can be modified. See Also Particle Effects"
  },
  "pages/docs/effects/particle-effects/particle-effects-overview.html": {
    "href": "pages/docs/effects/particle-effects/particle-effects-overview.html",
    "title": "Particle Effects | ezEngine",
    "keywords": "Particle Effects Particle effects are used to create the visual part of things like explosions, smoke, fire, water splashes and much more. They are randomized to have slight variations every time. To create a full effect, like an explosion, with sound and physical properties such as pushing nearby objects away or damaging objects and creatures, a particle effect is typically put into a prefab, which contains additional components for sound and game play logic (e.g. through scripts). To create a new particle effect, use File > Create... and select Particle Effect as the file type. Or alternatively, right-click on any asset in the asset browser and select New > Particle Effect. Particle Editor UI This is an overview screenshot of the particle editor: The 3D viewport plays the effect in a loop. Using the toolbar buttons you can pause, reset, slow down or speed up the playback. On the right hand side there are multiple tabs which hold the various settings of the effect. If you are not too familiar with particle effects yet, please read how particle effects work. On the right hand side you see multiple tabbed panels: Systems The Systems panel is very central. Here you add new particle systems to the effect. However, this is also where you select which particle system to edit. The combo box specifies which particle system is currently active. All panels below (Emitter, Initializers, Behaviors and Renderers) show only the settings of the active particle system. When you add a new particle system with the green '+' button, you get a new system that uses a default configuration. Effect The Effect panel lists options for the overall effect, independent of the individual particle systems. Adjusting these options is typically only necessary once an effect is working well and you need to tweak its performance or allow users to adjust details through exposed parameters. Emitter, Initializers, Behaviors and Renderers These panels show the various options for the active particle system. When you select a different particle system from the combo box in the Systems panel, these panels will show different options. Event Reactions See events below. Particle System Configuration Every particle system has exactly one emitter, usually multiple initializers and behaviors, and typically one renderer. Most parameters are configured on those parts. Additionally, every particle system has these properties: Visible: This is an option for testing. If Visible is deactivated, the particle system is not simulated or rendered. Use this when you need to focus on editing other systems, or when a particle system is not yet good enough to be used. Invisible particle systems don't cost performance. Life: This is a value with variance. It specifies how long each particle will be simulated and rendered before it is removed from the system. A low variance means all particles live equally long, a high variance means some will have a short lifespan, others a much longer one. Note: A life span of zero seconds guarantees that a particle lives for exactly one frame, independent of frame rate. This is useful for effects where something visually striking (e.g. a flash) should pop up for the shortest possible time. LifeScaleParam: An optional effect parameter which can be used to scale the particle lifespan. OnDeathEvent: An optional name for the event when a particle dies. This can be used to spawn other effects. Emitter The emitter is what defines how new particles in this system get spawned. It mostly specifies when and how many particles are spawned. Typically particles are either spawned in one big burst or continuously. However, for advanced use cases the emitter may only spawn particles as a reaction to some event or when the particle effect node was moved a certain distance. For details about all available emitter types, see Particle Emitters. Initializers Every particle has a number of properties, such as position, velocity, color, size and rotation speed. For a newly spawned particle, these values must get a starting value. Initializers allow you to affect the starting value. For example, a particle's position is by default (0, 0, 0), but using a Sphere Position Initializer, the starting position will be set to a random position inside a sphere, or even just on its surface. Initializers are executed exactly once for each new particle, thus they cost little performance. However, they only have an effect, if the starting value isn't subsequently overwritten by Behaviors. For instance if the Random Color Initializer is used, it will set the color of new particles. If, however, the Color Gradient Behavior is also used, the behavior will set the particle's color to a new value in every update, thus making the initializer pointless. Prefer to use initializers over behaviors, if the desired result can be achieved with either. For details about all available initializer types, see Particle Initializers. Behaviors Behaviors are the core particle effect update routines. Every time a particle moves, changes color, grows, shrinks or rotates, this is implemented by a behavior. Behaviors are executed for every particle, in every update step. For performance reasons, you should strive to use as few behaviors as possible. Every behavior reads some particle properties and writes one or two properties. For example the Size Curve Behavior reads a particles age and maximum lifespan and then looks up its new size from the provided size curve. Therefore it overwrites the particle's size property in every update. Initializers set a particle's property once when it is spawned, behaviors set (or update) a property continuously. Consequently a behavior may overwrite an initial value, making it redundant. Or it may build on top of the starting value. For example the Velocity Behavior can be used to have particles fly upwards (rise). Although the behavior modifies the particle's position property, it still works well together with the various position initializers, as it only adds to the position instead of replacing it. It is common for a particle system to have at least one, but often multiple, behaviors. For details about all available behavior types, see Particle Behaviors. Renderers Conceptually a particle is just a point in space. There are many ways this point can be visualized. Renderers are used to select how to do that. Most particle systems use one renderer, often the Billboard Renderer, which is the most versatile. However, you are free to use multiple renderers. For example to achieve a fire effect with heat haze, you may want to use two renderers. One to render the particles as billboards using a fire texture, and another one to apply the screen space distortion effect. For details about all available renderer types, see Particle Renderers. Effect Parameters Effect parameters are an advanced feature that allows you to make certain parts of an effect configurable from the outside. You add effect parameters in the Effect tab. At the moment only number parameters and color parameters are supported. All parameters that you add there will appear as exposed parameters on particle effect components. Effect parameters can only affect select features. For example the quad renderer has a TintColorParam property. If you type in the name of a color parameter there, the quad renderer will look up the value of the color parameter during every update, and use that to modulate the final color of the rendered particle. You can use this in a static way, as a means to add more variety to multiple instances of the same effect. Or you can use this in a dynamic way, by modifying the exposed parameter through (script) code, for example to visualize how hot something burns. Events and Event Reactions Particles may raise events. The most common one is when a particle dies, but different particle behaviors can raise other events as well. For example when a particle collides with the environment (see raycast behavior). The Event Reactions tab allows you to configure what happens for a specific event. This is mostly used to chain effects. For example the fireworks effect below has particles that represent the rockets flying up, and when one 'dies', an explosion is spawned at that position, using event reactions. Instead of spawning other particle effects, you can also spawn entire prefabs, which enables even more complex effects. Every event reaction has a Probability value which should be between 1 and 100. For example, if one reaction has a probability of 50, then for half of all events, that event reaction will be spawned. If there is no other event reaction for the same event type, then nothing is spawned. If however, another event reaction exists, say with a probability value of 30, then it also gets a chance to be spawned. Since 20 probability points are still not assigned, there is an overall 20 percent chance that no reaction is spawned. Be aware that probabilities are not normalized across event reactions. If you have two reactions for the same event type, both with a probability of 100, then in practice the first event reaction will always be spawned, and the second one will never be spawned. Apart from event reactions, it is also possible to react to events using the OnEvent emitter, however, that is less common. Misc Variance Values The particle editor presents many values as values with variance. They appear as a single value with a slider next to it: The input box represents the base value and the slider represents the variance. The variance is between 0 and 1. Generally, this type represents a random value, centered around the base value using a normal distribution. The variance affects the range from which random values are drawn. The range is always between BaseValue - Variance*BaseValue and BaseValue + Variance*BaseValue. Consequently: If Variance is 0, the range and thus the result for every 'random' value will be exactly BaseValue. If the variance is 1, the random value will be anywhere between 0 and 2 * BaseValue. For a variance of 0.5, the random value will be between BaseValue - 0.5*BaseValue and BaseValue + 0.5*BaseValue However, due to the normal distribution of the random numbers, values close to BaseValue will appear much more often than values far away from it. Such distributions are common in nature and therefore the result looks more natural. For most such values you should use at least some variance (0.2 to 0.4) to make your effects look less repetitive and sterile. However, extremely large variance values (0.7 and up) can result in unexpected outliers. Local Space Simulation Some effects should always behave the same, no matter how the owning game object is rotated, or how fast it moves. This is often the case for rocket exhaust effects, for example. Such behavior is hard to achieve with the way particle effects typically work, though, as each particle would need to have an extremely short life span and move very fast. When enabling SimulateInLocalSpace in the Effect tab, the effect is simulated as if it was positioned at the world origin and with the default orientation. This removes any influence that the effects position, orientation and movement would otherwise have on the effect, as shown below: Some behaviors won't properly work for effects that are simulated in local space. For example the raycast behavior will do it's raycasts at the origin of the scene, rendering it pointless. There is no performance benefit to using local space simulation. However, when using shared effects, the shared state must be simulated in local space. Other options to keep particles closer to the owning object are to use the pull along behavior or to inherit the owner velocity. Owner Velocity Inheritance In the Effect tab there is a property ApplyOwnerVelocity which is a value between 0 and 1. By default the value is zero, which means that all particles are initialized with either a zero velocity or with whatever some initializer decided. In that case, particles will fly away from the emitter position unaffected by the velocity of the effect object itself. However, if the value is set to non-zero, a part of the velocity of the owning game object will be added to newly spawned particles. This can be used for effects that may be spawned from moving objects and that shall retain some of that momentum. However, unless you additionally configure the effect to have some velocity damping (ie. using the friction property of the velocity behavior), the particles will fly into that direction continuously, which may look weird, especially when the owner object changes direction or brakes, and the spawned particles overtake it. Other options to keep particles closer to the owning object are to use the pull along behavior or to fully simulate in local space. Shared Effects Typically every particle effect in the world is simulated every frame. However, especially 'ambient' effects, such as the fire of wall torches, is often instantiated many, many times. You may only see a few of them at a time, but if all those effects were simulated every single frame, that would cost significant performance. A solution to this problem is to use shared effects. When an effect is set to be shared, it is simulated only once. All instances, that reference the effect will only be used to render the effect at their position. If not a single instance is visible, the simulation of the shared effect will even be paused. There are two ways to make an effect shared. The global option is the AlwaysShared property which can be found in the Effect tab. If this is enabled, then all instances of the effect will always be shared. This should be used for ambient effects which are expected to be instantiated often and where simulating more than one has no benefit. The second option is to set a SharedInstanceName on the particle effect component. All effect components that use the exact same shared instance name, will share one simulation state and thus look identical. With this method, you can use a finite number of simulated effects, which allows you to have the same effect multiple times next to each other, without having them look identical, but still limiting how many effects need to be simulated. Because the various instances have different positions and orientations, shared effects are always simulated in local space. Effect sharing should mainly be used for continuous effects, and the effect should be authored to not be very distinctive. Also note that the rendering cost still has to be paid for every visible instance. Pre-Simulation Some effects are supposed to always look as if they are constantly running. Mostly this is needed for ambient effects, such as torch fire or chimney smoke. The game may only spawn these effects when the player enters an area, but it is not desirable to see them getting started before they reach a stable simulation state. Instead, they should always already be in the state that they reach after a couple of seconds of simulation. For such cases, you can use the PreSimulationDuration option from the Effects tab. When this is set to a couple of seconds, the first time an effect is simulated, it will be simulated multiple times, to reach the desired state. Note: Pre-simulation obviously has a performance cost during the first simulation step. Therefore, keep the pre-simulation duration as low as possible. Also be aware that for many ambient effects, that are instantiated a lot throughout a scene, prefer to use shared effects. Pre-simulation may still be necessary to fix their very first appearance, though. Update Rate When Invisible When a particle effect is not visible, it may still need to be updated, as the way that it changes may make it visible in the first place. For example the smoke of a smoke grenade that is behind the player may become visible when it is blown into the players view by the wind. It may be sufficient, though, to only update the effect ten times, or even just 5 times per second, while invisible. Thus reducing the computational overhead. However, there are also effects which do not need to be updated, at all, when invisible. A waterfall effect, for instance, will always look similar. Thus once it is out of view, it can be simply paused. And there are even effects that can be discarded entirely, when out of view. Bullet impact effects, for example, may be so small and have such a short life span, that there is no value in updating them at all, unless they are visible to begin with. Which update method to use can be chosen from the Effect tab using the WhenInvisible property. See Also How Particle Effects Work Particle Effect Component"
  },
  "pages/docs/effects/particle-effects/particle-emitters.html": {
    "href": "pages/docs/effects/particle-effects/particle-emitters.html",
    "title": "Particle Emitters | ezEngine",
    "keywords": "Particle Emitters This page lists and describes all particle emitters. Burst Emitter This emitter type spawns particles either in one instantaneous burst or over a limited amount of time. It is mainly used for one-off effects like explosions, impacts, etc, which have a short lifespan. Once the burst emitter is finished, the particle effect will only continue to live until all spawned particles have reached the end of their life. For such effects the particle effect components are typically set to auto-delete themselves after the effect is finished. Duration: The timespan over which the emitter will distribute the spawning of the particles. If this is set to zero, all particles spawn at the same instant. StartDelay: An optional delay from when the particle effect is created, until the emitter starts spawning particles. Useful in effects with multiple particle systems, to tweak when one type of particles becomes visible, relative to other types of particles. MinSpawnCount, SpawnCountRange: A random number of particles between MinSpawnCount and MinSpawnCount + SpawnCountRange is emitted over the emitter's duration. SpawnCountScaleParam: An optional name of an effect parameter that can be used to scale the number of emitted particles up or down. Note: At the moment this mostly allows to reduce the number of emitted particles. Increasing the amount of particles may have no visible effect. Continuous Emitter This emitter type continuously spawns new particles. Effects which have at least one such emitter type will never stop, unless custom code specifically switches the effect off, or the owning particle effect component is deleted. In both cases all spawned particles will continue to be simulated and rendered, until they reach the end of their life. This emitter type is commonly used for ambient effects such as smoke and fire. By exposing effect parameters, continuous particle effects can be adjusted dynamically to visualize game mechanics, such as how hot something burns or how active some machine is. StartDelay: See the burst emitter. SpawnCountPerSec, SpawnCountPerSecRange: A random number of particles between SpawnCountPerSec and SpawnCountPerSec + SpawnCountPerSecRange is emitted every second. SpawnCountScaleParam: See the burst emitter. CountCurve, CurveDuration: If no CountCurve is specified, particles are spawned in regular intervals. Only a large value for SpawnCountPerSecRange may introduce irregularities. Using a count curve, the distribution of how many particles are spawned at what time can be controlled. If a curve is given, CurveDuration specifies its timespan. For instance, a curve duration of two seconds means, that the count curve is sampled from left to right over a duration of two seconds, before it repeats again. The value of the curve at a given time determines how many particles will get spawned. The curve is only used as a scale factor between zero and one, though (its absolute values don't matter, it is normalized internally). Every time the emitter attempts to spawn particles, SpawnCountPerSec and SpawnCountPerSecRange determine the maximum amount of particles to spawn. Then the curve is sampled and the current value is used to scale the number of particles down. Thus count curves can be used to introduce more elaborate spawn patterns. Distance Emitter This emitter type only spawns new particles when the particle effect is moved for a distance of at least DistanceThreshold units. This can be used when an effect should have a relatively uniform particle density when in motion, without constantly spawning large amounts of particles. When the effect stands still, this emitter will not spawn any particles, so you may want to combine this with another continuous emitter. DistanceThreshold: The distance that the effect has to be moved for the emitter to spawn another set of particles. MinSpawnCount, SpawnCountRange: See the burst emitter. SpawnCountScaleParam: See the burst emitter. OnEvent Emitter This emitter type spawns new particles whenever a specific event happens. It does not create the new particles at the position of the event. If that is desired, use an event reaction instead. EventName: The name of the event which shall trigger spawning particles. MinSpawnCount, SpawnCountRange: See the burst emitter. SpawnCountScaleParam: See the burst emitter. In the animation below, the blue particles use a raycast behavior to get removed when a collision is detected. The behavior also sends an event. This is picked up by a second particle system, which then spawns a number of red particles. See Also Particle Effects"
  },
  "pages/docs/effects/particle-effects/particle-initializers.html": {
    "href": "pages/docs/effects/particle-effects/particle-initializers.html",
    "title": "Particle Initializers | ezEngine",
    "keywords": "Particle Initializers This page lists and describes all particle initializers. Box Position Initializer Initializes a particle's position to a random point within a box shape. PositionOffset, Size: These values define the size and position of the box, relative to the origin of the particle system. With a position offset of (0, 0, 0), the box will be centered around the system's origin. ScaleXParam, ScaleYParam, ScaleZParam: Optional names of effect parameters. This allows to scale the volume in which particles spawn. Note: Scaling the volume will change particle density. To compensate, the particle system will automatically spawn more or fewer particles. Thus you can author an effect as a 1x1x0 meter sized patch and then let the user decide how large a patch she needs by exposing these parameters. If your 1x1x0 patch requires roughly 100 particles at all times, then scaling it to a 10x5x0 patch will require 5000 particles. Cylinder Position Initializer Initializes a particle's position to a random point either within a cylinder or on its surface. A cylinder of height 0 initializes the position to a random point on a circle or its circumference. PositionOffset, Radius, Height: These values define the size and position of the cylinder, relative to the origin of the particle system. With a position offset of (0, 0, 0), the cylinder will be centered around the system's origin. A height of 0 turns the cylinder into a circle. OnSurface: If enabled, particles will only spawn on the surface of the cylinder, not inside it. This also excludes the caps. For a cylinder of height 0 that means the particles will spawn on the circumference of a circle. SetVelocity, Speed: If enabled, the initializer will additionally set the particle's starting velocity. The velocity is always outward from the cylinder's center line. ScaleRadiusParam, ScaleHeightParam: Optional names of effect parameters. This allows to scale the volume in which particles spawn. Note: Scaling the volume will change particle density. See the box position initializer for details. Sphere Position Initializer Initializes a particle's position to a random point within a sphere shape. PositionOffset, Radius: These values define the size and position of the sphere, relative to the origin of the particle system. With a position offset of (0, 0, 0), the sphere will be centered around the system's origin. OnSurface: If enabled, particles will only spawn on the surface of the sphere, not inside it. SetVelocity, Speed: If enabled, the initializer will additionally set the particle's starting velocity. The velocity is always outward from the sphere's center. ScaleRadiusParam: Optional name of an effect parameter. This allows to scale the volume in which particles spawn. Note: Scaling the volume will change particle density. See the box position initializer for details. Random Color Initializer Initializes a particle's color to a random color. Gradient: If specified, the random color will be picked from the given color gradient. Color1, Color2: A random interpolated color between the two given colors is used. So if one color is white and the other is black, particles will get a random grey value as their color. If a gradient is set as well, the two colors are combined. Random Size Initializer Initializes a particle's size to a random value. Size: The base size for the particles to start with. To initialize all particles to have a fixed size, set the variance to zero. SizeCurve: If specified, the curve is sampled at a random location and the normalized value (always between 0 and 1) is used to scale the randomly chosen base size. The shape of the curve has no meaning for this use case, it only provides a way to affect the distribution of the random sizes. For example, you could have a curve that sets exactly half of all particles to exactly a tenth of the base size. If you want exactly the same distribution as the curve has, you should set the variance of the base size to zero. Rotation Speed Initializer Initializes a particle's rotation and rotation speed to a random value. RandomStartAngle: If enabled, the particle will start out with a random rotation. For particles with a distinct texture or shape, this can make the effect look significantly more natural. DegreesPerSecond: If set to a non-zero value, particles will rotate with a constant speed. Each particle gets its own random speed assigned. With a low variance all particles will rotate similarly fast, with a high variance you will see some particles rotate very fast and some very slowly. Half of the particles rotate clockwise, the other half counter-clockwise. Velocity Cone Initializer Initializes a particle's velocity to a random up vector. Angle: The maximum opening angle of the upside down cone. With a small opening angle, particles will fly straight up. With a wide opening angle, particles will fly in all directions. Speed: The initial speed for the particles. See Also Particle Effects Particle Behaviors Particle Renderers"
  },
  "pages/docs/effects/particle-effects/particle-renderers.html": {
    "href": "pages/docs/effects/particle-effects/particle-renderers.html",
    "title": "Particle Renderers | ezEngine",
    "keywords": "Particle Renderers Quad Renderer This renderer visualizes each particle as a quad. There are several modes to choose from how this quad is oriented. There are also different modes how to blend the particle with the background. This renderer is very versatile. Orientation: This mode defines how the quad is oriented and around which axis it may rotate. In the 'rotating' modes the quad geometry constantly rotates around some axis that is decided when the particle is spawned. In the 'fixed' modes the quads themselves will not rotate, but have a fixed plane decided when they spawn. If they have a non-zero rotation speed, their texture will rotate around the quads center. In the 'axis' modes, the quads have one fixed axis and one that orients itself into the direction of the camera. Billboard - This is the most common mode. Billboards always face the camera. If the camera moves around the effect, the billboards keep orienting towards it. When billboards should rotate, they always rotate in screen-space, meaning around the current forward axis of the camera. Rotating: Ortho Emitter Dir - In this mode the quads rotate around the orthogonal axis between the direction of the emitter and the direction the particle moves into. This mode is particularly useful for simulating debris of concrete for bullet impacts and such. When the particles have a high rotation speed, they will fly off in a spectacular fashion. For this type of effect it is also best to use alpha-masked textures representing debris, and to use the 'Opaque' render mode. Rotating: Emitter Dir - Similar to the mode above, but uses the direction of the emitter as its rotation axis. This can be used for muzzle flashes (the part that shows along the direction of the barrel) or impact effects. Fixed: Emitter Dir - In this mode the quad always uses the emitter direction as its plane normal (with some optional Deviation). This mode is useful for creating shockwave effects at a point of impact, or things like ripples in water. It can also be used for muzzle flashes (the part sideways out of a barrel). Fixed: World Up - Similar to Fixed: Emitter Dir but the axis used is always the world up direction. This can be useful for effects where the emitter may have an arbitrary direction, but the particles should always face upwards. For example for the ripples of water impact effects. Fixed: Random Dir - In this mode each quad gets a random axis assigned when it is spawned. This can be useful for creating shockwave effects in explosions. Axis: Emitter Dir - In this mode the quads fixed axis is the direction of the emitter. The quad will rotate around this axis to face the camera as much as possible. Additionally, the quad will not scale around its center, but around one of its edges. Therefore, when scaling up or down, that edge will stay in a fixed position. This can be used to create muzzle flashes and other impact effects which should generally move into the direction of the emitter, but also face the camera as much as possible, to be well visible. This mode won't look too convincing if the particles' movement deviates strongly from the emitter direction. For most common use cases, the particles may not move at all, but only change their size. Axis: Particle Dir - In this mode the quads fixed axis is its own fly direction. The quad will rotate around this axis to face the camera as much as possible. This is useful for creating sparks or laser blasts which shall stretch a little while moving into some direction. Render Mode: This mode specifies how the color from the particle will be combined with the scene background. Opaque - The particle will use the alpha channel of the texture as mask. Pixels are either fully transparent or fully opaque. This is mostly useful for debris. Additive - The particle's color will be added to the background. The alpha-channel is not used. This is used for everything that should glow (magic spells, sparks, ...) Blended - The alpha channel of the texture is used to interpolate the particle's color with the background. This is used for everything that should be transparent, but not glowing (smoke and such). It is also the most tricky mode to make look good, as it depends the most on a good texture with a proper alpha channel. Particles rendered with this mode must be sorted by distance by the renderer, which incurs an additional performance cost. Blended Foreground/Background - Same as Blended but when there are multiple particle systems using Blended mode, this allows you to influence in which order the systems are rendered. This is used to fix rendering issues. Distortion - This is used to create a heat haze effect, which distorts the scene behind it. The alpha-channel of Texture is used to determine the shape of the distortion effect. The Distortion Texture and Distortion Strength are used to decide for each pixels how much to distort the background. Any texture can be used as the distortion texture, but the effect works best using a normal map. Texture: The texture used for rendering. May be a texture atlas or contain flipbook animations. Texture Atlas: Specifies how to interpret the content in Texture: None - The texture contains only a single image. Random Variations - The texture contains NumSpritesX x NumSpritesY images in a regular grid. Each image is independent and each particle uses a random one. Flipbook Animation - The texture contains an animation starting at the top left, going to the right and down. Every particle starts with the first image, and over its lifetime will traverse through the frames to play the animation. Random Row, Animated Column - The texture contains NumSpritesY animations, each with NumSpritesX frames. Each particle plays one random animation over its lifetime. TintColorParam: An optional effect parameter name. If set, the parameter is used to tint the final color of the particle. Particle Stretch: Only available in the 'axis' render modes. Allows to stretch the particles along their fixed axis. Useful to create sparks. Mesh Renderer This renderer visualizes each particle using a mesh. Mesh: The mesh to use for rendering. Material: The material to use on the mesh. TintColorParam: An optional effect parameter name. If set, the parameter is used to tint the color of the meshes. Light Renderer The light renderer treats each particle as a light source and thus illuminates the scene around it. Usually one would add this renderer as a second renderer to a particle system, such that one can easily reuse the behavior and color of for example billboards. Since light sources have a very high performance overhead, and adding as many lights into a scene as billboards would often be way too much, this renderer may visualize only a fraction of all particles in the system. SizeFactor: A factor to scale the particle's size with to determine the light influence radius. If the particles are also visualized as, for instance, billboards, the light source around the billboard should often be three to five times bigger. Intensity: The light intensity. Percentage: How many of the particles in the system should also be used as light sources. Typically you should only use 10% or so of the particles. Fewer lights is not only better for performance, it often also looks better, as there will be more contrast and flickering. Too many lights will result in constant brightness, making the effect less interesting. TintColorParam: An optional effect parameter name. If set, the parameter is used to tint the color of the lights. IntensityScaleParam: An optional effect parameter name. If set, the parameter is used to scale the intensity of the lights. SizeScaleParam: An optional effect parameter name. If set, the parameter is used to scale the light influence radius. Trail Renderer This renderer visualizes particles as long lines that draw the path that the particle took. Trail particles are made up of a fixed number of segments. The more segments the renderer uses, the longer the trails will be. Also the faster a particle moves, the longer the trail will stretch. More segments cost more performance to update and render. For very short sparks that should just stretch a little, it is better to use billboards with Orientation set the Axis: Particle Dir and StretchFactor set to some value between 2 and 5. Render Mode, Texture, Texture Atlas, TintColorParam: These options are identical to the quad renderer. Segments: The number of segments to use for each particle. More segments cost more performance but also result in longer and more detailed trails. Effect Renderer This renderer visualizes each particle using another particle effect. The referenced effect is spawned at the position of each particle and then moved along as the particle moves. This allows you to create effects like fireworks, where each 'rocket' is represented by a single particle flying into the sky, but visualizes as a complete burning particle effect. Effect: The particle effect to spawn and move along for each particle in this system. Usually the other particle system would use at least one continuous emitter, such that the effect will be active for the whole lifetime of the particle that references it. RandomSeeed: An optional random seed to pass into the spawned effect. See Also Particle Effects Particle Initializers Particle Behaviors"
  },
  "pages/docs/effects/post-processing/post-processing-component.html": {
    "href": "pages/docs/effects/post-processing/post-processing-component.html",
    "title": "Post-Processing Component | ezEngine",
    "keywords": "Post-Processing Component The post processing component is used to dynamically modify settings of the render pipeline (TODO). The component relies on volume components to define where in a level which values shall be used. The following image shows a scene without custom post processing values: With post processing, the area can be made to look very different: The level uses a volume component to specify that this area should have a different athmosphere and thus use different values for post processing. To enable custom post processing, attach the post processing component to the same object where the main camera component is attached to. This would typically be inside a player prefab. If, however, the post processing component is attached to an object with a camera component that is configured for render to texture (TODO), it will only affect that. It is also possible to place this component simply anywhere in a level. In this case it is always active and affects the currently active camera. This can be very useful during testing, since it also affects the editor camera and thus you can test values and volume placement just by moving the editor camera around, without even simulating the scene. Component Properties Volume Type: A spatial category that is used as a filter to determine which volume components to use for looking up values. Mappings: An array of mappings from that configure which values in a render pipeline (TODO) to modify. See the images below for an example. Render Pass: The name of the render pass in the pipeline to change. Property: The name of the property to change. Volume Value: The name of the value to use from the volume. Default Value: The value to use for Property if the camera is currently in no volume. Interpolation Duration: The property is interpolated towards the target value over this time. A duration of 0 means the value changes immediately, anything larger means the change happens more smoothly. Example In this example, the post processing component is used to alter the Mood Color: The image below shows the Tonemapping render pass from the game's render pipeline (TODO). On the right are it's properties that get modified. Be aware that once post processing component is used, the values for these properties on the render pipeline have no effect anymore, since they are always being overwritten anyway. Constant Overrides If you leave the Volume Value property for a mapping empty, the component overrides the render pipeline with the default value, but never reads a value from a volume. This can be used to just set a value to a different value in a level. This way you can also use this component just to have different values per level. In this case the component should not exist on the player object, but just be added to each level. See Also Volume Components Lighting Fog"
  },
  "pages/docs/effects/post-processing/volume-components.html": {
    "href": "pages/docs/effects/post-processing/volume-components.html",
    "title": "Volume Components | ezEngine",
    "keywords": "Volume Components Volume components are used to define custom environmental conditions in areas of a level. By itself, volume components have no functionality and no noticeable effect. They only specify an area and what values to use there. Values are usually specified by referencing a blackboard template asset. Other systems may use this information to implement behavior. One such system is the post-processing component which uses these volumes to modify parameters of the rendering pipeline, for example to have different color grading per area. A custom system could for example also use these volumes to determine whether the player is inside water. There are multiple volume components for different shapes: ezVolumeBoxComponent ezVolumeSphereComponent They only add options to define their shape, such as extents or radius, but do not differ in functionality. Component Properties All volume components share these properties: Type: A spatial category used for separating volumes that represent different things. This way one volume may be used to configure graphics settings, while other volumes may affect gameplay relevant functionality, and they don't accidentally interfere with each other, since the respective systems only get to see the volumes that are meant to affect them. SortOrder: In case two volumes overlap, the one with a higher sort order value has precedence. Template: A reference to a blackboard template asset to define the key/value pairs. It is usually more convenient to use a blackboard template as a preset for values, than to specify them directly on the volume component. Values: Individually added key/value pairs. Prefer to use a Template, but If the same key is also added here, it overrides the value from the template. Falloff: Volumes may have a soft edge, meaning that the boundary of the volume is not considered to be aprubt. This is used to smoothly fade values from one value to the one inside the volume. For example if a volume represents a foggy area, where the whole color grading is supposed to change, the colors are not supposed to change exactly the moment that the camera enters the volume, but rather the colors should become stronger the farther the camera is inside the volume. The falloff value is a value between 0 and 1 that configures how smooth the edge of the volume is. At 0 the edge is hard and change happens immediately. This is useful for example for water, where you are either inside or outside, but not in between. Any value above 0 is meant for smoother transitions. Be aware that additional to this, some systems may also transition to new values over time. To test the falloff value, it is best to deactivate any time delay. See Also Post-Processing Component"
  },
  "pages/docs/effects/ropes/fake-rope-component.html": {
    "href": "pages/docs/effects/ropes/fake-rope-component.html",
    "title": "Fake Rope Component | ezEngine",
    "keywords": "Fake Rope Component The fake rope component is used to simulate simple cables, ropes and wires for decorative purposes. These ropes are not able to pull on another object and thus can't be used as a gameplay element. Use the rope component for such use cases. On the other hand, the fake rope component is more lightweight to simulate and is optimized to have very little overhead when it has reached a resting state (doesn't swing anymore). Therefore it can be used in larger quantities for decorative purposes. Setting Up a Rope A rope requires two anchor points between which it hangs. One anchor point is the rope object position itself, for the other one typically uses a dummy game object. The Anchor object reference is used to select which one to use. In the object hierarchy it typically looks like this: The position of the anchors can be moved in the 3D viewport to position the rope as desired. The shape of the simulated rope will be shown as a preview. Use the Slack property to make the rope sag. Run the scene to see the final shape and behavior. Rendering With just the rope simulation component, you won't be able to see the rope, at all. You also need to attach a rope render component to the same game object. Examples The Testing Chambers project contains a dedicated Ropes scene with many examples. Component Properties Anchor: A reference to an object whose position determines where the rope ends. AttachToOrigin, AttachToAnchor: Whether the rope is fixed at the origin or anchor location. If the rope is not attached at one or both ends it is free to move away from there. Pieces: How many individual pieces the rope is made up of. More pieces look prettier, but cost more performance and may decrease the simulation stability. Slack: How much slack the rope has. A value of zero means the rope is hung perfectly straight between its anchors. Positive values make the rope sag downwards. Damping: How quickly the rope loses energy while swinging. Higher values make the rope come to rest more quickly, low values make the rope swing for a long time. Once a rope comes to rest, it takes significantly less processing power. WindInfluence: How strongly wind should make the rope swing. Be aware that having many swinging ropes costs a lot of performance. See Also Jolt Rope Component Rope Render Component"
  },
  "pages/docs/effects/ropes/rope-render-component.html": {
    "href": "pages/docs/effects/ropes/rope-render-component.html",
    "title": "Rope Render Component | ezEngine",
    "keywords": "Rope Render Component The rope render component is used to render a rope or cable. The rope simulation is done by other components, such as the rope component or the fake rope component. The rope render component has to be attached to the same object as the simulation component. Component Properties Material: The material to use for rendering. Color: The object color. This is typically multiplied into the diffuse part of the material, but the shader may use the color in different ways, as well. Thickness: The thickness of the rope mesh. Detail: How many polygons the rope mesh uses to appear round. More detail costs more rendering performance. Note that this has no effect on how many segments the rope is made up of, that is a property of the simulation component. Subdivide: Whether the rope mesh should have an extra segment subdivision to make it look smoother at strong bends. This doubles the amount of triangles in the mesh. Enable this for ropes that still should look as good as possible even under strong curvature. UScale: The texture is always wrapped exactly once around the rope. Howver, along the rope's length, this option defines how often it will be repeated. See Also Jolt Rope Component Fake Rope Component"
  },
  "pages/docs/effects/sky.html": {
    "href": "pages/docs/effects/sky.html",
    "title": "Sky | ezEngine",
    "keywords": "Sky By default the background of a rendered scene is black. To change this, you need to create a game object in the scene and attach a sky component. Video: How to set up a Skybox SkyBox component The SkyBox component implements a simply sky, which displays a cubemap texture as a static background. The position and scale of the game object has no effect on the sky, it will always appear behind all other geometry. The rotation, however, can be used to orient the sky as desired. CubeMap: The cubemap texture asset to use. ExposureBias: This specifies how bright the sky will appear. A higher value results in a brighter sky. InverseTonemap: Switches the tonemapping mode. For HDR skyboxes this should stay off. For skyboxes that do not have high-dynamic range values, enabling this mode will improve brightness and contrast of the colors. UseFog, VirtualDistance: If enabled, fog will be applied to the sky. In that case VirtualDistance is being used to compute how foggy the sky should appear. See Also Fog Textures Lighting"
  },
  "pages/docs/effects/wind/simple-wind-component.html": {
    "href": "pages/docs/effects/wind/simple-wind-component.html",
    "title": "Simple Wind Component | ezEngine",
    "keywords": "Simple Wind Component The simple wind component implements a very basic wind system. By placing a component of this type in a scene, the simple wind world module is created. Things that can react to wind are then able to retrieve a wind value at any location. The simple wind system provides one global wind value, which changes randomly. Additionally, it supports wind volume components. If any such shape is in a scene, its contribution is added to the global wind value, for objects that are inside such a volume. Component Properties MinWindStrength, MaxWindStrength: The minimum and maximum strength with which the wind shall blow. A random value in between will be chosen every couple of seconds. To make it easier to get different things working well with each other, the wind values are hard-coded. They are inspired by the Beaufort scale. Most things that can react to wind (for example particle effects or ropes) also have a wind influence parameter for tweaking how strongly they react to wind. The Beaufort scale enables you to get an idea what reaction to expect, e.g. you know how an effect should look under a light breeze or under storm conditions, and can then tweak the wind influence value accordingly. MaxDeviation: How much the wind direction may deviate from the local x-axis. Set this to the maximum value, if the wind is allowed to come from anywhere. See Also Wind Wind Volume Components"
  },
  "pages/docs/effects/wind/wind-volume-components.html": {
    "href": "pages/docs/effects/wind/wind-volume-components.html",
    "title": "Wind Volume Components | ezEngine",
    "keywords": "Wind Volume Components Wind volume components are used to define areas in a scene where wind should blow in a certain way. Note: On their own these components won't have any effect in a scene. A wind system additionally has to be set up. Placing a simple wind component in a scene creates a basic wind system. Shared Component Properties All wind volume components have these properties: Strength: The strength with which the wind shall blow. ReverseDirection: If set, the wind direction is reversed. This can be used to pull things inwards, instead of pushing them. BurstDuration: If this is set to zero, the wind blows continuously. Otherwise it blows for the specified amount of time and then stops. OnFinishedAction: If BurstDuration is non-zero, the component will deactivate itself once the burst is done. Additionally, the component may delete itself or the entire object. Note that if None is selected, the wind burst can be restarted, simply by reactivating the component. Sphere Wind Volume Properties Radius: The radius of the wind volume. Cylinder Wind Volume Properties Length, Radius: The size of the cylinder. Mode: How the wind force is computed. Directional: The wind force is along the main cylinder axis. Vortex: The wind whirls around the cylinder's axis. See the video below. Cone Wind Volume Properties Angle, Length: Angle and length of the cone shape. See Also Wind Simple Wind Component"
  },
  "pages/docs/effects/wind/wind.html": {
    "href": "pages/docs/effects/wind/wind.html",
    "title": "Wind | ezEngine",
    "keywords": "Wind Some components can be animated by wind. For instance particle effects, ropes and Kraut trees will react to wind. Usually these animations are for decorative purposes. Wind is implemented as a world module. Thus, it is possible to have different wind system implementations, and choose the most suitable for each scene. For example, one system may do a full volumetric fluid simulation, whereas another does not. You instantiate a specific wind system by adding the respective component to a scene. At this time, EZ only ships with a basic implementation. You instantiate it with the simple wind component. As long as there is no such component in a scene, there won't be any wind. Querying Wind Values At runtime you query the wind value by location. First you need to retrieve the wind world module: const ezWindWorldModuleInterface* pWindInterface = GetWorld()->GetModuleReadOnly<ezWindWorldModuleInterface>(); Make sure to check the pointer for nullptr, which happens when there is no wind system set up for a scene. Then the wind can be queried by location: ezVec3 wind = pWindInterface->GetWindAt(position); This returns a vector with the direction and strength of the wind at the queried position. To react properly to wind, this value must be polled every frame. However, be careful to query only few values. Depending on the active system, this can be a very fast or a rather slow operation. However, usually wind doesn't change drastically within short distances. For example the Kraut trees only query the wind once per tree instance, there is no need for finer detail. Note: The wind system returns a vector of wind direction and strength. This alone often does not yield a convincing wind effect though. For example a tree or a piece of cloth would only be pushed to one side, but that looks very unnatural. Instead objects should flutter in the wind, e.g. wildly swing up and down or sideways. Such behavior is very object specific and must be implemented on top of the general wind value. The utility function ezWindWorldModuleInterface::ComputeWindFlutter() might be sufficient to get you started. Controlling Wind To add wind locally, have a look at the wind volume components. These can be used both for static wind fields, for example to make a flag blow in the wind nicely, as well as for short lived dynamic effects, such as the shockwave of an explosion. Affecting Physics Objects Be aware that wind does not affect any physics objects. Such behavior could be implemented, but it would be difficult to not have a serious performance impact, since it would keep the physics engine constantly busy (usually objects go to sleep when no forces act upon them, but wind would be a constantly active force). Instead, explosions and such rather use a physics shape query to determine objects in range, and then apply a short impulse to only those objects once. See the area damage component as an example. Custom Wind Systems It is possible to write your own wind system. Just implement a new world module, derive it from ezWindWorldModuleInterface and override the GetWindAt() function. Put your code into a custom engine plugin and also add a custom component type to instantiate your wind world module, and make it configurable. For inspiration, just have a look at ezSimpleWindWorldModule and ezSimpleWindComponent. See Also Wind Volume Components Simple Wind Component"
  },
  "pages/docs/gameplay/area-damage-component.html": {
    "href": "pages/docs/gameplay/area-damage-component.html",
    "title": "Area Damage Component | ezEngine",
    "keywords": "Area Damage Component The area damage component posts an ezMsgDamage to all objects in its vicinity every time it is triggered. It may also send an ezMsgPhysicsAddImpulse to push objects away from its location. This is used to implement the effect of explosions and other things that should damage close-by objects. Component Properties OnCreation: If enabled, the component will apply damage the moment it gets activated. Radius: The radius in which objects will receive damage. CollisionLayer: The physics collision layer to use to find objects to which to apply damage. Damage: The maximum amount of damage to apply. Damage is scaled down linearly by distance, so an object further away will receive less damage. Impulse: An optional physical impulse to apply to damaged objects. This will push objects away from this object. The applied impulse is also scaled down linearly by distance. Scripting ApplyAreaDamage(): This function can be called manually to control when and how often this component applies damage. For example a 'dangerous' area can be implemented by repeatedly triggering a component of this type. See Also Spawn Component Timed Death Component"
  },
  "pages/docs/gameplay/grabbable-item-component.html": {
    "href": "pages/docs/gameplay/grabbable-item-component.html",
    "title": "Grabbable Item Component | ezEngine",
    "keywords": "Grabbable Item Component The Grabbable Item component is used to define points on an object that are good anchor points to pick the object up. The component has no behavior by itself, it only holds data. Other components, such as the grab object component can utilize this information to improve the experience of picking an object up. Or a game may even only allow objects with this component to be grabbable in the first place. Both the position and rotation of each grab point are important. The rotation defines how the object will be oriented when it is picked up. Note: The component can be attached to any object, but for example the grab object component expects it to be attached to the same object as the dynamic actor component that has been picked through its raycast. Component Properties GrabPoints: An array holding all the grab points. Extend this array to add grab points. Use an item's manipulators to adjust the position and orientation of a grab point. See Also Jolt Grab Object Component"
  },
  "pages/docs/gameplay/headbone-component.html": {
    "href": "pages/docs/gameplay/headbone-component.html",
    "title": "Head Bone Component | ezEngine",
    "keywords": "Head Bone Component The head bone component is a very simple component that applies a vertical rotation, ie around the right axis (+Y). It clamps the maximum rotation, so that it only applies a limited relative rotation to the parent object. This is mainly used for vertical rotation of a camera attached to a character controller. The character controller already defines the horizontal rotation, ie the direction into which it moves. Usually custom code would take some input and forward it to the head bone component to allow the player to look up and down. This component is for very simple use cases. In a proper game one would often want to have more advanced camera behavior, which should be done through a custom C++ components. Component Properties VerticalRotation: The maximum relative rotation away from the parent. Scriptable Functions SetVerticalRotation(angle): Sets the vertical rotation to a known value. ChangeVerticalRotation(angle): Adds to the rotation. The final rotation is always clamped to the valid range. See Also Camera Component Character Controller Custom Code with Visual Scripts"
  },
  "pages/docs/gameplay/marker-component.html": {
    "href": "pages/docs/gameplay/marker-component.html",
    "title": "Marker Component | ezEngine",
    "keywords": "Marker Component The marker component can be used to markup objects and locations with gameplay relevant semantical information. To implement game mechanics, especially some form of AI, your code must be able to reason about objects in the game. An AI for an NPC must be able to scan its nearby environment to detect objects that it can interact with, other NPCs, the player and locations that may be of interest. The spatial system is there to provide efficient means to do such queries. Using spatial queries, you can find all objects within an area that belong to some group. For this to work, you obviously need to insert objects into the spatial system. The marker component is a simple and convenient way to do so. All that the marker component does, is to insert a sphere of a given size and category into the spatial system, so that the object that the component is attached to, can be found with spatial queries. Note: Keep in mind that the number of categories available for use is limited to about 25. You should therefore prefer generic categories where possible. Component Properties Marker: Which spatial data category to use for this marker. Radius: The size of the marker. Examples The marker component can be used for many purposes. Here are a couple of examples: Tag NPCs and players. Tag objects that can be picked up. The position and rotation of the marker node can be used to orient the object when it is picked up. Set up points for visibility checks and for targeting when determining whether an NPC sees another NPC. Each character may have a 'target' node at its head, its torso, each elbow and knee. The AI can then do raycasts against all these points to determine whether a character is visible, and if so, shoot at one of the visible markers. Identify usable objects, such as buttons. The marker should be used to mark up that something is usable at that location, other mechanisms should be used to narrow down what the function is and how an AI could interact with it. Mark useful locations, for example good hiding spots, or sniper positions. Warn of dangerous locations. A grenade may have a large 'danger' marker attached, which informs NPCs to run away. The same can be used in front and behind vehicles, where they are enabled when a car starts driving, such that NPCs will get out of its way. For an example how marker components and spatial queries can be used to find nearby objects, have a look at the Sample Game Plugin. See Also Spatial System Sample Game Plugin Tags"
  },
  "pages/docs/gameplay/player-start-point.html": {
    "href": "pages/docs/gameplay/player-start-point.html",
    "title": "Player Start Point | ezEngine",
    "keywords": "Player Start Point The Player Start Point component is used to indicate a position in a level from where the game should start playing. The component references a prefab which represents the player object. This prefab must be built such that it handles input and implements the desired player movement and interactions. When the game is run either using Play-the-Game mode or stand-alone, it will execute its game state. The default game state implementation will look for a player start component and instantiate the referenced prefab. This is most useful for games where a specific object represents the player. For games that do not have a player presence, such as RTS games, the custom game state should ignore this type of component and instead implement the player interaction logic itself. When a scene contains a player start point component, you can use the Play From Here feature. See Also Running a Scene ezPlayer"
  },
  "pages/docs/gameplay/power-connector-component.html": {
    "href": "pages/docs/gameplay/power-connector-component.html",
    "title": "Power Connector Component | ezEngine",
    "keywords": "Power Connector Component This component is for propagating the flow of power in cables or fluid in pipes and determine whether it arrives at a receiver. It is meant for building puzzles where you have to connect the right objects to power something. It uses physics constraints to physically connect two pieces and have them snap together. The component also reacts to being grabbed (ezMsgObjectGrabbed) to disconnect. On its own this component doesn't do anything. However, it can be set to be connected to another object with a power connector component, in which case it would propagate its own output as the input on that component. If its output is non-zero and thus the input on the connected component is also non-zero, the other component will post ezEventMsgSetPowerInput, to which a script can react and for example switch a light on. Connectors are bi-directional (\"full duplex\"), so they can have both an input and an output and the two values are independent of each other. That means power can flow in both or just one direction and therefore it is not important with which end a cable gets connected to something. To enable building things like cables, each power connector component can also have a buddy, which is an object on which another power connector component exists. If a connector gets input, that input value is propagated to the buddy as its output value. Thus when a cable gets input on one end, the other end (if it is properly set as the buddy) will output that value. So if that end is also connected to something, the output will be further propagated as the input on that object. This can go through many hops until the value reaches the final connector (if you build a circular chain it will stop when it reaches the starting point). The component automatically connects to another object when it receives a ezMsgSensorDetectedObjectsChanged, so it should have a child object with a sensor. The sensor should use a dedicated spatial category to search for markers where it can connect. To have a sensor (or other effects) only active when the connector is grabbed, put them in a child object with the name ActiveWhenGrabbed and disable the object by default. The parent power connector component will toggle the active flag of that object when it gets grabbed or let go. To build a cable, don't forget to set each end as the buddy of the other end. Component Properties Output: Sets how much output (of whatever kind) this connector produces. If this is zero, it is either a receiver, or a pass-through connector, e.g. a cable, or just currently inactive. If this is non-zero, it acts like a source, and when another connector gets connected to it, that output will be propagated through the connection/buddy chain. Buddy: If this is intended to act like a cable with two ends, specify the object references to the other end of the cable here. This way, if this end gets power, the other end will output that power, and vice versa. ConnectedTo: If this object is supposed to start in a state connected to something else, e.g. a power socket or another cable, use this object references to configure it connected. Note that this will create a physics joint with between this object and the target object. If they aren't aligned already, this will make the two objects snap together during the first simulation steps. Message Handlers ezMsgSensorDetectedObjectsChanged: Tells the connector that it is close to another connector that it should attach to. If a sensor components is active on the same object, this can automatically make the connector connect to other things that it comes close to. ezMsgObjectGrabbed: Tells the connector that it was just grabbed, e.g. by a grab object component. Will make it disconnect, if it is currently attached to anything. Events ezEventMsgSetPowerInput: This event is sent every time the available power input changes. This is the power that 'arrives' at the connector. Note that power is never 'used up'. It should just be seen as a threshold value to decide whether something receives enough power to be active. See Also Jolt Rope Component Jolt Fixed Constraint Component Sensor Components Spatial System"
  },
  "pages/docs/gameplay/projectile-component.html": {
    "href": "pages/docs/gameplay/projectile-component.html",
    "title": "Projectile Component | ezEngine",
    "keywords": "Projectile Component The projectile component makes an object move along a straight line (with optional gravity), checks for collisions and triggers surface interactions. It also applies damage and a physical impulse when it hits something. There are many ways projectiles may work in different games. The projectile component only implements the most commonly needed functionality. For some games this may already be sufficient. For more creative games you most certainly need to implement your own component. The built-in projectile component is mostly meant as a showcase and demonstration, how you can generally implement such functionality, especially how to interact with surfaces. Component Properties Speed: The speed (m/sec) at which the projectile will fly along the +X axis. For dramatic effect it is often more interesting for bullets to fly much slower than is realistic, so that one can see them. GravityMultiplier: How strong gravity should affect the bullet. If set to 0, the projectile will fly straight ahead. With values > 0, it will fall downwards. MaxLifetime: If the projectile lives longer than this, it will destroy itself. OnTimeoutSpawn: If the projectile destroys itself because it didn't hit anything before MaxLifetime ended, it will spawn this prefab. Can be used to let rockets 'explode' after a while. CollisionLayer: The physics collision layer to use for raycasting whether the projectile hit something. ShapeTypesToHit: Specifies what types of physics geometry to hit, at all. FallbackSurface: If the projectile hits something that has no surface assigned, it will assume that this type of surface was hit. This just guarantees that you always get any kind of hit response, even when the hit geometry is just dummy or placeholder geometry. Interactions: This array allows you to specify exactly what the projectile will do, when it hits different types of surfaces. Each entry is used to configure the action of the projectile if a certain type of surface is hit. Note that surfaces and surface interactions already work hierarchically. That means for most types of projectiles this array only needs to contain a single entry, with a base surface, and all it needs to specify is what 'interaction' to trigger. The exact type of surface that is hit, will then decide what prefabs to spawn for that kind of interaction. Interaction Properties Surface: The (base) type of surface for which this array element applies. Reaction: How the projectile itself should react. Absorb means it will stop there, Reflect means the bullet will bounce off, Attach means the bullet will stop but attach itself to the target object and Pass Through means it will continue on its path. Interaction: The surface interaction to trigger when the projectile hits this type of surface. Impulse: The amount of physical impulse to exert on the hit object. Damage: The amount of damage to apply to the hit object. Example The image below shows a basic bullet projectile setup: The projectile flies relatively fast, has a short lifetime and uses a collision layer that would hit most physical objects. In the interactions array we only have two elements. One is the default surface, which is the base surface for all others, that means no matter what type of surface a projectile hits, this entry will always apply, unless there is a more specific entry. As you can see, the bullet is configured to be absorbed when it hits anything solid, it will apply some damage and push the object backwards with some impulse. It is also set to trigger the 'BulletImpact' surface interaction. What that means, ie. whether there will be sparks or splinters and what sounds to play, etc, is all configured through the surfaces. There is a second entry in this array, for when the bullet hits water. The main difference here is, that the bullet reaction is set to pass through, which means that the projectile will not stop, but continue forwards. It will still trigger the 'BulletImpact' surface reaction, though, so this allows the surface to spawn a water splash effect. Conclusion With these settings you can build a wide variety of interesting projectiles, especially if you use all the flexibility of surfaces and surface interactions. Still, there are many things that are not possible, and you are encouraged to write your own projectile component(s) that suit your needs. The code of the ezProjectileComponent is a good starting point for inspiration. See Also Surfaces Collision Layers Particle Effects"
  },
  "pages/docs/gameplay/raycast-placement-component.html": {
    "href": "pages/docs/gameplay/raycast-placement-component.html",
    "title": "Raycast Placement Component | ezEngine",
    "keywords": "Raycast Placement Component The raycast placement component does a ray cast and positions a target object there. The image below shows raycast placement components being used together with a beam components to create laser beams. This component does a ray cast along the forward axis of the game object it is attached to. If this produces a hit, the target object is placed there. If no hit is found the target object is either placed at the maximum distance or deactivated depending on the component configuration. This component can also trigger messages when objects enter the ray. E.g. when a player trips a laser detection beam. To enable this set the trigger collision layer to another layer than the main ray cast and set a trigger message. Sample setup: CollisionLayerEndPoint = Default CollisionLayerTrigger = Player TriggerMessage = \"APlayerEnteredTheBeam\" This will lead to trigger messages being sent when a physics actor on the 'Player' layer comes between the original hit on the default layer and the ray cast origin. Component Properties MaxDistance: The maximum distance to do the raycast. DisableTargetObjectOnNoHit: If set, the RaycastEndObject is set to inactive when the raycast hits nothing within MaxDistance. This can be used to for things like laser pointers, where the target object represents the 'laser dot'. If the laser pointer hits nothing, the laser dot object should temporarily disappear. Once the raycast hits something again, the component will make sure to reactivate the target object again. RaycastEndObject: A referenced object that this component should affect. Every time the placement component determines a different position for the raycast hit, it will move the referenced object there. ForceTargetParentless: If set, the placement component will make sure that the referenced RaycastEndObject will be detached from any parent object. The practical reason for this is, that to prevent multiple objects from modifying the position of the end object, it should have no parent game object, which may pass down its own transform changes. However, when the end object is part of a prefab, it will always have a parent, and that parent may need to move. For example when a weapon is attached to a character controller. Therefore the placement component can take care of detaching the end object at the appropriate time. CollisionLayerEndPoint: The collision layer to use for the raycast to detect where the RaycastEndObject should be placed. CollisionLayerTrigger: An optional, different collision layer to detect whether a specific type of object is closer than the placed end object. If this is the case, the event ezMsgTriggerTriggered is raised, using the identifier given in TriggerMessage. This can be used for things like trip mines, where the first collision layer detects how far away the closest wall is, and the second collision layer checks whether any player or NPC has come between the mine and the opposing wall. TriggerMessage: The trigger identifier message to use when CollisionLayerTrigger detects an object. See Also Beam Component Trigger Component"
  },
  "pages/docs/gameplay/spawn-component.html": {
    "href": "pages/docs/gameplay/spawn-component.html",
    "title": "Spawn Component | ezEngine",
    "keywords": "Spawn Component The SpawnComponent is frequently used to spawn instances of prefabs at runtime. The component references a prefab. It can then either spawn this prefab continuously in intervals, or at will by triggering the spawn command from (script) code. Component Properties Prefab: The prefab that will be spawned by this component. AttachAsChild: If true, the spawned object will be attached to the owner of the SpawnComponent. In most cases this should be disabled. SpawnAtStart: If true, the SpawnComponent will spawn the prefab immediately when it gets activated. SpawnContinuously: If true, the component will continue to spawn more and more prefab instances. This can only be stopped either through custom (script) code or by deleting the spawn component. MinDelay, DelayRange: The minimum time that has to pass before the component will spawn another instance, plus some random delay. This not only applies to the SpawnContinuously case, but also to cases where the spawn may be triggered from code. Meaning, this can be used to limit how often an action is allowed. For example, a gun can use a SpawnComponent to launch projectiles, and the gun code can simply trigger the SpawnComponent every time the user clicks. However, due to the MinDelay, the gun will only fire every once in a while, without having to write that logic in the gun code. Deviation: When a new prefab instance is created, it will be positioned at the location of the SpawnComponent. The Deviation allows you to add a random rotation away from the X axis. In EZ most components use the +X axis as their main axis of operation. For instance, projectiles fly along +X, spot lights point into +X direction, etc. Therefore the SpawnComponent tilts the new instances away from the +X axis and all prefabs should be authored to work with this accordingly. See Also Prefabs Spawnbox Component"
  },
  "pages/docs/gameplay/spawnbox-component.html": {
    "href": "pages/docs/gameplay/spawnbox-component.html",
    "title": "Spawnbox Component | ezEngine",
    "keywords": "Spawnbox Component This component spawns prefabs inside a box. The prefabs are spawned over a fixed duration. The number of prefabs to spawn over the time duration is randomly chosen between a minimum and maximum value. Each prefab may get rotated around the Z axis and tilted away from the Z axis. If desired, the component can start spawning automatically, or it can be (re-)started from code. If spawn continuously is enabled, the component restarts itself after the spawn duration is over, thus for every spawn duration the number of prefabs to spawn gets reevaluated. Component Properties HalfExtents: The dimensions of the box in which the prefabs are spawned. Prefab: The prefab that will be spawned by this component. SpawnAtStart: If true, the component will spawn the prefab immediately when it gets activated. SpawnContinuously: If true, the component will restart itself after a round. How many prefabs to spawn is reevaluated for each round. MinSpawnCount, SpawnCountRange: How many prefabs to spawn during one round. Duration: The lenght of one spawn round. The randomly chosen amount of prefabs to spawn is distributed over this time. MaxRotationZ: How much the spawned objects may be rotated away from the forward axis, around the Z (up) axis. MaxTiltZ: How much to tilt objects away from the Z axis. See Also Prefabs Spawn Component"
  },
  "pages/docs/gameplay/timed-death-component.html": {
    "href": "pages/docs/gameplay/timed-death-component.html",
    "title": "Timed Death Component | ezEngine",
    "keywords": "Timed Death Component The timed death component is used to automatically delete the object that it is attached to, after a timeout. Additionally, it may spawn a prefab when its timeout has been reached. Component Properties MinDelay: The minimum time to wait before deleting the parent object. DelayRange: An optional random range to wait. If this is set to zero, the component will execute exactly after a delay of MinDelay. TimeoutPrefab: If the component is triggered to delete the object, it may additionally spawn an instance of the selected prefab, at the location of the object. See Also Spawn Component"
  },
  "pages/docs/graphics/always-visible-component.html": {
    "href": "pages/docs/graphics/always-visible-component.html",
    "title": "Always Visible Component | ezEngine",
    "keywords": "Always Visible Component The always visible component makes the renderer consider the object that this component is attached to as always in view. This effectively disables culling optimizations. This can be used to enforce rendering of an object at all times, even when it is outside the view. The need for this is extremely rare, though. See Also"
  },
  "pages/docs/graphics/camera-component.html": {
    "href": "pages/docs/graphics/camera-component.html",
    "title": "Camera Component | ezEngine",
    "keywords": "Camera Component The camera component is used to tell the renderer from which position and with which settings to render the scene. Apart from the component that acts as the main camera, there can be additional cameras in active use for render to texture (TODO) effects. Additionally, camera components can be used in the editor as 'bookmarks' to be able to quickly jump to specific positions in a level. When an object with a camera component is selected, the editor shows a preview of what the camera sees in the top left corner. Video: How to add a Game Camera Main Camera The camera settings that are used for rendering the scene are fully under control of the game state. Every frame it decides where to place the main camera and with which settings. At this point, no camera component is involved, your game may control the main camera without having any camera component in the scene. However, the default behavior of the game state (see ezFallbackGameState), is to check the scene for a camera component that has its UsageHint set to Main View. Unless you write a custom game state and override this behavior, the game state will simply copy all the camera settings from the first camera component that it can find with this usage hint. Consequently, you can control the main camera, by placing a component and setting its UsageHint to Main View. If you want a different camera component to take over from the current one, you need to change the usage hint on those camera components. Important: The ezFallbackGameState is mostly for development and therefore has other convenience features for cameras. For example, you can switch through cameras in the scene using Page Up and Page Down. If you release a game, you should make sure to disable this behavior. Other Cameras A scene can contain any number of camera components. Unless they are referenced by other systems, they won't do anything and will have no performance impact. Camera Bookmarks Camera components can be placed as 'bookmarks', such that people working on a scene can quickly move the editor camera to areas of interest. This chapter describes how to do so. Include/Exclude Tags By default a camera renders all objects in the scene. Sometimes it can be desirable, though, for a camera to render only specific objects, or to ignore those. For example you may have descriptive labels attached to some objects, which the player can display on demand. Using the inclusion and exclusion tags on the camera, you can control which objects are going to be considered for rendering from this camera view. If any inclusion tag is set, only objects with any of these tags are rendered. If an exclusion tag is set, no object with any of these tags is rendered. Important: Don't forget that tags are not inherited. You can't hide an object by setting a tag on its parent node. So for the example with the object labels above, you would assign a 'label' tag to those objects and on your camera you would set 'label' as an exclusion tag. This way those objects are not rendered. When the player wants to see the labels, you would simply remove the exclusion tag from the camera, to make them appear. Important: Especially when using include tags, be aware that not only meshes, but also light sources (and everything else that's part of the rendering process) are affected by this. If you forget to set the necessary include tags on your light sources, the output will stay dark. Render to Texture Camera components can be used to render their view to a texture, which can then be referenced by a material and displayed on any mesh. To enable this mode, the UsageHint has to be set to Render to Texture. You also need to select a CameraRenderPipeline. The render pipeline defines how the scene is rendered and which rendering effects are applied. You need to configure which render pipelines (TODO) are available to the cameras in the asset profiles. The RenderTargetOffset and RenderTargetSize allow you to render only to a part of the texture. Note that rendering to a texture involves additional steps. See the chapter about render-to-texture (TODO) for full instructions. Component Properties EditorShortcut: Used to configure level cameras. UsageHint: A hint what the camera is supposed to be used for. Systems like the game state may use this information to use or ignore this component. Mode, FOV, Dimensions: Configure whether this is a perspective or an orthographic view and how the other options are applied. Field-of-view (FOV) is used for perspective modes, dimensions are used for orthographic modes. NearPlane, FarPlane: The distances for the near and far plane. For best performance keep the far plane distance as low as possible. To prevent z-fighting make sure that the near plane is not too close and the far plane is not too far out. IncludeTags, ExcludeTags: See Include/Exclude Tags above. CameraRenderPipeline: Allows you to select a specific render pipeline (TODO) that shall be used to render the output from this camera. Available render pipelines are set up in the asset profiles. RenderTarget, RenderTargetOffset, RenderTargetSize: Only available when UsageHint is set to Render to Texture. Aperture, ShutterTime, ISO, ExposureCompensation: These options are currently only used for tonemapping. They all affect the final exposure value, which means you can adjust any one of them to change the brightness of the output. In the future these values may be used for motion blur and depth-of-field. See Also Editor Camera Render to Texture (TODO) Render Pipeline (TODO) Asset Profiles Tags"
  },
  "pages/docs/graphics/lighting/ambient-light-component.html": {
    "href": "pages/docs/graphics/lighting/ambient-light-component.html",
    "title": "Ambient Light Component | ezEngine",
    "keywords": "Ambient Light Component The Ambient Light Component lights up all objects equally. It is used to ensure that no area of a level is ever entirely dark. The component uses two colors, one for light coming from the sky (top down) and one for light coming from the ground (bottom up). Usually the top color should be slightly brighter and the bottom color should represent the top color as it would appear after being bounced off the ground. Both colors and the overall intensity should be kept low, otherwise the colors in the scene will appear washed out due to missing contrast. The image below shows a scene without ambient light on the left, and with ambient light on the right. Ambient light should be used sparingly. Prefer to use directional light components for the main sky and sun light contributions. You can even use multiple directional light components (without shadows and low intensity) to fake ambient light but with more directionality, ie. by having each directional light shine from roughly the same direction, to add some variation. Instead of ambient light, you could also use a sky light component. Component Properties TopColor: The ambient light coming from above. This is used to illuminate polygons that are facing the sky. BottomColor: The ambient light coming from below. This is used to illuminate polygons that are facing the ground. Intensity: The overall light intensity. See Also Lighting Sky Light Component"
  },
  "pages/docs/graphics/lighting/directional-light-component.html": {
    "href": "pages/docs/graphics/lighting/directional-light-component.html",
    "title": "Directional Light Component | ezEngine",
    "keywords": "Directional Light Component The directional light component adds a light source that illuminates the entire scene from one direction. This is typically used for sun light. Since directional light affects everything, it isn't possible to cast shadows everywhere. Instead, shadows are restricted to a region around the camera and objects that are too far away from the camera, won't cast shadows. You can use multiple directional light sources, for example if you want directional ambient light, however, for performance reasons only one directional light should cast shadows. Component Properties See this page for shadow related component properties. LightColor, Intensity: The color and brightness of the light. NumCascades: How many shadow cascades to use. The more cascades are used, the crisper shadows close to the camera become. However, each cascade costs additional performance. MinShadowRange: How far from the camera the light should cast shadows. A low value means that only objects a short distance away will cast shadows, and objects farther away won't. FadeOutStart: At what fraction of the shadow range it should start to fade out. For instance, if the MinShadowRange is set to 10 meters, and FadeOutStart is set to 0.8, then the shadows will start to fade out at a distance of 8 meters. SplitModeWeight: TODO NearPlaneOffset: TODO See Also Lighting Dynamic Shadows"
  },
  "pages/docs/graphics/lighting/dynamic-shadows.html": {
    "href": "pages/docs/graphics/lighting/dynamic-shadows.html",
    "title": "Dynamic Shadows | ezEngine",
    "keywords": "Dynamic Shadows Light sources, such as point lights, spot lights and directional lights may cast dynamic shadows. The image below shows a spot light casting shadows: Whether an object casts a shadow depends on whether it has the tag CastShadow set. Performance Shadows are implemented using shadow maps. That means every light source that shall cast a shadow, has to first render the current scene depth to a texture. This is a very costly operation, which is why you should keep the number of shadow casting light sources as low as possible. This also is more expensive, the more complex the shadow casting geometry is. Therefore consider switching shadows off for complex geometry and for small objects that don't contribute much anyway. For large complex geometry, you can also use low resolution proxy geometry for casting shadows, though you have to be careful with self-shadowing artifacts if the geometry is very different. Use your knowledge about the scene to switch shadow casting lights off when they are not needed. For example, if you need a light inside a room to cast dramatic shadows, but the room entrance is only visible from a corridor, use a trigger to only switch the light on when the player can actually see the light. Prefer to use spot lights over point lights, if that makes it possible to get away without shadows in the first place. All shadow casting light sources share a single shadow texture atlas. Every frame the engine determines the on-screen size of each light source and then allocates some area of the texture atlas to each light source. That means lights that are farther away will use a lower resolution shadow map than close up lights. Shadow Quality Shadow maps are prone to artifacts called shadow acne. Either light leaks through objects where there should be shadows, or shadows leak through objects where there should be light. This happens due to precision issues, especially when a shadow is cast nearly perpendicular to a surface. The SlopeBias and ConstantBias properties (see below) allow to tweak the shadows to reduce this issue in specific places, but there is no solution that will always work. Shadow Component Properties Dynamic light sources such as directional lights, point lights and spot lights can cast dynamic shadows. These components all have properties to tweak the shadows for quality. The following properties are common to these component types: CastShadows: If enabled, the light will cast dynamic shadows. Important: Casting shadows costs a lot of performance. Make sure to only have a small number of lights with shadows active at any one time, otherwise your game may perform poorly. PenumbraSize: This value specifies how soft the edge of shadows is supposed to be. The image below shows a penumbra size of 0 on the left and 0.5 on the right: SlopeBias, ConstantBias: TODO See Also"
  },
  "pages/docs/graphics/lighting/lighting-overview.html": {
    "href": "pages/docs/graphics/lighting/lighting-overview.html",
    "title": "Lighting | ezEngine",
    "keywords": "Lighting Lighting is the most important aspect of making a scene look good. Physically Based Rendering There are many formulas for computing lighting on surfaces. The defacto industry standard, which is also used in ezEngine, is Physically Based Rendering (PBR) which describes a surface in terms of color, the surface normals, its roughness, whether it is a metal. Using this data, very convincing lighting can be computed. Therefore the standard type of material requires you to provide such textures. Optionally an occlusion texture can pronounce the lighting for small crevices. Static vs. Dynamic Lighting Many games differentiate between static or baked lighting, and dynamic lighting. Static lighting is precomputed and typically stored in lightmaps (dedicated textures) and other data structures. Dynamic lighting does not require any preprocessing or extra data. Baked lighting typically has the advantage that it can look much better because it can simulate light bounces and thus illuminate areas that are not directly lit. Currently ezEngine only supports dynamic lighting. That means every light source that you add to the scene can be moved around and change its color or brightness. It also means that every light source has a performance cost. The renderer uses a clustered forward rendering approach which can handle a relatively large amount of light sources efficiently. The most important rule is to reduce the number of overlapping light sources. The editor render modes allow you to look for hotspots. Shadows Dynamic lights have the disadvantage that they don't provide shadows by default. Instead, casting shadows is a separate process, which costs a lot of performance for every light source involved. Therefore, each light source requires you to decide whether it should cast shadows or not. You can use many small fill lights, as long as they don't cast shadows, but you should keep the number of shadow casting lights as low as possible, and each light should only cover an area as small as possible. For more details see the chapter about dynamic shadows. Light Component Types There are different component types to provide different types of lighting: Ambient Light Component: For lighting up a scene in general. Directional Light Component: For sun/moon light. Point Light Component: For light bulbs and overall fill lights. Spot Light Component: For flashlights and directed lighting. Sky Light Component: For dynamic light contribution from the sky. Reflection Probe Components: For localized reflection probes. See Also Materials Dynamic Shadows Render Modes"
  },
  "pages/docs/graphics/lighting/point-light-component.html": {
    "href": "pages/docs/graphics/lighting/point-light-component.html",
    "title": "Point Light Component | ezEngine",
    "keywords": "Point Light Component The point light component adds a dynamic light source that illuminates the scene equally in all directions. This is the most common type of light source. Each light source costs performance. Try to reduce the number of light sources that illuminate the same area. Use the light count render mode to find hotspots. Note that unless a light source casts dynamic shadows, it's light will shine through walls. Casting shadows is very costly, though. When it is possible to avoid casting shadows, for example by using multiple smaller lights instead, the performance should always be better. Video: How to create a light source Component Properties See this page for shadow related component properties. LightColor, Intensity: The color and brightness of the light. Range: The distance over which the light source affects geometry. By default this is set to Auto, meaning the necessary range will be computed from the light's brightness. For full control, the range can be set manually. The light will always attenuate to zero within the given range, so by specifying a small range you can create a small, yet very bright light. See Also Lighting Spot Light Component"
  },
  "pages/docs/graphics/lighting/reflection-probe-components.html": {
    "href": "pages/docs/graphics/lighting/reflection-probe-components.html",
    "title": "Reflection Probe Components | ezEngine",
    "keywords": "Reflection Probe Components The two reflection probe components, box reflection probe component and sphere reflection probe component allow for localized reflections to be added to a scene. The probe makes a 360 degree screenshot of the scene to capture the overall lighting at this location into a cubemap which can be sampled at runtime to generate reflections. Scene Setup Sphere reflection probes project the captured cubemap to infinity, i.e. no parallax effect is observed when moving. Box reflection probes on the other hand project the cubemap to their extents, allowing for parallax correction of the reflection when moving in the scene. Probes take the parent game object's scale into account. Thus, sphere probes can actually be ellipsoids if scaled non-uniformly. The image below shows the difference between a box (to the left) and a sphere reflection probe (to the right). While the reflection in the metal ball both look convincing, the reflection on the floor is clearly off for the sphere reflection probe. By default, each probe captures everything in its radius. In many cases it makes sense to tag the ExcludeTags with SkyLight though so that the resulting cubemap is transparent where no geometry was rendered. This allows for the cubemaps to be dynamically composited at runtime. The benefits of this is that the skylight can change dynamically and won't be baked into the probe and it allows for parallax between e.g. a box probe and the sky light fallback reflection. The captured reflection is only visible in the probe's influence volume. For sphere probes this is limited by their radius and for box probes by their extents. At runtime, all probes are sorted by their volume and the smallest probe is sampled first. If the reflection is transparent in the cubemap, then next bigger cubemap that influences the geometry is sampled next. This continues until we hit an opaque pixel in a cubemap or if the fallback reflection of the sky light component is reached. The probe's ReflectionProbeMode by default is set to static, in which case the captured cubemap reflection is only updated once at the start or whenever the settings change. Alternatively it can be set to dynamic, in which case the probe is updated continuously. Component Properties Sphere reflection component: Radius: The influence range of the probe. The reflection is only visible on geometry intersecting this radius. Falloff: Percentage of the radius that is smoothly blended into other probes. SphereProjection: Enables paralax correction to project the reflection onto the shape of the sphere. Box reflection component: Extents: The extents of the box projection. The cubemap will be projected to this box. InfluenceScale, InfluenceShift: The influence volume can be smaller than the projected volume (extents) of the probe. This can be useful if you have e.g. a long corridor that you want to place multiple probes in. Each will have the same projection but a different part of the projection volume will be set as the influence volume centred around a probe capture offset. PositiveFalloff, NegativeFalloff: Percentage of the influence volume in each direction that is smoothly blended into other probes. The falloff is defined for each face of the box. BoxProjection: Enables paralax correction to project the reflection onto the shape of the box. Common properties for both sphere and box reflection probes that describe how the probe is captured: ReflectionProbeMode: Dynamic makes the skylight update continuously. Static will only update once at the start. IncludeTags, ExcludeTags: These tags define which objects in the scene are used to capture the scene. This is the same mechanism as used in the camera component. By default, the SkyLight tag is excluded to allow for dynamic composition with the sky light component. NearPlane, FarPlane: Camera settings used when the lighting is captured from the scene. If NearPlane is set to Auto, a value is computed automatically from the FarPlane. CaptureOffset: The capture offset allows for the capture position of the probe to be decoupled from the game object position. ShowDebugInfo: If enabled, a sphere with a preview of the probe cubemap is rendered at the position of the capture offset. Use this to check whether all desired objects contribute to the probe. Above the sphere will be a stack of other spheres that showcase the reflection with increased roughness. Best Practices Don't try to create mirrors with reflection probes Avoid mirrors or other perfectly reflective surfaces. Reflection probes are approximate and should not be used to emulate mirrors and other perfect reflectors. Even when using projection, the illusion of a perfect mirror immediately brakes down once objects are in the box-shaped room as the objects will be splatted to the box extends as seen here: Ensure box projections have roughly the same length on each side If a box projection has very different dimensions on each axis you will run into stretching artifacts at the long ends of the box as can be seen here: Instead of just one box probe, use multiple boxes to span long corridors to prevent this. To do this, take the existing box that spans the entire area and do the following: Change it's Influence scale to e.g. 0.4 on the long axis, X in this case. Duplicate the object at the same positions. Set the duplicate's influence shift of the X-axis to 1. Duplicate the object in-place again. Set the new duplicate's influence shift of the X-axis to -1. This should result in the following image. The first box is highlighted to show the yellow influence box. Don't align box probes perfectly with walls When using BoxProjection, it is best to not try to align a box's extent perfectly with a room. Projections are intended to anchor reflections, not as a mirror substitute. In the following image you can see some of the pitfalls when trying to do so: While the floor reflection now fits perfectly, there is no reflection on the wall mirror. This is because there is a door to another room and to avoid hard interpolation discontinuties between reflection probes, the falloff in that direction needs to be set. As the falloff moves inwards the mirror on the wall basically gets no contribution of the room's probe anymore. On the left wall this problem can be avoided by setting the falloff to zero but that is generally not an option in most cases. Instead, increase the extents and set a falloff in each direction to fade into the neighboring reflection probes. Once the floor material is replaced with a more reasonable one that is not a perfect mirror the missalignment will no longer be aparent while still providing the desired parallax effect to the reflections. See Also Lighting Ambient Light Component Sky Light Component"
  },
  "pages/docs/graphics/lighting/sky-light-component.html": {
    "href": "pages/docs/graphics/lighting/sky-light-component.html",
    "title": "Skylight Component | ezEngine",
    "keywords": "Skylight Component The skylight component illuminates the entire scene, similar to the ambient light component. There are three main differences: Objects are illuminated using 6 different colors, one for each main direction (a so called 'ambient cube'). The ambient light component uses only 2 colors (top and bottom). The colors are dynamically computed from the scene. If the color of your sky changes, the ambient lighting of objects in the scene will reflect this. A global fallback reflection probe is generated. This is used for reflections if no local reflection probe components are present. The image above shows some objects lit only with the skylight component. Here the scene uses a skybox with blue sky, which is why the objects appear with a slightly blue tint. Skylight vs. Ambient Light Component The ambient light component uses a fixed color for lighting objects. Although you could animate those colors over time, for instance using a property animation (TODO), it is not possible to be directional. Using the skylight component you can have a bit of directional lighting. In the image below the objects on the left are lit with an ambient light component. Note that the lighting is very flat. The objects on the right are lit with a skylight component. To demonstrate how it illuminates objects directionally, the skybox is set to have red, green and blue faces. The ambient light component also does not generate a fallback reflection probe. So you are completely reliant on reflection probe components for reflections. Scene Setup The skylight component continuously makes a 360 degree screenshot of the scene to capture the overall lighting. However, typically you don't want to capture the entire scene, but only very few elements. Most notably, you want to capture the background sky, e.g. the skybox. You may also want to capture the ground. Finally, if you have distant background geometry, like a city backdrop or mountains, which the player can never reach, you may want to include those in your skylight snapshot as well, especially when that geometry can affect the visibility and thus brightness of the sky. Therefore, the skylight component requires you to select those few objects and tag them, such that the update of the skylight only includes those objects. By default the IncludeTags property is already set to SkyLight, which means that only objects with this tag will be used for computing the overall lighting. Consequently, you have to select objects, like your skybox, and assign that tag to them, otherwise the skylight will stay black. The skylight ReflectionProbeMode can also be set to static. In this mode, it will either just do one scene capture at the start or, if CubeMap is set, just compute the ambient light and reflection from the given cubemap. The image below shows the ShowDebugInfo mode. Here the skylight component visualizes the geometry that is used to compute the skylight. In this case the skybox, the floor and the red object were all tagged with SkyLight, and therefore appear in the preview. The green box though, was not tagged and therefore does not affect the result. Important: When you insert the skylight into the scene, it will override existing ambient lighting, and your scene may turn black. That's because no object in the scene is yet contributing to the skylight. You need to add the proper tag on your sky to get the desired illumination. Component Properties ReflectionProbeMode: Dynamic makes the skylight update continuously. Static will only update once at the start or generate the lighting from a cubemap asset, if the CubeMap property is set. CubeMap: Select a static cubemap asset as the source of the lighting instead of capturing the scene. Only available if ReflectionProbeMode is set to Static. Intensity: This allows you to adjust the intensity of the applied ambient light. Saturation: With a saturation of 1, the color of the sky is applied exactly as it is to the scene. Often this would result in too colorful lighting, for example a strong blue hue. By reducing saturation, the light will become more monochrome. In the image at the top, saturation was set to 0.4 to reduce the blue tint from the sky. IncludeTags, ExcludeTags: These tags define which objects in the scene are used to compute the skylight. Make sure that the object that renders your sky has this include tag set. This is the same mechanism as used in the camera component. NearPlane, FarPlane: Camera settings used when the lighting is captured from the scene. If NearPlane is set to Auto, a value is computed automatically from the FarPlane. ShowDebugInfo: If enabled, a sphere with a preview of the sky image is rendered at the position of the skylight object. Use this to check whether all desired objects contribute to the skylight. Above the sphere will be a stack of other spheres that showcase the reflection with increased roughness. See Also Lighting Ambient Light Component Reflection Probe Components"
  },
  "pages/docs/graphics/lighting/spot-light-component.html": {
    "href": "pages/docs/graphics/lighting/spot-light-component.html",
    "title": "Spot Light Component | ezEngine",
    "keywords": "Spot Light Component The spot light component adds a dynamic light source that illuminates the scene within a cone. The cone's inner angle determines the area that is illuminated equally bright. Between the inner angle and outer angle the light will fade to black. Spot lights should be preferred over point lights when this can prevent the need for dynamic shadows. For example, a light mounted to a ceiling should illuminate the area below it, but not shine through the ceiling and illuminate objects above. A spot light with a large cone can achieve this. Each light source costs performance. Try to reduce the number of light sources that illuminate the same area. Use the light count render mode to find hotspots. Note that unless a light source casts dynamic shadows, it's light will shine through walls. Casting shadows is very costly, though. When it is possible to avoid casting shadows, for example by using multiple smaller lights instead, the performance should always be better. Video: How to create a light source Component Properties See this page for shadow related component properties. LightColor, Intensity: The color and brightness of the light. Range: The distance over which the light source affects geometry. By default this is set to Auto, meaning the necessary range will be computed from the light's brightness. For full control, the range can be set manually. The light will always attenuate to zero within the given range, so by specifying a small range you can create a small, yet very bright light. InnerSpotAngle: The inner angle of the spot light's cone. Within this angle the spot light will not attenuate (except by distance) and stay equally bright. OuterSpotAngle: The spot light will attenuate between the inner angle and the outer angle to zero. If the outer angler is very close to the inner angle, the spot light will have a very sharp cut off. If the outer angle is considerably larger than the inner angle, the spot light will smoothly fade to black at the edges. See Also Lighting Point Light Component"
  },
  "pages/docs/graphics/meshes/custom-mesh-component.html": {
    "href": "pages/docs/graphics/meshes/custom-mesh-component.html",
    "title": "Custom Mesh Component | ezEngine",
    "keywords": "Custom Mesh Component The ezCustomMeshComponent can be used in place of a mesh component, but rather than loading a mesh asset, the mesh geometry is provided through (C++) code dynamically at runtime. This component is useful when you need to build geometry dynamically, for example to create visualizers that show what a player can do at a specific point in your game world. These things may not be possible to build up from fixed pieces, and therefore need custom geometry. Mesh Buffer Resource Each custom mesh component references a ezDynamicMeshBufferResource. Either its very own one, or a shared resource. At runtime, you would modify the geometry in that resource, and the ezCustomMeshComponent takes care of rendering it. Instancing If you want to render multiple instances of the same geometry, create multiple ezCustomMeshComponents and set all of them to reference the same mesh buffer resource. Materials Each component only uses a single material for rendering, but if you want to use multiple materials to render different pieces of the geometry, you can use multiple custom mesh components, and have each one render a different part of the geometry by giving it a limited primitive range. Vertex Colors Custom mesh components use vertex colors, meaning every vertex stores its own color information. This is different to typical mesh data in EZ, where per-vertex colors are usually not used. If you do not require vertex colors, this is fine and you can use any default material (and thus shader). The extra information will simply be ignored. However, if you do need the vertex color information, for instance to precisely control transparency, then you also need to set a material, which uses a shader that actually reads the vertex color value and uses it in the way that you desire. You may need to write a custom shader for that. See Also Meshes Shaders"
  },
  "pages/docs/graphics/meshes/instanced-mesh-component.html": {
    "href": "pages/docs/graphics/meshes/instanced-mesh-component.html",
    "title": "Instanced Mesh Component | ezEngine",
    "keywords": "Instanced Mesh Component The instanced mesh component is used to place multiple instances of the same mesh in a scene, with only a single game object. This component is meant for rare special cases where you want to render many mesh instances, but remove the performance overhead of having a dedicated game object for each one. Important: Using the instanced mesh component does not mean, that only this enables the use of instancing on the GPU. The renderer already uses instancing, even for regular mesh components. So you don't need to fear that using non-instanced mesh components would be inefficient for rendering. Editing Instances The instanced mesh component has an array of items which hold the transform and color for each mesh instance: You add instances by extending the array. Click the blue properties of an entry, to make a gizmo appear with which you can transform that instance (see image at the top). This workflow is currently not particularly enjoyable. You can't select individual instances through the viewport and you can't use the standard gizmos to edit an instance. When and How to Use the Instanced Mesh Component This component is not meant to be used manually. For everything that you edit by hand, the regular mesh component is efficient enough and much more convenient to use. This component has been created to be used by runtime code that procedurally places meshes and may create hundreds or thousands of instances of the same mesh. In such a situation it does make a performance difference, whether the engine has to handle a single game object with a single instanced mesh component, or thousands of game objects with equally many regular mesh components. Consequently, if you write custom code that places meshes procedurally, you may utilize this component. Be aware though that the instanced mesh component acts as a single object in regards to frustum culling, meaning either all instances are visible or none, but nothing in between. So you should group instances by proximity and use multiple instanced mesh components, if the instances span a large area. Component Properties Mesh: The mesh asset to render. MainColor: This color will be combined with the color of each individual mesh instance. Materials: An array of materials to override the default material. Works the same way as for regular mesh components. InstanceData: An array of instances. Each instance has its own transform and color. Click the blue transform properties to activate a manipulator for that instance. See Also Meshes"
  },
  "pages/docs/graphics/meshes/mesh-asset.html": {
    "href": "pages/docs/graphics/meshes/mesh-asset.html",
    "title": "Mesh Asset | ezEngine",
    "keywords": "Mesh Asset A mesh asset represents a mesh that can be used for rendering. In the most common case the mesh asset imports the mesh data from an external file, such as an FBX file. However, it also supports generating the mesh data for common shapes (spheres, cylinders, ...) procedurally. Mesh assets are typically added to a scene with a mesh component. The left hand side of the asset document shows a 3D preview of the mesh. The viewport allows to switch the render mode to inspect the mesh normals, UV coordinates and so on. On the right hand side the asset properties specify how to import or generate the mesh data. Important: The mesh asset does not automatically update when you edit its properties. Instead you need to transform the asset (Ctrl+E or with the rightmost button in the toolbar). Video: How to import meshes Asset Properties PrimitiveType: This selects how the mesh data is generated. If From File is chosen, you need to also specify the MeshFile property. If you choose a procedural method, other configuration options appear. ForwardDir, RightDir, UpDir: With these you can change which axis is considered forward, right and up in the mesh data. For mesh data from FBX files, this information is typically embedded in the file. For other file types, you may need to adjust these to make the imported data appear correctly upright. UniformScaling: Adjusts the size of the mesh, for example to convert a mesh from centimeter to meter scale. RecalculateNormals, RecalculateTangents: If enabled, information about normals or tangents in the mesh file is ignored, and is instead computed from the vertex data. NormalPrecision, TexCoordPrecision: These options allow you to choose how precise normals and UV coordinates are represented. Leave these at the default, unless you notice precision issues. Higher precision means the mesh takes up more RAM on the GPU and is slightly slower to render. ImportMaterials: If enabled, the mesh import automatically generates material assets for the materials that the mesh file specifies. It also tries to populate those materials with sensible values and if possible also creates texture assets. Unfortunately this rarely works perfectly, and typically requires you to fix the generated assets afterwards. Note: Materials are only generated when the mesh has no materials set yet. After the initial creation of these other assets, you usually need to transform the mesh a second time to make them properly show up. Materials: The list of materials to use. The mesh may have multiple sub-meshes, and each sub-mesh uses a different material slot. Mesh components can override which material is used for which slot. Procedural Mesh Generation Through the PrimitiveType option you can choose to create a mesh procedurally. In this case object specific options appear. Note that by default objects use a detail level of 0 which means that the editor will pick a decent value, depending on the chosen primitive type. Be aware that some detail values seemingly have no effect. For instance, for cones, capsules and cylinders the detail represents the number of subdivisions along the circumference, and therefore can't be lower than 3. Therefore the values 1, 2 and 3 all produce the same result. See Also Meshes Materials Assets Asset Import"
  },
  "pages/docs/graphics/meshes/mesh-component.html": {
    "href": "pages/docs/graphics/meshes/mesh-component.html",
    "title": "Mesh Component | ezEngine",
    "keywords": "Mesh Component A mesh component is used to instantiate a mesh asset. Mesh components are purely visual, they have no physical interaction, so other physical objects cannot collide with them. To add physical interaction capabilities, an object has to have an additional collision shape and a static or dynamic physics actor. Mesh components will cast shadows when the CastShadow tag is set on the owner game object. The referenced mesh is rendered according to the used materials, which determine lighting and other visual effects. Component Properties Mesh: The mesh asset to render. Color: A tint color for the mesh instance. Typically this is just multiplied into the diffuse color of the mesh materials, though if the material uses a visual shader (TODO), the mesh color can be used to represent arbitrary input data, for example to blend between material states. Materials: By default the referenced mesh is rendered with the materials that are set up inside the mesh asset. However, the mesh component can override the materials. Each mesh has one or many sub-meshes, meaning mesh parts that use different materials. This array allows to set an override for each of those sub-meshes. See Also Meshes Materials Lighting"
  },
  "pages/docs/graphics/meshes/meshes-overview.html": {
    "href": "pages/docs/graphics/meshes/meshes-overview.html",
    "title": "Meshes | ezEngine",
    "keywords": "Meshes Meshes are the central feature of any 3D engine. Meshes can be separated into two kinds: the ones used for rendering, and the ones used for interactions (physics). Mesh data is either imported from external files, such as FBX files, or procedurally generated. Generating meshes procedurally is mostly useful for very basic shapes either for special cases or as placeholders during early development. Graphical meshes are handled by the mesh asset. Meshes that are used in physics simulations are called collision meshes. Once a mesh is imported as an asset, it can be placed in a scene as often as you like. For the most common use case you would use a mesh component to do so, but there are other components for special cases, such as the instanced mesh component. Meshes may also be used by other things, for example as a type of particle. To instantiate collision meshes, you need to use the proper shape component. Graphical meshes reference materials which define how the mesh gets rendered. Collision meshes may use surfaces to set their physical properties. See Also Materials Surfaces"
  },
  "pages/docs/graphics/occluder-component.html": {
    "href": "pages/docs/graphics/occluder-component.html",
    "title": "Occluder Component | ezEngine",
    "keywords": "Occluder Component The occluder component is used to add invisible geometry to a scene that is only used for occlusion culling. Currently the occluder component always uses a box shape. Contrary to greybox geometry, the occluder component itself is invisible. Enable occluder geometry visualization to see it in action. Occluders can be moved around dynamically, so you can attach it to a door and it will properly occlude objects when the door closes. You can also (de-)activate the entire component programmatically. For example a breakable wall can use an occluder as long as it is intact, and deactivate it when the wall breaks. Component Properties Extents: The size of the box. See Also Occlusion Culling Greyboxing"
  },
  "pages/docs/graphics/render-pipeline-overview.html": {
    "href": "pages/docs/graphics/render-pipeline-overview.html",
    "title": "Render Pipeline | ezEngine",
    "keywords": "Render Pipeline Render pipelines are fully functional, but currently undocumented. See Also Shaders"
  },
  "pages/docs/graphics/render-to-texture/render-target-activator-component.html": {
    "href": "pages/docs/graphics/render-to-texture/render-target-activator-component.html",
    "title": "Render Target Activator Component | ezEngine",
    "keywords": "Render Target Activator Component Render to texture functionality is working, but needs a rewrite, which is why this feature is currently undocumented. See Also Render to Texture (TODO)"
  },
  "pages/docs/graphics/render-to-texture/render-to-texture.html": {
    "href": "pages/docs/graphics/render-to-texture/render-to-texture.html",
    "title": "Render to Texture | ezEngine",
    "keywords": "Render to Texture Render to texture functionality is working, but needs a rewrite, which is why this feature is currently undocumented. See Also Render Target Activator Component (TODO) Render Pipeline (TODO)"
  },
  "pages/docs/graphics/shaders/shader-debugging.html": {
    "href": "pages/docs/graphics/shaders/shader-debugging.html",
    "title": "Shader Debugging | ezEngine",
    "keywords": "Shader Debugging To debug a shader, one can configure it such that the shader compiler includes debugging information. To do so, include DEBUG as a platform in the [PLATFORMS] section of the shader: [PLATFORMS] ALL DEBUG [PERMUTATIONS] ALPHATEST WIREFRAME [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif [VERTEXSHADER] VS_OUT main(VS_IN Input) { ... } [PIXELSHADER] ... See Also Shaders Shader Templates ShaderCompiler"
  },
  "pages/docs/graphics/shaders/shader-permutation-variables.html": {
    "href": "pages/docs/graphics/shaders/shader-permutation-variables.html",
    "title": "Shader Permutation Variables | ezEngine",
    "keywords": "Shader Permutation Variables Permutation variables are global variables set either through C++ code or exposed through materials. The value of a permutation variable at the time of a drawcall affects which permutation of a shader is used for rendering. Permutation variables allow to create different variants of the same shader, without creating different shader files. Since their state is global, they decouple the decision which shader to use from the code that actually has this information at hand. For instance, materials support different rendering modes. By default they use proper lighting, but for debugging purposes we might want to override this and always output unlit diffuse color (or normals, UV coordinates, etc). The information which shader to use to render a certain object is stored either in a material or directly set through code. Without permutation variables we would either need to use an entirely different shader to get our debug output, which would mean that everything would need to support this functionality, or the shader would need to decide the final output mode dynamically, adding a large performance hit for a feature that is not used in the final game. Permutation variables solve this problem by creating different variants of the shader, and letting the engine pick the correct one depending on the current values. In shader code, permutation variables are exposed as #define'd preprocessor variables and therefore can be evaluated like any other preprocessor directive. The Shader Permutations Section Each shader is made up of several sections: [PLATFORMS] ALL DEBUG [PERMUTATIONS] ALPHATEST WIREFRAME [MATERIALPARAMETER] Permutation ALPHATEST; [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif [VERTEXSHADER] VS_OUT main(VS_IN Input) { ... } [PIXELSHADER] ... In the [PERMUTATIONS] section the shader author has to list all permutation variables that are going to be evaluated inside the shader code. If a variable is used without being mentioned in this section, your shader might compile and work, but the result will always be the same. ezPermVar Files Every permutation variable must be defined in a file that has the exact name of the permutation variable and the .ezPermVar extension. All ezPermVar files must reside in a specific subfolder in any data directory. By default the subfolder is \"Shaders/PermutationVars\". bool Permutation variables The definition of a boolean permutation variable in its ezPermVar file simply looks like this: bool TWO_SIDED; A boolean permutation variable is permuted over the values TRUE and FALSE. In a shader it would be evaluate like this: #if defined(PIXEL_SHADER) && TWO_SIDED == TRUE uint FrontFace : SV_IsFrontFace; #endif In C++ code the variable is set like this: ezRenderContext::SetShaderPermutationVariable(\"TWO_SIDED\", \"TRUE\"); enum Permutation Variables Enum permutation variables allow you to use more than two permutation values and they can have more descriptive names. The definition of an enum variable in its ezPermVar file looks like this: enum BLEND_MODE { OPAQUE, MASKED, TRANSPARENT, ADDITIVE, MODULATE }; Note: When evaluating an enum variable in a shader, the value must be prefixed with the name of the variable and an underscore: #if BLEND_MODE == BLEND_MODE_MASKED return opacity - MaskThreshold; #else return opacity; #endif As you can see, the name used for comparison is BLEND_MODE_MASKED although in the definition it was named MASKED. In C++ code we use the actual name though: ezRenderContext::SetShaderPermutationVariable(\"BLEND_MODE\", \"MASKED\"); Exposing Permutations to materials By default permutation variables do not show up in materials and therefore cannot be manually specified by artists. If you want a variable to show up, simply list it in the [MATERIALPARAMETER] section: [MATERIALPARAMETER] Permutation ALPHATEST; The type (bool or enum) and the available values are automatically read from the ezPermVar file that defines the variable and will show up in the material properties accordingly. See Also Shaders Shader Templates Shader Debugging The Shader Render State Section"
  },
  "pages/docs/graphics/shaders/shader-render-state.html": {
    "href": "pages/docs/graphics/shaders/shader-render-state.html",
    "title": "Shader Render State | ezEngine",
    "keywords": "Shader Render State The state of the rendering pipeline (TODO) can only be set through shaders. There is no way to change its state other than to select a shader which includes that specific state. Use shader permutations to create variants of a shader. Each variant may incorporate a different render state. By setting shader permutation variables at runtime, you select the specific shader variant (permutation) and thus also get its render state. This design follows what rendering APIs such as DirectX 12 and Vulkan require. The Shader Render State Section Each shader is made up of several sections: [PLATFORMS] ALL DEBUG [PERMUTATIONS] ALPHATEST WIREFRAME [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif [VERTEXSHADER] VS_OUT main(VS_IN Input) { ... } [PIXELSHADER] ... The render pipeline state associated with the shader is defined in the [RENDERSTATE] section. It may use permutation variables just like the shader code. To have different state for different permutations, use standard C preprocessor syntax. Render States The following variables are available in the [RENDERSTATE] section. Simply overwrite them with the desired value. Rasterizer States bool DepthClip = false bool FrontCounterClockwise = false bool LineAA = false bool MSAA = false bool ScissorTest = false bool WireFrame = false enum CullMode = CullMode_Back CullMode = CullMode_None CullMode = CullMode_Back CullMode = CullMode_Front float DepthBiasClamp = 0.0 float SlopeScaledDepthBias = 0.0 int DepthBias = 0 Depth-Stencil State enum BackFaceDepthFailOp = StencilOp_Keep BackFaceDepthFailOp = StencilOp_Keep BackFaceDepthFailOp = StencilOp_Zero BackFaceDepthFailOp = StencilOp_Replace BackFaceDepthFailOp = StencilOp_IncrementSaturated BackFaceDepthFailOp = StencilOp_DecrementSaturated BackFaceDepthFailOp = StencilOp_Invert BackFaceDepthFailOp = StencilOp_Increment BackFaceDepthFailOp = StencilOp_Decrement enum BackFaceFailOp = StencilOp_Keep BackFaceFailOp = StencilOp_Keep BackFaceFailOp = StencilOp_Zero BackFaceFailOp = StencilOp_Replace BackFaceFailOp = StencilOp_IncrementSaturated BackFaceFailOp = StencilOp_DecrementSaturated BackFaceFailOp = StencilOp_Invert BackFaceFailOp = StencilOp_Increment BackFaceFailOp = StencilOp_Decrement enum BackFacePassOp = StencilOp_Keep BackFacePassOp = StencilOp_Keep BackFacePassOp = StencilOp_Zero BackFacePassOp = StencilOp_Replace BackFacePassOp = StencilOp_IncrementSaturated BackFacePassOp = StencilOp_DecrementSaturated BackFacePassOp = StencilOp_Invert BackFacePassOp = StencilOp_Increment BackFacePassOp = StencilOp_Decrement enum BackFaceStencilFunc = CompareFunc_Always BackFaceStencilFunc = CompareFunc_Never BackFaceStencilFunc = CompareFunc_Less BackFaceStencilFunc = CompareFunc_Equal BackFaceStencilFunc = CompareFunc_LessEqual BackFaceStencilFunc = CompareFunc_Greater BackFaceStencilFunc = CompareFunc_NotEqual BackFaceStencilFunc = CompareFunc_GreaterEqual BackFaceStencilFunc = CompareFunc_Always enum FrontFaceDepthFailOp = StencilOp_Keep FrontFaceDepthFailOp = CompareFunc_Never FrontFaceDepthFailOp = CompareFunc_Less FrontFaceDepthFailOp = CompareFunc_Equal FrontFaceDepthFailOp = CompareFunc_LessEqual FrontFaceDepthFailOp = CompareFunc_Greater FrontFaceDepthFailOp = CompareFunc_NotEqual FrontFaceDepthFailOp = CompareFunc_GreaterEqual FrontFaceDepthFailOp = CompareFunc_Always enum FrontFaceFailOp = StencilOp_Keep FrontFaceFailOp = CompareFunc_Never FrontFaceFailOp = CompareFunc_Less FrontFaceFailOp = CompareFunc_Equal FrontFaceFailOp = CompareFunc_LessEqual FrontFaceFailOp = CompareFunc_Greater FrontFaceFailOp = CompareFunc_NotEqual FrontFaceFailOp = CompareFunc_GreaterEqual FrontFaceFailOp = CompareFunc_Always enum FrontFacePassOp = StencilOp_Keep FrontFacePassOp = CompareFunc_Never FrontFacePassOp = CompareFunc_Less FrontFacePassOp = CompareFunc_Equal FrontFacePassOp = CompareFunc_LessEqual FrontFacePassOp = CompareFunc_Greater FrontFacePassOp = CompareFunc_NotEqual FrontFacePassOp = CompareFunc_GreaterEqual FrontFacePassOp = CompareFunc_Always enum FrontFaceStencilFunc = CompareFunc_Always FrontFaceStencilFunc = CompareFunc_Never FrontFaceStencilFunc = CompareFunc_Less FrontFaceStencilFunc = CompareFunc_Equal FrontFaceStencilFunc = CompareFunc_LessEqual FrontFaceStencilFunc = CompareFunc_Greater FrontFaceStencilFunc = CompareFunc_NotEqual FrontFaceStencilFunc = CompareFunc_GreaterEqual FrontFaceStencilFunc = CompareFunc_Always bool DepthTest = true bool DepthWrite = true bool SeparateFrontAndBack = false bool StencilTest = false enum DepthTestFunc = CompareFunc_Less DepthTestFunc = CompareFunc_Never DepthTestFunc = CompareFunc_Less DepthTestFunc = CompareFunc_Equal DepthTestFunc = CompareFunc_LessEqual DepthTestFunc = CompareFunc_Greater DepthTestFunc = CompareFunc_NotEqual DepthTestFunc = CompareFunc_GreaterEqual DepthTestFunc = CompareFunc_Always int StencilReadMask = 255 int StencilWriteMask = 255 Blend State bool AlphaToCoverage = false bool IndependentBlend = false The following variables exist with suffix 0 to 7. If IndependentBlend is disabled, only the ones with suffix 0 are used. bool BlendingEnabled0 = false enum BlendOp0 = BlendOp_Add BlendOp0 = BlendOp_Add BlendOp0 = BlendOp_Subtract BlendOp0 = BlendOp_RevSubtract BlendOp0 = BlendOp_Min BlendOp0 = BlendOp_Max enum BlendOpAlpha0 = BlendOp_Add BlendOpAlpha0 = BlendOp_Add BlendOpAlpha0 = BlendOp_Subtract BlendOpAlpha0 = BlendOp_RevSubtract BlendOpAlpha0 = BlendOp_Min BlendOpAlpha0 = BlendOp_Max enum DestBlend0 = Blend_One DestBlend0 = Blend_Zero DestBlend0 = Blend_One DestBlend0 = Blend_SrcColor DestBlend0 = Blend_InvSrcColor DestBlend0 = Blend_SrcAlpha DestBlend0 = Blend_InvSrcAlpha DestBlend0 = Blend_DestAlpha DestBlend0 = Blend_InvDestAlpha DestBlend0 = Blend_DestColor DestBlend0 = Blend_InvDestColor DestBlend0 = Blend_SrcAlphaSaturated DestBlend0 = Blend_BlendFactor DestBlend0 = Blend_InvBlendFactor enum DestBlendAlpha0 = Blend_One DestBlendAlpha0 = Blend_Zero DestBlendAlpha0 = Blend_One DestBlendAlpha0 = Blend_SrcColor DestBlendAlpha0 = Blend_InvSrcColor DestBlendAlpha0 = Blend_SrcAlpha DestBlendAlpha0 = Blend_InvSrcAlpha DestBlendAlpha0 = Blend_DestAlpha DestBlendAlpha0 = Blend_InvDestAlpha DestBlendAlpha0 = Blend_DestColor DestBlendAlpha0 = Blend_InvDestColor DestBlendAlpha0 = Blend_SrcAlphaSaturated DestBlendAlpha0 = Blend_BlendFactor DestBlendAlpha0 = Blend_InvBlendFactor enum SourceBlend0 = Blend_One SourceBlend0 = Blend_Zero SourceBlend0 = Blend_One SourceBlend0 = Blend_SrcColor SourceBlend0 = Blend_InvSrcColor SourceBlend0 = Blend_SrcAlpha SourceBlend0 = Blend_InvSrcAlpha SourceBlend0 = Blend_DestAlpha SourceBlend0 = Blend_InvDestAlpha SourceBlend0 = Blend_DestColor SourceBlend0 = Blend_InvDestColor SourceBlend0 = Blend_SrcAlphaSaturated SourceBlend0 = Blend_BlendFactor SourceBlend0 = Blend_InvBlendFactor enum SourceBlendAlpha0 = Blend_One SourceBlendAlpha0 = Blend_Zero SourceBlendAlpha0 = Blend_One SourceBlendAlpha0 = Blend_SrcColor SourceBlendAlpha0 = Blend_InvSrcColor SourceBlendAlpha0 = Blend_SrcAlpha SourceBlendAlpha0 = Blend_InvSrcAlpha SourceBlendAlpha0 = Blend_DestAlpha SourceBlendAlpha0 = Blend_InvDestAlpha SourceBlendAlpha0 = Blend_DestColor SourceBlendAlpha0 = Blend_InvDestColor SourceBlendAlpha0 = Blend_SrcAlphaSaturated SourceBlendAlpha0 = Blend_BlendFactor SourceBlendAlpha0 = Blend_InvBlendFactor int WriteMask = 255 See Also Shaders Shader Permutation Variables Shader Templates Render Pipeline (TODO)"
  },
  "pages/docs/graphics/shaders/shader-resources.html": {
    "href": "pages/docs/graphics/shaders/shader-resources.html",
    "title": "Shaders Resources | ezEngine",
    "keywords": "Shaders Resources Shader resources are things like textures, samplers constant buffers etc. that need to be separately bound in the renderer for the shader to function. Each resource must be bound to a set and slot. Depending on the platform used, the requirements for this binding can be very different. E.g. in Vulkan slot assignments must be unique within a set across all stages while in DX11 most slots only need to be unique within a stage. Not following these rules will result in a runtime error. Manually assigning slots is an option but is very tedious. To make this easier, the shader system can automate this process provided some constraints are met how resourced are declared. Currently, EZ does not support arrays of resources like Texture2D Diffuse[3] in its shaders. Resources must have unique names across all shader stages. The same resource name can be used in multiple stages as long as the resource it maps to is exactly the same. Resource Binding The shader system only supports the DX11 / DX12 register syntax for resource binding. Both the set and slot can be bound. If no set is given, it is implicitly set 0. Here is a list of a few examples of how to bind resources properly: Texture2D Diffuse : register(t3, space1); // DX12 syntax, slot 3, set 1 SamplerState MySampler : register(s4); // DX11 syntax slot 4, set 0 (default) ByteAddressBuffer MyBuffer BIND_RESOURCE(SLOT_AUTO, SET_RENDER_PASS); // Slot Auto, set 1 ByteAddressBuffer MyBuffer2 BIND_SET(SET_RENDER_PASS); // Slot Auto, set 1 CONSTANT_BUFFER(ezTestPositions, 1) // Slot 1, set 0 (default) { ... }; CONSTANT_BUFFER2(ezTestPositions, SLOT_AUTO, SET_MATERIAL) // Slot Auto, set 2 { ... }; The HLSL register syntax is a bit impractical, so the macros BIND_RESOURCE(Slot, Set) and BIND_SET(Set) were introduced. These will generate invalid HLSL code which the shader compiler will eventually parse, organize and patch to do the correct thing on each platform. In most cases, you should only be concerned about deciding in which set a resource should reside in. Either use the macro SLOT_AUTO when setting a slot or just use the BIND_SET macro which omits the slot entirely. While you can set any integer for the set, some platforms like Vulkan have a limit on how many sets can be managed at the same time with a minimum of four. EZ defines macros for these four sets: SET_FRAME, SET_RENDER_PASS, SET_MATERIAL and SET_DRAW_CALL. Resources should ideally be bound to these sets according to their update frequency. Constant Buffers Constant buffers map to ezGALShaderResourceType::ConstantBuffer in C++. To facilitate C++ interop, constant buffers should be placed into a separate header file that looks like this: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> CONSTANT_BUFFER(ezTestPositions, 3) { FLOAT4(Vertex0); FLOAT4(Vertex1); FLOAT4(Vertex2); }; By using the macros defined in ConstantBufferMacros.h like CONSTANT_BUFFER and the data types like FLOAT4, the file can be included in both shader and C++ code. This makes it easy to create an instance of the constant buffer as a C++ struct in code to update it. Care must be taken to ensure that the constant buffer has the same layout in C++ and HLSL though: Make sure that the size of your struct is a multiple of 16 bytes. Fill out any missing bytes with dummy FLOAT1 entries. A FLOAT3 can't be followed by another FLOAT3. It should be followed by a FLOAT1 first or some other types of the same byte counts to ensure the next FLOAT3 starts at a 16 byte boundary. This is necessary as the layout rules are different between HLSL and C++. Push Constants Push constants map to ezGALShaderResourceType::PushConstants in C++. Push constants allow for fast updates of a small set of bytes. Usually at least 128 bytes. You can check ezGALDeviceCapabilities::m_uiMaxPushConstantsSize for the max push constant buffer size. On platforms that don't support push constants like DX11, this is emulated via a constant buffer. Only one push constants block is supported across all shader stages of a shader. Like with constant buffers, special macros have to be used and the declaration should be put into a separate header so it can be included in both shader and C++ code: // Header: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> BEGIN_PUSH_CONSTANTS(ezTestData) { FLOAT4(VertexColor); FLOAT4(Vertex0); FLOAT4(Vertex1); FLOAT4(Vertex2); } END_PUSH_CONSTANTS(ezTestData) // Shader: float4 main(VS_OUT a) : SV_Target { return GET_PUSH_CONSTANT(ezTestData, VertexColor); } // C++: ezTestData constants; constants.VertexColor = ...; pContext->SetPushConstants(\"ezTestData\", constants); The BEGIN_PUSH_CONSTANTS and END_PUSH_CONSTANTS macros define the struct. Unlike with constant buffers, you can't simply access the values inside a shader by just the name of the variable, e.g. VertexColor. This is because depending on the platform, a different syntax needs to be used to access the content. To make the same shader compile on all platforms, you need to use the GET_PUSH_CONSTANT(Name, Constant) macro to access a member of the push constant buffer. Samplers Samplers map to ezGALShaderResourceType::Sampler or ezGALShaderResourceType::TextureAndSampler in C++. Two types of samplers are supported: SamplerState and SamplerComparisonState. The naming of the samplers is important, as it can be used to optimize your workflow. ezEngine has a concept of immutable Samplers, these samplers are automatically bound so you can use them in the shader without needing to define them in C++. Immutable samplers are registered in code via ezGALImmutableSamplers::RegisterImmutableSampler. Currently, these samplers are registered: LinearSampler, LinearClampSampler, PointSampler and PointClampSampler. ezEngine does not allow for two different resources to have the same name, the only exception is textures and samplers which can have the same name by calling the sampler NAME_AutoSampler. The compiler will rename the sampler to NAME and on platforms that support combined image samplers both will be combined into a single resource of type ezGALShaderResourceType::TextureAndSampler. The benefit of this approach is that when binding a texture resource to a material for example, the texture resource can define both the texture as well as the sampler state, binding both to the same name. SamplerState DiffuseSampler; SamplerComparisonState ShadowSampler; // Auto sampler combines with texture of the same name: Texture2D BaseTexture; SamplerState BaseTexture_AutoSampler; Textures Textures map to ezGALShaderResourceType::Texture or ezGALShaderResourceType::TextureAndSampler in C++ (see samplers above). ezEngine supports all HLSL texture types except for 1D textures. You can work around this by creating 1xN 2DTextures. Texture1D texture1D; // 1D textures currently not supported. Texture1DArray texture1DArray; // 1D textures currently not supported. Texture2D texture2D; Texture2DArray texture2DArray; Texture2DMS<float4> texture2DMS; Texture2DMSArray<float4> texture2DMSArray; Texture3D texture3D; TextureCube textureCube; TextureCubeArray textureCubeArray; Read-write variants are also supported and map to ezGALShaderResourceType::TextureRW in C++. RWTexture1D<float> rwTexture1D; // 1D textures currently not supported. RWTexture1DArray<float2> rwTexture1DArray; // 1D textures currently not supported. RWTexture2D<float3> rwTexture2D; RWTexture2DArray<float4> rwTexture2DArray; RWTexture3D<uint> rwTexture3D; Buffers There are three types of buffers supported by EZ: HLSL's Buffer<T> type is very similar to a 1D texture. A buffer of the same type T needs to be bound to the resource. Maps to ezGALShaderResourceType::TexelBuffer in C++. StructuredBuffer<T> should follow the same rules as for constant buffers: Put the declaration in a separate header file to allow access to it from C++ and ensure each struct is 16 bytes aligned. Maps to ezGALShaderResourceType::StructuredBuffer in C++. ByteAddressBuffer in just an array of bytes. A raw buffer needs to be bound to the resource. With HLSL 5.1, you can cast any offset of the buffer into a struct. Maps to ezGALShaderResourceType::StructuredBuffer in C++. // Header: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> struct EZ_SHADER_STRUCT PerInstanceData { TRANSFORM(ObjectToWorld); }; // Shader: Buffer<uint> buffer; StructuredBuffer<PerInstanceData> structuredBuffer; ByteAddressBuffer byteAddressBuffer; Read-write variants of these buffers are also supported and map to ezGALShaderResourceType::TexelBufferRW and ezGALShaderResourceType::StructuredBufferRW respectively. RWBuffer<uint> rwBuffer; RWStructuredBuffer<ezPerInstanceData> rwStructuredBuffer; RWByteAddressBuffer rwByteAddressBuffer; Append / Consume Buffers TODO: Future work: Append / consume buffers can be defined in shaders and are correctly reflected, but EZ does not support binding resources to them right now. // Header: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> struct EZ_SHADER_STRUCT ezAppendData { FLOAT2(Value); }; // Shader: AppendStructuredBuffer<ezAppendData> appendStructuredBuffer; ConsumeStructuredBuffer<ezAppendData> consumeStructuredBuffer; See Also Shaders ShaderCompiler"
  },
  "pages/docs/graphics/shaders/shader-templates.html": {
    "href": "pages/docs/graphics/shaders/shader-templates.html",
    "title": "Shader Templates | ezEngine",
    "keywords": "Shader Templates Writing a custom shader from scratch is a rather difficult task, especially if you want a custom material shader, that has all the PBR lighting features. However, using shader templates this becomes quite straight forward. A shader template is simply a functioning shader file that you can edit. Shaders are generally written in HLSL, though EZ shader files have additional sections for configuring various other aspects. There are different templates available for common use cases. Creating a New Shader from a Template Open a material asset that should use a custom shader. As it's ShaderMode select From File. Now click the browse (...) button next to the Shader property, to open the context menu: From this menu select Create Shader From Template.... The following dialog is shown: Select the output file name and which template to create the shader from. When you select a template, the area below shows additional options. These simply select what code from the template should be included in your new file and is purely for convenience to get started more easily. You can add and remove any feature manually afterwards as needed. Once you click OK the shader file is created, applied to the material and opened in a text editor for you to start editing. When you make structural changes (exposed parameters, permutation variables used and such) you need to transform the material for the changes to properly show up. As long as you only modify the HLSL code, you can also just press F4 to reload resources at any time. If there are any errors, they are shown in the log. Adding a Shader Template If you want to add another shader template itself, have a look at the folder Data/Tools/ezEditor/ShaderTemplates. All you need to do is add another .ezShaderTemplate file here. Shader templates have an additional TEMPLATE_VARS section at the beginning that define what options to display in the dialog. The final shader code is built by running a C preprocessor over the file, but instead of replacing #if / #endif sections, it only looks at %if / %endif (using a percentage sign rather than a hash), to generate the output. See Also Shaders ShaderCompiler"
  },
  "pages/docs/graphics/shaders/shaders-overview.html": {
    "href": "pages/docs/graphics/shaders/shaders-overview.html",
    "title": "Shaders | ezEngine",
    "keywords": "Shaders Shaders are files with the .ezShader extension. These files not only provide the HLSL code for each shader stage used, but also the complete render state used when drawing with this shader. Several permutations of the same shader can exist. Permutations can inpact the render state or affect the HLSL source code. Thus, one shader file can produce several outputs. Shader Sections Each shader is made up of several sections. Not all sections need to be defined as most have a default state. Here is a very simple shader: [PLATFORMS] ALL [PERMUTATIONS] [RENDERSTATE] [SHADER] cbuffer PerObject : register(b1) { float4x4 mvp : packoffset(c0); }; struct VS_IN { float3 pos : POSITION; float2 texcoord0 : TEXCOORD0; }; struct VS_OUT { float4 pos : SV_Position; float2 texcoord0 : TEXCOORD0; }; typedef VS_OUT PS_IN; [VERTEXSHADER] VS_OUT main(VS_IN Input) { VS_OUT RetVal; RetVal.pos = mul(mvp, float4(Input.pos, 1.0)); RetVal.texcoord0 = Input.texcoord0; return RetVal; } [PIXELSHADER] Texture2D DiffuseTexture; SamplerState PointClampSampler; float4 main(PS_IN Input) : SV_Target { return DiffuseTexture.Sample(PointClampSampler, Input.texcoord0); } The following sections are supported: PLATFORMS The PLATFORMS section lists the shader platforms that are supported by this shader and for which the shader should be compiled. Currently, these values are supported: ALL: The shader is supported on all platforms. DEBUG: If set, the shader is not optimized and contains debug information to allow stepping through it in tools like RenderDoc. VULKAN: The shader will be compiled as SPIRV code for the Vulkan renderer. DX11_SM40_93: DX9 feature set. Used by the DX11 renderer when using downlevel hardware support. DX11_SM40: DX10 feature set. Used by the DX11 renderer when using downlevel hardware support. DX11_SM41: DX10.1 feature set. Used by the DX11 renderer when using downlevel hardware support. DX11_SM50: DX11 feature set. Default platform used by the DX11 renderer. [PLATFORMS] ALL DEBUG PERMUTATIONS The PERMUTATIONS section defines permutation variables which allow for modofication of the shader code. Each variable is exposed as a preprocessor variable, allowing for various sections of the shader to be modifed via preprocessor blocks. The values of these permutation variables are defined by the engine / material and the entire system is explained in detail in the dedicated shader permutation variables page. [PERMUTATIONS] ALPHATEST CAMERA_MODE = CAMERA_MODE_PERSPECTIVE MATERIALPARAMETER, MATERIALCONFIG If the shader is used as a material shader, two more sections are used: MATERIALPARAMETER: Used to define which permutation variables should be allowed to be changed on a material in the editor. This is explained in detail in exposing permutation variables to materials. MATERIALCONFIG: This section controls when a material is rendered during a frame. This is done by this line: RenderDataCategory = LitOpaque. You can use the preprocessor to change the value depending on some permutation variable, if neccessary. The valid values for RenderDataCategory are defined in code via ezRenderData::RegisterCategory. Commonly used values are LitOpaque for opaque materials, LitMasked for alpha-tested materials and LitTransparent for alpha blended materials. [MATERIALCONFIG] #if (BLEND_MODE == BLEND_MODE_OPAQUE) RenderDataCategory = LitOpaque #elif (BLEND_MODE == BLEND_MODE_MASKED) RenderDataCategory = LitMasked #else RenderDataCategory = LitTransparent #endif RENDERSTATE Each shader defines the complete state of the renderer. This includes, but is not limited to blendind, rasterizer, depth stencil etc. You can use permutations variables and preprcessor macros to change the render state of shader permutations. This is explained in more detail on the shader render state page. [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif SHADER The SHADER section contains code that is shared among all shader stages. The content is simply prepended to all used stages before compiling. *SHADER stages Each shader stage has its own section. The following stages are supported: VERTEXSHADER, HULLSHADER, DOMAINSHADER, GEOMETRYSHADER, PIXELSHADER and COMPUTESHADER. Even if you define a shader section, you can use the preprocessor to remove its content via permutation variables, allowing you to remove stages from certain permutations. The entry point into each stage must be called main. The shader code supports preprocessor macros that are defined by permutation variables as well as include directives. Beyond that, any HLSL code is fine as long as it compiles on the platforms the shader defines. However, when defining resources, special care must be taken to ensure no conflicting resource mappings are created between the stages. Please refer to the shader resource page for further details and on how to facilitate interop with the C++ code. TEMPLATE_VARS This section is only used when creating a shader template. See Also Shader Render State Shader Permutation Variables Shader Resources Shader Templates ShaderCompiler Render Pipeline (TODO)"
  },
  "pages/docs/graphics/sprite-component.html": {
    "href": "pages/docs/graphics/sprite-component.html",
    "title": "Sprite Component | ezEngine",
    "keywords": "Sprite Component The sprite component is used to render a textured quad that always faces the camera and whose on-screen size never exceeds a defined limit. Sprites are mainly used to place 3D icons or markers in the world. For example all the shape icons in the editor are sprites. However, they can also be used for simple effects, for instance to represent small projectiles. Although sprites have a world space size, their on-screen size is clamped to a maximum value. That means they won't fill up the screen when the camera comes very close. Component Properties Texture: The texture to use. BlendMode: How to blend the sprite with the background. Color: A tint color to multiply into the texture. Size: The actual size of the sprite in the world. Based on this, the screen space size is computed. This mostly affects how large the sprite appears from far away. MaxScreenSize: The maximum size of the sprite on screen. When the camera is close to the sprite, it will not fill up the entire screen. Instead, its on-screen size is clamped to this. Therefore, when getting close to a sprite, it appears to shrink. AspectRatio: The ratio of width to height of the sprite texture. See Also Textures"
  },
  "pages/docs/graphics/substance-textures.html": {
    "href": "pages/docs/graphics/substance-textures.html",
    "title": "Substance Textures | ezEngine",
    "keywords": "Substance Textures EZ has support for Substance Designer textures. This is provided through the Substance plugin. Once you enable the plugin, the Substance Package asset type is available. This asset type allows you to reference a *.sbs file. Then you can define which graphs to import from the SBS file. The outputs of the substance material are made available as Substance Texture assets. These can be referenced anywhere, where any other 2D texture could be used. Note: The Substance integration is only an editor feature. At runtime there is no procedural texture generation. Thus, there is also no memory saving for using SBS files over regular textures. This is purely a convenience feature for people who use Substance Designer, so that they don't need to manually export the textures. Instead the EZ editor takes care of updating the outputs for you. Important: This feature requires you to have Substance Designer installed and a valid license for its usage. If you want to use this workflow in a team, every user of the editor must have Substance Designer on their machine, otherwise asset transform will fail for them. See Also Textures Substance Designer"
  },
  "pages/docs/graphics/textures-overview.html": {
    "href": "pages/docs/graphics/textures-overview.html",
    "title": "Textures | ezEngine",
    "keywords": "Textures Textures come in multiple forms. Most common are 2D textures loaded from file. 2D textures can also be created as render targets for render-to-texture (TODO). For sky boxes or special effects you can also set up cubemap textures. When the source texture comes from a file, you can create a texture asset for that file by importing it. Otherwise you need to create the respective asset document manually, for example for render targets. Textures are most often referenced by materials. Texture assets only specify how source files are imported and combined, they don't define rendering behavior. Therefore most components don't use textures directly, but rather use materials, which configure the overall rendering, unless the component in question already sets up the rendering itself, such as the SkyBox component. All texture assets show a 3D preview. Here you can move the camera around to observe how the texture looks. Using the Mipmap slider in the toolbar, you can inspect individual mipmaps. With the channel selection dropdown next to the slider you can choose which channels to show. In the bottom left of the viewport, some stats are displayed, for instance what compression format is used. On the right-hand side you see the property grid where you choose the texture settings. Since transforming a texture can take quite some time, these assets are not automatically updated when you change their properties or even save the asset. Instead, transform the asset manually either by clicking the transform button in the toolbar or with Ctrl + E. Video: How to import textures 2D Texture Asset The following properties are available: Usage: Tells the texture converter what type of data the texture represents. This affects what color space (TODO) will be used, ie. whether the format uses sRGB or Linear encoding. If your color texture ends up using the incorrect color space (e.g. Linear when it should be sRGB) you will typically notice that the texture appears too bright and washed out. Auto: In this mode the converter will guess the usage, utilizing file name heuristics, the source file format and sometimes even the content. If your file names use suffixes like _D, _Normal, etc. this can work pretty well. Color: Use this for textures that represent color. For example diffuse textures, skyboxes and emissive color textures. Linear: Use this for textures that represent 'data' that is not directly a color. For example roughness, metallic, ao and specular maps. HDR: For textures that contain high dynamic range data. Since HDR data requires special file formats, Auto mode can detect this reliably. Normal Map: For textures that represent normal maps. Normal Map (Inverted): For normal maps that use a different convention for the direction vectors. Bump Map: For greyscale textures that represent height values, which should be converted to a normal map. Mipmaps: This property allows you to choose whether the texture should have mipmaps and what algorithm to use for generating them. Unless you have a very special use case, mipmaps should always be enabled. The Kaiser filter usually gives slightly better results than the Linear filter. PreserveAlphaCoverage, AlphaThreshold: When you want to use a texture for vegetation, where the alpha channel is used to define transparent areas, then using this feature can improve how the vegetation looks when it is far away. The smaller a mipmap becomes, the more it represents the average color of the original texture. The same is true for the alpha channel. The problem then is, that if a vegetation texture is mostly transparent, the lower mipmaps will become more and more transparent. In practice that means the further you are away from a plant, the more transparent it becomes. For example all the leaves in a tree may disappear. If, on the other hand, the texture is mostly opaque, the lower mipmaps would become more and more opaque, so tree would appear much thicker when far away. PreserveAlphaCoverage can improve this, by making sure that the percentage of pixels with a value below and above AlphaThreshold stays the same throughout all mip levels. Thus, if the leaves of a tree are 70% transparent in the original texture, even when viewed from far away about 70% of its pixels will be transparent and not more. For this to work, the AlphaThreshold that will be used by the shader to discard pixels has to be known exactly and may not differ when the texture is used later. Computing the alpha coverage takes more time. Unless you are trying to solve the problem described above, keep this option disabled. Compression: Specifies how strong the texture should be compressed. Depending on Usage and whether an alpha channel is present, this determines which exact file format will be used (printed in the lower left corner of the viewport). Note that for some combinations there may be no difference between medium and strong compression. Don't choose Uncompressed unless you actually notice compression artifacts. Texture compression saves significant amount of memory (usually a 4:1 compression, sometimes even 8:1) and has no performance overhead during rendering. The only downside is, that it takes longer to transform compressed texture assets. DilateColor: For textures with an alpha mask. Will dilate (smear) the color channel outwards everywhere where the alpha channel is zero. Useful for example for vegetation textures, where the alpha channel defines a cutout mask. If the color channel does not contain some average color in the transparent areas, but something different, usually black, this unwanted color will bleed into the opaque areas in the lower mipmaps. As a result, masked objects appear to have a black edge. To fix this, either author your source textures to have a representative color in the transparent areas, or enable this option to automatically generate it. Note that this option makes texture transform slower. The image below shows the difference of a texture that is completely black in the transparent areas, versus the same texture, using color dilation. Notice how there is a black edge around the plant in the left image. Flip Horizontal: For textures that are stored upside down. HdrExposureBias: For textures that have the HDR Usage. Allows you to scale the brightness. Filtering: Specifies with which filter method the texture is sampled during rendering. If you select one of the Fixed modes, it will be filtered exactly with that mode. Mostly useful, if you want to use Nearest filtering, for artistic reasons. Using one of the other modes (Low Quality / Default Quality / High Quality) the texture will use a filtering mode that is decided at runtime. For example, if the default sampling mode is set to Anisotropic 2x, then High Quality will result in Anisotropic 4x (one mode higher) and Low Quality will be result in Trilinear sampling (one mode lower). Lowest and Highest will use two modes lower/higher, though Nearest filtering will never be used. AddressMode: Specifies whether a texture will be repeated when the texture coordinates used to sample the texture are outside the [0; 1] range. The 3D preview shows the effect of this property after you transform the asset. Channel Mapping: This option allows you to choose which channels the texture will have, and from which files to take the data for each channel. RGB(A) - Single Input is the most common choice, but you can also combine multiple textures, e.g. take the color from one input file and the alpha channel from another. Input N: Depending on the chosen channel mapping you need to specify one to four input textures. Cubemap Texture Asset Cubemap assets have a subset of the properties that 2D textures have. Their behavior is identical, except for Channel Mapping, which allows you to build a cubemap either from one or from six input files. Render Target Asset Render targets can be used like regular 2D textures. That means materials can reference them and display their content. However, they are filled by rendering to them at runtime, for example by using a camera to render the scene from a certain viewpoint. Have a look at the render to texture (TODO) article for details. Apart from some properties shared with 2D texture assets, render targets have these unique properties: Format: Determines whether the texture can store HDR data (10 bit or 16 bit) and whether the texture represents colors (sRGB) or linear data. Resolution: The dimensions of the texture. If set to CVar RT Resolution N the resolution will be read from the CVar r_RenderTargetResolutionN. This can be used to adjust the resolution dynamically. CVarResScale: If the resolution is read from a CVar, this allows to scale it. For instance to create a half resolution render target. See Also Materials Render to Texture (TODO) Sky Color Spaces (TODO)"
  },
  "pages/docs/input/input-component.html": {
    "href": "pages/docs/input/input-component.html",
    "title": "Input Component | ezEngine",
    "keywords": "Input Component The input component is used to forward input from a selected input set to all components in the same sub-tree of game objects via the ezMsgInputActionTriggered message. For the desired input set to show up in the editor, it has to be set up through the project settings. Deactivating Input Notifications You may have many objects in a scene that the player can take control of. Each object would have its own input component to route input into its scripts. However, every object is only interested to receive input notification messages, while it is actually controlled by the player. At other times it would be wasteful to still receive input notifications, only to ignore them. Therefore an object should deactivate its input component when the player is not controlling it. Input Notification Message The message ezMsgInputActionTriggered contains information about a single input action. It passes along the current state (up, down, pressed, released) and how much the input slot got activated (for instance how far the mouse was moved). Component Properties InputSet: The name of the input set to use. All input actions that are part of this input set will be forwarded as messages. Granularity: Configures whether the component sends messages only for certain state changes, or also continuously while a button is held down. ForwardToBlackboard: If enabled, the input component will attempt to store input states in a nearby blackboard. If it can find a blackboard on the same owner game object, or a parent game object, it will set a value with the name of the action to a float value between 0 and 1, depending on whether the action is fully triggered (1), not triggered (0) or partially triggered (e.g. for a thumb stick). This is an quick way to forward input data to an easily accessible data structure. See Also Input"
  },
  "pages/docs/input/input-config.html": {
    "href": "pages/docs/input/input-config.html",
    "title": "Input Set Configuration | ezEngine",
    "keywords": "Input Set Configuration Input sets can be configured either from code, for example from a game state, or through the editor project settings. The project settings mostly exist for convenience. They allow you to set up a fixed default input scheme, which is useful during development, and doesn't require custom C++ code. A finished game should allow the player to choose their own key binding, though. From the Editor Project > Project Settings > Input Configuration... opens a dialog to configure the available input actions. Using the New Input Set button at the top left, you can add input sets. These typically represent a certain state in your game. For example you may have one set for when the player character is by foot, and another input set for when they are steering a vehicle. Both input sets can use the same keys, but they may represent different actions. Since both states are mutually exclusive in the game, they don't clash. Each input component takes its input state from one specific input set. Using the New Action button, you can add an action to the selected input set. Each action represents something that the player can do. You can then select a an input slot that triggers this action. Slots represent keyboard keys, controller buttons, voice commands, head movements and everything else that can be considered 'input'. The Scale value allows to adjust how strongly a slot activates the action. The Time Scale option specifies whether the [0; 1] value of an action will be taken 1:1 from the input slot, or whether the value will be scaled by the amount of time passed between frames. As an example, many actions like shoot or jump are purely on/off decisions. For these actions time scaling has no use. However, an action like 'rotate left/right' should be something that is smooth and always the same speed, no matter whether the game runs at 30 Hz or at 60 Hz or wildly varies. Thus the value that is given to the game code should be scaled by the time that has passed between frames, such that applying the value to your character or vehicle will still result in a constant, smooth change. Configuring Available Slots The input slots that can be chosen in this dialog are listed in files located under Data/Tools/ezEditor/InputSlots. If you want to expose new slots for a custom input device, you can just add a file there. At the moment the available slots are not automatically extracted from the plugins. From Code Input actions are registered through ezInputManager::SetInputActionConfig(). The code below shows a game state that uses a helper function to quickly register a couple of actions. static void RegisterInputAction(const char* szInputSet, const char* szInputAction, const char* szKey1, const char* szKey2 = nullptr, const char* szKey3 = nullptr) { ezInputActionConfig cfg; cfg.m_bApplyTimeScaling = true; cfg.m_sInputSlotTrigger[0] = szKey1; cfg.m_sInputSlotTrigger[1] = szKey2; cfg.m_sInputSlotTrigger[2] = szKey3; ezInputManager::SetInputActionConfig(szInputSet, szInputAction, cfg, true); } void SampleGameState::ConfigureInputActions() { SUPER::ConfigureInputActions(); RegisterInputAction(\"SamplePlugin\", \"SpawnObject\", ezInputSlot_KeyO, ezInputSlot_Controller0_ButtonA, ezInputSlot_MouseButton2); RegisterInputAction(\"SamplePlugin\", \"DeleteObject\", ezInputSlot_KeyP, ezInputSlot_Controller0_ButtonB); } Building a Key Binding UI To build a UI where the player can select an action and then press a key to bind it to that action, have a look at ezInputManager::GetPressedInputSlot(). ezInputManager::GetAllInputSets() and ezInputManager::GetAllInputActions() enable you to build the UI for all known actions and sets. For display purposes there are also ezInputManager::GetInputSlotDisplayName() and ezInputManager::GetActionDisplayName(). See Also Input Project Settings"
  },
  "pages/docs/input/input-overview.html": {
    "href": "pages/docs/input/input-overview.html",
    "title": "Input System | ezEngine",
    "keywords": "Input System All input devices, such as mouse, keyboard, controllers and other periphery are managed by the central ezInputManager. New devices are exposed to the system by implementing a custom ezInputDevice. For details, please see the API Docs for those classes. Abstract State All input state is abstracted away. There are two layers of abstraction: input slots and input actions. Slots are the lower level hardware representation, actions are a higher level semantic representation of what the player can do. Actions are bound to one or multiple slots and if possible the game should allow the player to change those bindings. Input Slots Input slots represent the state of actual device features, such as the buttons on a controller or the keys on a keyboard. Slots are already an abstraction, though. For instance for a mouse there are input slots for both the last relative movement, as well as the current absolute cursor position. Input devices are free to define arbitrary input slots, which may represent actual physical features or virtual functionality. Each input slot is a single float value that is typically in the range [0; 1] or [0; inf]. If a device feature has both a positive and a negative value, such as the X and Y axis of a stick, these are typically exposed as two input slots, one for the positive part of the axis, one for the negative part, and each uses an absolute value. This generalized concept makes it easier to map input slots to actions in various ways. For example, each stick on a controller is represented as four input slots (+X, -X, +Y, -Y). The same would be true for a DPad. That allows to map either the stick or the DPad to, e.g. steering a vehicle. The only difference is that the stick can report values between zero and one, whereas the DPad would only report values that are exactly zero or one. If you want to allow the player to map input slots themselves, you can query ezInputSlotFlags for each slot, which describe how a slot can be used, to only let them map keys that make sense for a given action. In practice though, you rarely work directly with input slots. Typically the only situation where one works directly with input slots, is during the initial setup of the slot to action key binding. Input Actions Input actions represent the features that the game exposes to the player. Actions are typically things like walk forwards/backwards, jump, shoot and so on. In actual game code you typically only work with input actions. Actions are bound to input slots, meaning one or multiple slots will trigger the action. This binding can be modified at any time. It is common to bind both keyboard/mouse slots to an action, as well as controller slots, such that a game can be played with either device. Input Sets An input set represents a specific use case in a game. For example you might have a dedicated input set for being on foot and one for driving a vehicle. Each input action is associated with one input set. That means you can have an action for 'drive forwards' in one input set and an action for 'walk forwards' in another input set. Each input action may be triggered by any input slot, meaning that the same input slot, e.g. the W key, can trigger both 'walk forwards' and 'drive forwards' at the same time. The game would either query the one action or the other, depending on whether the player is currently on foot or in a vehicle. When you have multiple input sets, you can reuse the same names for actions (e.g. 'shoot') and still allow the player to bind the keys differently. In practice you may only need a single input set, and you shouldn't use more than three. Key Value Each input slot and therefore also each action has an amount how much it is being triggered. This is mostly of interest for analog signals such as from a stick, since buttons only report the values 0 and 1 with nothing in between. You can query these values, if an actions grants fine grained control over something. For many actions, though, you only require the key state. Key State Both slots and actions have a current ezKeyState, which describes whether the action is currently active or not, and whether the stated changed just now (between the last frame and this frame) or has been persisting. If you want to react only once to a button press, you would check for the state ezKeyState::Pressed, whereas if you want to do something as long as a button is held, you would react to ezKeyState::Down. The key state is derived from the key value. Once a button gets pressed, the key value jumps from 0 to 1. As a consequence the key state transitions from ezKeyState::Up to ezKeyState::Pressed for this frame, and continues to ezKeyState::Down in the next frame. Once a button is depressed, the key value goes back to 0 and the key state transitions first to ezKeyState::Released for one frame, and finally back to ezKeyState::Up in the next frame. Accessing Input State Through ezInputManager all input state (both for slots and actions) is accessible by all code at all times. However, depending on the type of game you build, you may prefer to use the input component to get a specific input set routed to a specific component through ezMsgInputActionTriggered. Direct Input Access In some games the player doesn't have a physical presence, but is rather an outside observer. Examples would be an RTS or Tetris. Here you don't have a character controller inside the world. Implementing the control scheme for such game logic through game objects and components can be tedious. Instead, it is much easier to write a custom game state and handle all the interaction, the camera movement and the general game logic there. In such a scenario, the game state would call ezInputManager::GetInputActionState() directly to retrieve the state of an action. This is also what the ezFallbackGameState uses to provide the most basic functionality (such as quitting when ESC is pressed). Therefore, if you write a custom game state to show a main menu, you would use this direct access to hook up the input system to the UI navigation. Direct access to ezInputManager is (currently) not possible through TypeScript components. Component Based Input Access In games where the player does have a physical presence, such as creatures or vehicles, and they may swap between those, it might be difficult to retrieve the input in a game state and then use it to control any one of the many vehicles. Instead, it is easier to have each vehicle fully handle its own state and therefore also retrieve the input locally. In this case each vehicle would know whether is currently 'possessed' by the player, and as long as it's not, it would just ignore all input (or generally disable receiving input notifications). If you write a custom component for your vehicle, that component could access the input state directly, during its update. Another option, though, is to use an input component. All that this component does, is to check for state changes of input actions from a selected input set and send those state changes as messages to its sub-tree of game objects and components. Any component that handles this message type, can react to the input. This message based approach is how TypeScript components are able to handle input. Since the input messages are delivered to all child objects, you can have multiple scripts or other components which each react to different input. For example one script can forward movement related input to a character controller and another script can handle input for weapons. Setting Up Input Sets Input sets can be configured either from code, or through the editor project settings. For details, see this page. See Also Input Set Configuration Input Component"
  },
  "pages/docs/materials/materials-overview.html": {
    "href": "pages/docs/materials/materials-overview.html",
    "title": "Materials | ezEngine",
    "keywords": "Materials Materials are used to define the visual properties of rendered objects. Materials specify what shader to use to render an object and they allow you to configure that shader. Most commonly you select which textures to use. The most common use case for materials is through meshes. Each sub-mesh can have its own material. The image above shows the material asset editor. Material assets are often automatically created when you import a mesh asset. For example importing an FBX or OBJ file will not only create a mesh asset, but can additionally create the necessary material and texture assets for you. Unfortunately, this process is not always perfect, so you should always review which assets were created and how. Video: How to create a material Physical Properties Although materials are mainly used to configure the rendering, they can optionally reference a surface, which is used to define physical properties (e.g. friction) and gameplay relevant interactions. Whether the referenced surface is actually used depends on where the material is applied. For instance, on a greyboxing component the material sets up both the graphics and the physics properties, on a mesh component it only sets up the rendering and you would need to additionally select a surface for the corresponding collision mesh. Selecting a Shader Through the ShaderMode property there are three ways a material can select which shader to use: From Base Material: This is the most convenient and most commonly used method. In this mode, you need to select a BaseMaterial, which is just another material. All the properties of that base material are copied over to your material, including the shader selection. You can then override each property as you like. This makes it easy to set up a few common base materials and then \"derive\" all other materials from this common base. From File: In this mode the material actually references a proper shader file. This allows you to select a custom shader. By default, EZ doesn't have many different shader files, as all important variations are provided by the same shader file. However, if you do decide to write a custom one, this is the way to select it for your material. The engine parses the shader file for configurable properties and displays those as UI elements in the material editor. So things like which texture you can select and what other lighting properties the material will have, are all defined by the selected shader. You can easily create a custom shader from a shader template. Visual Shader: In this mode the material editor will show an additional editing area beneath the 3D view, where you can create your own shader through a visual graph system. This enables you to create custom shader effects like animated textures. There is a dedicated chapter about visual shaders (TODO) that explains how to do so. If you change the selected shader, you need to transform (Ctrl+E) the material asset for the change to take full effect. Shader Properties The Shader Properties section lists all the properties that the selected shader exposes. The 3D viewport will live update for any change you make here. DefaultMaterial Properties The DefaultMaterial shader that comes with EZ implements a Physically Based Rendering model (PBR), which is the de facto industry standard these days. The details of PBR rendering are beyond the scope of this documentation, if you want to get an understanding of how roughness and metalness are use (see below), please search the internet. The DefaultMaterial provides these options: Blend Mode: Defines whether the object will appear opaque or transparent. Opaque: The object appears solid. Masked: In this mode the object can have fully transparent (invisible) areas and fully opaque ones. Blending is not possible. This is commonly used for vegetation or things like chain-link fences to cut out part of the object. Masked geometry does not require any sorting during rendering and is therefore the most efficient and reliable mode of transparency. Which areas appear transparent are defined by the alpha channel of the base texture and the MaskThreshold property. Every pixel whose alpha value is above the threshold (e.g. white) will be visible (opaque) and every pixel whose value is below the threshold (e.g. black) will be invisible. Transparent: In this mode geometry will appear see-through, ie. it will be blended with the geometry behind it. This mode is commonly used for things like glass or water. Again, the alpha channel of the base texture determines which areas appear more or less transparent. Additive: In this mode the geometry will not be blended with the background but simply added on top of it. The alpha channel affects how strongly it is added. Modulate: This mode allows you to darken or brighten the background. A pure white material (base texture and base color) will brighten everything that is behind the object. A pure black material will darken the background. A material that is mid grey will let the background through unmodified. This mode can be used for various special effects, especially when writing a visual shader (TODO) that animates the texture and the alpha channel with noise. For testing transparent materials it may be useful to create an object in a scene and observe it there, where you can place it in front of different backgrounds. Shading Mode: This mode allows you to select whether objects with this material should receive realistic lighting or should always appear fullly bright. The latter is useful for 2D sprites and UI elements. Two Sided: If enabled, polygons with this material can be seen from both sides. This is useful for fences, vegetation and other masked geometry that is often represented only by a single polygon but can be looked at from both sides. Base Color: The base color of the material. When no base texture is used, this is its only color, otherwise it is multiplied into the base texture color. UseBaseTexture: If enabled, the Base Texture is used to color the object. This requires proper UV coordinates on the mesh. MaskThreshold: Used for the Masked blend mode (see above). UseNormalTexture: If enabled, the shader uses the Normal Texture to apply normal mapping. UseOcclusionTexture: If enabled, an OcclusionTexture is used to affect lighting and to pronounce crevices. The effect of this can be very subtle. Roughness Value: If no Roughness Texture is given, this value is used for lighting. If a roughness texture is available, this value is multiplied into the texture's value. The rougher a material is (value closer to one), the more diffuse the lighting will be (stone, cloth, etc). The smoother the material is (value closer to zero), the more pronounced specular highlights it will have (glass, ceramic). UseRoughnessTexture: If enabled, the shader uses the Roughness Texture to determine how rough the surface is. The texture's value is multiplied with the Roughness Value so make sure to set that 1 to get the exact result. Metallic Value: If no Metallic Texture is provided, this is the fallback value. Typically this should be 1 for metals and 0 for all other material types. UseMetallicTexture: If enabled, a dedicated Metallic Texture is used to specify per pixel whether it is a metal or not. In physics, a material is either a metal or not, in computer graphics values in between are allowed and used to blend between the two results. UseOrmTexture: If enabled, a dedicated Occlusion / Roughness / Metallic (ORM) Texture is used to specify these values per pixel. This takes precedence over the individual UseOcclusionTexture, UseRoughnessTexture and UseMetallicTexture options and if individual textures are also specified, they are not going to be used. ORM textures pack these three values into 3 channels (Red = Occlusion, Green = Roughness, Blue = Metallic) which is generally more efficient, though it means you can't have different resolutions for each texture. EmissiveColor: An additional overall emissive color. If an Emissive Texture is activated, these two colors are multiplied, so you need to set this to white for the texture to have an effect. This is an HDR color, so you can scale up its intensity and thus pronounce the glow even further. UseEmissiveTexture: If enabled, the Emissive Texture is used to define per pixel where the material will glow. This is multiplied with the EmissiveColor, so make sure that is not set to black (its default). Material Preview The 3D viewport of the material editor allows you to switch the render mode to inspect only specific aspects of the material. Using the eye dropdown menu you can also select different meshes for previewing the material on. See Also Visual Shaders (TODO) Textures Meshes"
  },
  "pages/docs/materials/surfaces.html": {
    "href": "pages/docs/materials/surfaces.html",
    "title": "Surfaces | ezEngine",
    "keywords": "Surfaces Surfaces are assets that define the physical properties of objects. This includes both the actual physics parameters such as friction, but also how objects interact from a gameplay perspective. For example, surfaces define what sounds are played when objects slide or roll around and what effects should be spawned when there is an impact on an object. Surfaces and materials are related. Materials are used to define the appearance, surfaces define the behavior. However, you may have hundreds of materials in a project, but typically only tens of surfaces. The main reason for this is that you may have many different wood textures that look different, but they will all behave the same way. Therefore you only need one surface defining the properties of wood, but many different materials for each wood texture. The image above shows a surface for glass. Each surface stores a small number of general parameters. Additionally there is an array of surface interactions which may hold many entries. These are the key to configuring what effects get spawned when you shoot something, what sounds and effects are spawned when objects collide, what footstep sounds the player makes, and so on. Physics Properties Surfaces are used by collision meshes and other physics components to look up basic physics properties. Restitution: Configures how \"bouncy\" a material is. Objects with a high restitution will bounce very strongly (like rubber) and objects with a low restitution will come to rest quickly (like soft wood). Friction: The friction values affect how slippery a material is and thus how much it will slide or roll. Smooth surfaces typically have lower and rough surfaces higher friction. Some physics engines also differentiates between static friction for objects that are currently standing still and dynamic friction for objects that are already moving. Static friction is typically higher than dynamic friction, meaning it is more difficult to get something to move, than to keep something moving. Note that Jolt does not differentiate between the two, and we use an average value here. AI Properties Ground Type: This property is used by the AiPlugin during navmesh generation and path searches to determine whether a character can traverse this kind of terrain and at what speed. Surface Interactions The main feature of surfaces are surface interactions. These are used to tell the engine which effects it should spawn when a surface interacts with something else. The other thing may be another surface, for example when a box slides over the ground, then both the box and the ground have a surface. However, the other thing could also be something entirely different. For example a laser beam. In this case the surface may define whether the beam should create sparks or fire or steam, etc. Every surface interaction is configured by adding an entry to the Interactions array (see image above). Each entry represents one way that the surface can interact with something else. To determine which interaction to use, for instance when you shoot an object, you specify a Type string. Triggering Interactions Interactions are triggered by different systems. The physics engine triggers interactions when objects collide with enough force. Components, such as the projectile component trigger interactions when they hit something. The character controller component triggers an interaction with the ground every time the player moved a certain distance (for footsteps). There are many ways that both built in code, as well as custom code can leverage surface interactions to spawn effects. Having the configuration defined by the surfaces, and not by the components themselves, decouples and centralizes this information and makes maintaining and changing this configuration a lot easier. Example: If you want to shoot something in your game, you need a prefab that represents every type of projectile. You may have regular bullets and laser shots. The two types should result in different effects when hitting various surfaces, e.g. bullets create debris, lasers make burn marks. EZ comes with a projectile component which implements the functionality to move an entity forwards each frame and check the world for collisions. When it collides with something, it will apply damage and a physical impulse. The projectile component does not, however, specify what kind of effect to play when it hits something. Instead, each type of projectile is given a string what type of interaction to do when it hits a surface. So our bullet projectile would use the BulletImpact interaction, and our laser projectile would use the LaserImpact interaction. Then when a projectile hits an object, it first looks up what type of surface it hit exactly. Then it queries that surface for an interaction entry with the desired name. If such an entry exists, it spawns the referenced prefab at the point of impact. In the image above there is an interaction with Type set to BulletImpact. This entry references a prefab which, when instantiated, will play a particle effect and a sound that looks like a bullet hit glass. Consequently, when we now shoot at such a surface with a bullet, it will act believable. There is no interaction set up for LaserImpact, so if we were to shoot a laser at this type of surface, the projectile would stop there and apply damage to the hit object, but it would not spawn any kind of effect. Spawning Prefabs All surface interactions ultimately spawn prefabs. So if you want to have different footstep sounds when walking over stone, sand and mud, you need three prefabs, each playing a different sound. However, since they are prefabs, your creativity is not limited to playing a sound. Your \"sand footstep effect\" may contain spawning a small dust particle effect. And the \"mud footstep effect\" could additionally spawn a footprint decal. The following options allow you to adjust how prefabs are spawned: Parameters: Exposed Parameters can be used to spawn the same prefab but with different configurations. Alignment: Defines how the spawned prefab instance will be rotated. The +X axis of the prefab is considered to be 'forwards'. Surface Normal: The 'forwards' direction (+X) of the prefab will be aligned with the normal of the object where the interaction happened. With this mode, the angle at which you shoot at a surface would not make a difference, as the effect is always spawned orthogonal to the hit geometry. Incoming Direction: For things like bullets, this is the direction at which the surface was hit. So the 'forwards' direction of the spawned prefab would be directly dependent on the angle at which a shot hit the surface. Reflected Direction: The incoming direction reflected at the normal of the geometry. Reverse Normal/Direction: Same as the other three modes, just the opposite direction. Note that depending on which system triggers the interaction, there may or may not be a 'direction' associated with it, so the 'Incoming' and 'Reflected' mode may be identical to the 'Normal' mode. Deviation: An optional amount of random deviation away from the spawn direction. This allows you to randomize the spawn direction. Hierarchical Surfaces Interactions are often the same across many types of surfaces. A laser shot may leave a burn mark on pretty much any surface, except glass and water. Setting up the same effect cross many surfaces can be tedious and error prone, especially if you need to adjust it later on. Therefore surfaces allow you to configure them in a hierarchical way where you specify common behavior in base surfaces and either add or override specific behavior only for derived surfaces. In the example image above you can see that the glass surface has a BaseSurface specified, which references Default.ezSurfaceAsset. What this means is that if some system (let's say the projectile component) looks up an interaction like BulletImpact on a surface and does not find it on the surface directly, it will then go to its BaseSurface and try to look up the interaction there. This can go through multiple steps until it either found the desired interaction, or it reached the last base surface and still found nothing. This way you can set up many generic interactions on a common base surface and override specific interactions only where needed. This also helps to quickly prototype for example a new gun: you add a generic interaction for the gun type to the base surface and immediately get some feedback on all surfaces. Then you can step by step flesh out how the gun will affect different surface types, by overriding the interaction type on derived surfaces one by one. Physics Interactions Dynamic physics objects can have three types of interactions with surfaces: They can bump into each other They can slide across a surface They can roll around Surface interactions enable you to let these make sounds or play effects. Which types of physics interactions an object creates is configured on each dynamic actor with the On Contact flags. Only if the respective flags are set, will the engine even attempt to spawn a prefab for a physical interaction. Bump Each surface has an OnCollideInteraction property. This string specifies which surface interaction to trigger on another surface, when two objects collide. For example the wood surface may set its OnCollideInteraction property to Bump_Wood. When you now have a wooden box and let that box fall onto a stone floor, the physics system will look up the OnCollideInteraction property from the dynamic object (the box) and use the string (\"Bump_Wood\") to trigger this interaction from the stone surface. If the stone surface defines such an interaction, the corresponding prefab will be spawned, otherwise nothing happens. Now this behavior would trigger an interaction every single time the box touches anything and it would always be the same interaction, no matter whether it just touches something lightly, or collides with a lot of force. To fix this, each interaction additionally has an ImpulseThreshold. This allows you to set up multiple surface interactions of the same type that have different thresholds and reference different prefabs. If an object bumps into another object, the physics engine provides the amount of force with which the collision occurred. The system will then pick the surface interaction with the highest exceeded threshold. Thus if two objects collide with a lot of force, it may spawn a prefab with a loud sound, and if they collide with little force a quieter prefab may be spawned. And if they collide with even less force, no prefab may be spawned at all. Note that looking up surface interactions is generally hierarchical, meaning that if something like Bump_Wood is not part of a derived surface, but part of its base surface, it will be found. However, regarding the impulse threshold property, the search is not fully hierarchical. Instead, once any interaction of the right type is found in any (base or non-base) surface, no further search is done in additional base surfaces, even if there would be a surface interaction with a better matching threshold in the base surface. Basically, once a surface overrides a surface interaction, it completely replaces all available interactions of that type and should have full control which interaction is taken at which threshold. Slide and Roll Similar to the Bump interaction, prefabs can be spawned to deal with situations where an object slides or rolls across another object. The difference here is, that prefabs for these interactions get spawned when a slide or roll starts, and then only their (average) position gets updated, until the interaction stops, at which point the prefab gets deleted again. That means that slide and roll interaction prefabs should do things in a loop, for instance play a looped sound or contain an endless particle effect, such that the effect will last as long as the interaction is active. Video See Also Materials Jolt Physics Integration Sound"
  },
  "pages/docs/materials/visual-shaders.html": {
    "href": "pages/docs/materials/visual-shaders.html",
    "title": "Visual Shaders | ezEngine",
    "keywords": "Visual Shaders Visual Shaders are fully functional (except for several known limitations), but currently undocumented. An alternative to using visual shaders is to write a custom shader based off of a shader template. See Also Shaders Shader Templates"
  },
  "pages/docs/performance/asset-collections.html": {
    "href": "pages/docs/performance/asset-collections.html",
    "title": "Asset Collections | ezEngine",
    "keywords": "Asset Collections A collection asset references other assets. Its main purpose is to control which assets should be made available (loaded in the background) to prevent performance issues. Additionally, each referenced asset can be assigned a name, which makes it possible to load that asset by that name, instead of by its GUID. A collection asset is a simple list of references. The name property for the entries is optional. Preloading Assets If you need full control, a game state can load an ezCollectionResource and call ezCollectionResource::PreloadResources() to make certain assets available. A typical use case when you would do this, is when there are certain assets that are always needed in your game. For example, your game may already start loading all the player related assets in the background, while it displays the game's splash screen and main menu. Another option is to insert a collection component into a scene or prefab. If the component is active, that means it will automatically preload the assets from the referenced collection. Doing this makes sense when a script triggers actions that depend on assets, which are otherwise not loaded. If those assets are not preloaded, they will be loaded on demand, which may result in performance hiccups or visual glitches (if a loading fallback resource is used for a while). Keeping Assets Loaded Some assets are only ever needed for brief periods of time, for example particle effect textures. As long as no such effect is playing, their textures are not referenced and the resource manager (TODO) may decide to unload them. The next time those assets are needed, they will be loaded again either resulting in a stall, or a visual glitch. To prevent this, you can create a collection with all the assets that you consider vital, and place a collection component in your scene or prefab. The collection will not only preload those assets, but also hold a reference to each one, preventing them from being unloaded. Referencing Assets by Name Usually, if you want to load a resource from code, you need to know the asset GUID. If you loaded that asset through a collection, and the collection gave a name to that asset, you can also load that asset by that name instead. See the code of the Asteroids sample as an example. Not only can this be more convenient, it also allows you to switch which asset will be loaded under the given name, by modifying the collection, or even by having multiple collections, and loading one or another. See Also Collection Component Resource Management (TODO)"
  },
  "pages/docs/performance/collection-component.html": {
    "href": "pages/docs/performance/collection-component.html",
    "title": "Collection Component | ezEngine",
    "keywords": "Collection Component A collection component references a collection asset. When it is part of a scene and active, it makes sure to preload all resources that are referenced by the collection. Since the collection holds a reference to each resource, this also prevents those resources from being unloaded prematurely, which can prevent artifacts and performance hiccups. Placing a collection component into a scene or even into a prefab can be useful when you know that some assets will be needed, but they are currently not referenced in such a way that the engine keeps them loaded. By referencing a collection that contains all the needed assets, you can be certain that those assets are loaded as soon as possible and will stay loaded at least as long as the collection component exists. If a deactivated collection component is part of a scene, it will not trigger a resource preload until it gets activated. You can use this to control exactly when you need the preload to happen. For instance, when the player enters a certain area, a collection component can be activated to preload data that is needed for a cutscene. Component Properties Collection: The referenced collection asset. RegisterNames: If enabled, the component will register the resources in the collection asset that have a dedicated name under that name, so that these resources can be loaded that way. See Referencing Assets by Name. See Also Asset Collections Profiling"
  },
  "pages/docs/performance/occlusion-culling.html": {
    "href": "pages/docs/performance/occlusion-culling.html",
    "title": "Occlusion Culling | ezEngine",
    "keywords": "Occlusion Culling Occlusion Culling is an optimization to speed up rendering. The general idea is to somehow determine whether an object is occluded by other geometry, and if so, to not render it at all, since it won't contribute to the final image anyway. There are different ways that occlusion culling can be implemented. EZ uses a software rasterizer to render occluder geometry to an offscreen buffer. For every object that is then supposed to get rendered by the GPU, it can quickly cross-check with the offscreen buffer, whether the object would be fully occluded, and if so, rendering is skipped. IMPORTANT! Occlusion culling is currently only available on Windows machines with CPUs that support the AVX instruction set. If not available, the occlusion culling optimization step is simply skipped. Without occlusion culling, an object that is behind a wall, will be rendered. We can observe this by making a transparent wall: Now if we turn the (transparent) wall into an occluder, the engine is able to skip rendering of the object. Of course in practice you would not use transparent walls as occluders, for obvious reasons. Here you can observe that the object isn't visible anymore, but its shadow is. That's because from the point of view of the light source, the object is visible, and in fact the shadow could be visible from the main camera position. If the object was also occluded from the light source's perspective, its shadow would also not get rendered. How to Use Occlusion Culling All rendered objects automatically benefit from occlusion culling. The only thing that's needed is for your level to contain occluders. The easiest way to do so, is to use greyboxing geometry. It is automatically used as occluders and all its shapes are supported. Another option is to use occluder components. These components allow you to create invisible occluder geometry, that can also be moved around dynamically. Visualizing Occluder Geometry Use the CVar Spatial.Occlusion.VisView to enable a screen overlay that displays the main view's occlusion buffer: This allows you to inspect whether occluders are well placed. Best Practices You should have few but large occluders. Disable the occluder flag on greybox geometry that is too small to have a significant contribution. Generally try to avoid small occluders as they can create artifacts. The occlusion buffer has a limited resolution and fine details may therefore \"occlude\" objects, even though they are not actually occluded at full resolution. Place occluder components inside large objects, such as boulders, cars or furniture. It doesn't matter whether they are static or can move around. Do not add occluder geometry to small or thin objects. If in doubt, don't add occluder geometry. Use the occlusion buffer visualization to inspect how well occluders are placed around your level. Use the CVar Spatial.Culling.ShowStats to see how many objects are rendered at any given time to get a feeling for how well occlusion culling works in your scene. See Also Profiling Greyboxing"
  },
  "pages/docs/performance/profiling.html": {
    "href": "pages/docs/performance/profiling.html",
    "title": "Profiling | ezEngine",
    "keywords": "Profiling Profiling an application means to record information about its performance to analyse where time is wasted and figure out how to make things faster. Capturing Profiling Data ezEngine has a built in profiling system with which you can record the function call graph, across all active threads, with precise timing information. The profiling system is very efficient, which is why in development builds it is constantly recording data to a ring buffer, which allows you to write recent profiling data to disk at any time. That means whenever you encounter a situation with bad performance, you can just press a button (in stock applications (TODO) such as ezPlayer it's the F8 key) to save a snapshot. The application will write to the log where it stored the profiling snapshot. You can see this in the console: You can reach this folder easier by typing %appdata% into the address bar of Windows Explorer. You can also store a capture yourself through ezProfilingSystem::Capture(). Investigating a Profiling Snapshot The profiling data is stored as a JSON file in Chrome Tracing Format. To inspect the file, you need to have Google Chrome installed. Type chrome://tracing into the address bar Click Load and select the profiling data file You should now see something like this: Each block represents a profiling scope (typically a function call). Blocks below other blocks represent nested scopes. When you select a block, Chrome displays the time it took. You can scroll and zoom the view. Important: The chrome://tracing view has a very particular method for navigation. Apart from the obvious modes that you can select with the mouse, the default way is actually to use the WSAD keys. W and S are used to zoom in and out. A and D are used to scroll left and right. Use the mouse wheel to scroll up and down. If you manage to get used to this, it is much more convenient than the other methods. Profiling Custom Code If you have custom C++ code that you want to profile in more detail, all you need to do is to insert profiling macros into each scope that you want to profile: void MyFunc() { EZ_PROFILE_SCOPE(MyFuncScope); // ... do stuff ... } This introduces a profiling scope, here with the display name 'MyFuncScope'. The time it takes to reach the end of the scope, starting at the macro, will be timed and recorded. If your scope makes rendering calls for which you want to record the GPU timings, use EZ_PROFILE_AND_MARKER. Important: Many profiling scopes end up taking only very little time. Inspecting those is rarely useful, but each recorded entry takes up valuable space in the ring buffer of the profiling system. Therefore the profiling system automatically discards information about scopes that fall below some threshold. That means your custom scope may not show up in the output. If you do want to see even very short scopes, you can adjust the threshold either through ezProfilingSystem::SetDiscardThreshold() or through the CVar g_ProfilingDiscardThresholdMs. See Also Debugging C++ Code ezInspector Chrome Tracing as Profiler Frontend"
  },
  "pages/docs/physics/jolt/actors/jolt-actors.html": {
    "href": "pages/docs/physics/jolt/actors/jolt-actors.html",
    "title": "Jolt Actors | ezEngine",
    "keywords": "Jolt Actors In EZ objects that are part of the physical simulation are referred to as actors. Note that inside Jolt Physics, they are called bodies. Every actor has its own simulation state, such as position, velocity, torque, contact points with other actors, and so on. Everything that should participate in the simulation, be it static background geometry, or fully simulated bodies, has to be an actor. In some special cases, for instance for greyboxing geometry, the engine takes care to create collision meshes and actors automatically for you. However, for the most part, you have to set up physics actors yourself. We distinguish between three types of actors: static actors, dynamic actors and triggers. Additionally, dynamic actors can be kinematic. Actors themselves don't have a physical shape. Instead they have to be made up of pieces which hold shape components. Upon creation, every actor traverses the node hierarchy below its owner game object to search for shape components. All shapes that are found are added to the actor as a compound shape. If another actor is found in the process, shapes below that node are ignored, though. This way a single actor can have a complex shape, even if every single piece is only a sphere, box, capsule or other simple shape. Static Actors Static actors represent physical objects that never move. This should be the case for the vast majority of the scene geometry. Static actors are much more efficient to deal with. Also, they are the only actors that can use concave collision geometry, meaning arbitrary triangle meshes. Obviously, those meshes cannot be animated. Static actors are set up by attaching a static actor component to a game object. Dynamic Actors Dynamic actors represent all physical objects that move. The physics simulation furthermore distinguishes between kinematic actors and fully simulated (non-kinematic) actors. Kinematic actors are objects whose transform is determined by the game logic. That means you can freely move them around your scene and they will always end up exactly where you moved them to. Regular actors (non-kinematic ones) are simulated using rigid body simulation. These objects collide with other objects, react to forces such as gravity, bounce off of objects that they collide with and slide or roll across surfaces realistically. Regular actors are used to represent all the physical objects in a world that should react realistically to external stimuli. Kinematic actors are used for everything that needs to move, and should affect the simulated objects, but should itself be under full control of the game logic. Kinematic actors will push other actors out of their way relentlessly. If a kinematic actor moves into another kinematic or static actor, the two will simply pass through each other. Whether a dynamic actor is treated as a kinematic actor or not, is an on/off switch. It is possible to switch this property back and forth at will. Dynamic actors are set up by attaching a dynamic actor component to a game object. Video: How to create a physics object Triggers Triggers are a special type of actor. Triggers don't interfere with the simulation, meaning nothing ever collides with them. Instead, triggers monitor whether any other actor overlaps with their volume. If so, they raise an event message to inform other code. Triggers are an efficient solution to detect overlaps, when it is imperative that no overlap is ever missed. If on the other hand you only want to check for overlapping objects at some time or only every couple of seconds, it can be more efficient to just do an overlap check through the physics API. Triggers are set up by attaching a trigger component to a game object. Other Actors ezEngine comes with a couple of additional components that end up as physics actors in the simulation, but have additional functionality for specific use cases. For example the query shape actor and the hitbox component can be used to define hit-boxes and the ragdoll component is used to physically simulate creatures. Character Controller A character controller is a special kind of kinematic actor that has convenience functions to move around a scene, slide along obstacles and slopes, and so on. Character controllers are used as very abstract representations of creatures and players and implement the important aspect of moving and colliding properly throughout a physical scene. See Also Jolt Shapes Jolt Static Actor Component Jolt Dynamic Actor Component Jolt Trigger Component"
  },
  "pages/docs/physics/jolt/actors/jolt-dynamic-actor-component.html": {
    "href": "pages/docs/physics/jolt/actors/jolt-dynamic-actor-component.html",
    "title": "Jolt Dynamic Actor Component | ezEngine",
    "keywords": "Jolt Dynamic Actor Component The Jolt dynamic actor component is used to add physical behavior to an object. Dynamic actors are also referred to as rigid bodies. They are simulated by the physics engine. Kinematic vs. Simulated Dynamic actors can be in one of two modes: fully simulated or kinematic. For a kinematic body, the game code dictates its position and rotation, and the physics engine uses this information to push simulated objects out of their way. Kinematic actors are typically used for elevators, doors and other large pieces that are supposed to push other objects away and strictly follow an animation without any physical simulation of their movement. Non-kinematic, or fully simulated objects on the other hand, are fully controlled by the physics engine. Their position and rotation is determined by forces, such as gravity, acting on them, as well as what other static and dynamic objects they collide with. Setting the position of such an actor has no effect, the physics engine will override the value with its own result. To affect a simulated object, you can apply external forces and impulses. For example the area damage component applies an outward impulse to all rigid bodies in its vicinity to push them away. Whether a dynamic actor is kinematic or not is simply a flag and it is possible to toggle that state back and forth at runtime. This for example allows to animate an object along a predetermined path by making it kinematic at first, and then switch it to simulated at the end of its animation, to make it fall and collide realistically from there on. In the video below a property animation (TODO) was used to do exactly that: Mass vs. Density Dynamic actors have a weight. The weight determines how much force it takes to push them and how much they push other rigid bodies. There are two ways to adjust an actor's weight. If you set the Mass property, this is the bodies absolute weight no matter its size and shape. Thus a small stone with mass 10 (kilogram) will appear heavy whereas a huge boulder also with mass 10 will appear light. The other way is to set its Density property instead. In this case the volume of all the attached shapes is computed and scaled by the density. That means the object's final mass will depend on its scale, so a small stone would get a weight of 0.5 (kilogram) whereas a huge boulder would get a weight of 1000 kg. Using densities is more convenient to get started. The default density often already produces believable results. If you create a prefab that is supposed to be instantiated at various sizes, it is best to use density. Important: Physics engines are notoriously bad at dealing with large mass differences. Objects should never be too light or too heavy in general. Objects with a mass below 1 tend to be flung away at ridiculous speeds when they are pushed by heavy objects. Objects with a mass above 100 should be avoided as well. Due to these limitations, it is not advisable to use realistic weights for objects, as many objects would become too light and their simulation would suffer from erratic behavior. Instead, choose a weight somewhere in the 0.5 to 100 range that looks good enough. Consequently, it can often be easier to specify their value as an absolute Mass, instead of trying to achieve the same through the indirect Density. Center Of Mass The center of mass (COM) is the point in space around which an actor spins when a force is applied to it. The COM is computed automatically from the shapes and their masses. It sometimes ends up too high and makes objects tip over too easily. To adjust the center of mass, enable the property CustomCenterOfMass and edit the CenterOfMass property value. OnContact Reactions TODO Simulation Stability Simulated rigid bodies may not act as desired. Some bodies jitter and don't come to rest, others fly off at high speeds after collisions. Some objects may even tunnel through walls, meaning that instead of colliding properly with a wall, they manage to get to the other side. These are all known issues with real-time physics engines. With the limited available computational power they have to do many approximations to achieve the desired real-time performance. Consequently, you have to be careful how you set up your rigid-bodies, to improve simulation stability: Avoid small and thin objects: Thin objects are always problematic. For small objects, consider making their collision shape as large as possible, potentially larger than the graphical representation. Avoid very heavy and very light objects: See Mass vs. Density above for details. Use Continuous Collision Detection (CCD) for important small objects: Continuous collision detection is mainly used to prevent objects from tunneling through other objects. For example a physically simulated grenade may be thrown at a high speed, which means it is prone to get through walls. This is less likely to happen for larger objects. CCD costs extra performance for every object on which it is used, but significantly reduces the likelihood for tunneling to happen. Increase angular damping: Some objects tend to spin too fast after collisions. By increasing angular damping, you can make them come to rest more quickly. Reduce the complexity of the shape: Especially convex meshes are prone to jittering when the mesh has long thin triangles. Build convex meshes by hand to control their complexity, if an automatically created convex mesh results in unstable behavior. Component Properties CollisionLayer: The collision layer to use. Kinematic: See Kinematic vs. Simulated above. StartAsleep: If enabled, the actor starts in the 'sleeping' state and will not be physically simulated until it gets into contact with another active actor. This is a performance optimization to prevent performance spikes after loading a level. If used badly, an object can float in air and not fall down until something else touches it. Make sure to only use this on objects that are convincingly placed to begin with. Mass, Density: See Mass vs. Density above. Surface: The surface to use for this actor's shapes. The surface determines the friction and restitution during simulation, but also determines what effects are spawned when you interact with the object. Note that collision meshes already specify the surface to use. If a surface is selected on the actor, it overrides the mesh's surface. GravityFactor: Adjusts the influence of gravity on this object. If set to zero, it will float in space. LinearDamping, AngularDamping: The damping properties affect how quickly an actor loses momentum and comes to rest. This can be adjusted separately for positional (linear) movement and rotational (angular) movement. ContinuousCollisionDetection: See Simulation Stability above. OnContact: See OnContact Reactions above. CustomCenterOfMass, CenterOfMass: See Center Of Mass above. See Also Jolt Static Actor Component Jolt Shapes Jolt Constraints"
  },
  "pages/docs/physics/jolt/actors/jolt-queryshape-actor-component.html": {
    "href": "pages/docs/physics/jolt/actors/jolt-queryshape-actor-component.html",
    "title": "Jolt Query Shape Actor Component | ezEngine",
    "keywords": "Jolt Query Shape Actor Component The Jolt query shape actor component is a kinematic actor that doesn't interact with the physics simulation. Its intended use is to define areas that you want to be able to detect via raycasts and overlap queries but otherwise shouldn't interfere with the physical simulation. Example The image above shows a mock-up of a lever that a player should be able to activate. The handle of the lever itself is very thin. If you use a raycast to detect what interactable objects a player is looking at, it can be difficult to hit. If the lever handle itself moves after activation, it would also move away from the point the player is looking at, so quickly activating it again becomes even harder. To make this easier, instead of making the lever into a kinematic object and raycast against that, we can use a sphere shape that has a much larger size and therefore is much easier to pick. However, we do not want this exaggerated shape to interfere with other objects. For example when a character controller moves towards it, we do not want it to block the path. Instead it should just pass through. We could achieve this with collision layers, however the more collision layers you have, the harder it becomes to maintain them properly. By using a query shape actor, it is very easy to define that this object only participates in queries, e.g. raycasts and overlap tests. And even then only, if those queries are configured to include query shapes. Component Properties CollisionLayer: The collision layer to use. Note that query shapes do not collide with anything, but this is used for additional filtering when raycasting against these objects. Surface: The surface to use for this actor's shapes. This is only used to report the surface type back in raycasts and such. Otherwise the surface does not affect the actor's behavior in any way. See Also Jolt Actors Jolt Hitbox Component"
  },
  "pages/docs/physics/jolt/actors/jolt-static-actor-component.html": {
    "href": "pages/docs/physics/jolt/actors/jolt-static-actor-component.html",
    "title": "Jolt Static Actor Component | ezEngine",
    "keywords": "Jolt Static Actor Component The Jolt static actor component is used to represent static collision geomtry. Most geometry in a scene should be static, meaning that it never moves, rotates, scales or is animated in any way. Static geometry is generally faster to process, and in the case of physics simulations, only static actors may use concave collision geometry. All Jolt shapes that can be found in the hierarchy below the static actor are combined to form the compound shape of the actor. However, if any other actor (static or dynamic) is part of the hierarchy below the static actor, the shapes below that object are ignored for this actor. Additionally, if the static actor itself references a collision mesh, it will also become part of the actor compound shape. Only static actors are able to reference concave triangle collision meshes. If you need your geometry to be able to move, use a dynamic actor instead. Component Properties CollisionLayer: The collision layer defines which objects will collide with this actor. CollisionMesh: An optional convex or concave collision mesh representing the static actor geometry. This will be combined with all shapes found in the hierarchy below the owner object. IncludeInNavmesh: If set, this object will be considered an obstacle for AI and navmeshes are generated around it. PullSurfacesFromGraphicsMesh: If this is enabled, at startup the actor will check whether there is a graphics mesh component attached to the same owner, which has the same amount of materials, as the collision mesh. If so, it will query those materials for their surfaces and use them to override the surfaces that are stored in the collision mesh. This can be very convenient, especially for complex meshes, because you only need to set up the materials for the graphics mesh, and don't need to mirror the same setup on the collision mesh. Also modifications to the graphics mesh (or its materials) will then apply to the collision mesh as well. Enabling this option forces the graphics mesh to be loaded at startup and therefore reduces potential for streaming data in the background. Surface: The surface to use for this actor's shapes. The surface determines the friction and restitution during simulation, but also determines what effects are spawned when you interact with the object. Note that collision meshes already specify the surface to use. If a surface is selected on the actor, it overrides the mesh's surface. See Also Jolt Dynamic Actor Component Jolt Shapes Jolt Collision Meshes"
  },
  "pages/docs/physics/jolt/actors/jolt-trigger-component.html": {
    "href": "pages/docs/physics/jolt/actors/jolt-trigger-component.html",
    "title": "Jolt Trigger Component | ezEngine",
    "keywords": "Jolt Trigger Component The Jolt trigger component is a special kind of actor that determines whether other actors overlap with its volume. If so, it sends a trigger event message. Other components or script code can react to this message to implement their game logic. Triggers are often used to open and close doors, to check whether a character walked over a pickup item and to detect when the player reached some location. A trigger is set up the same way as a static actor or a dynamic actor, by attaching collision shapes to it. Which other physics objects activate the trigger is determined through the collision layers on the attached shapes. Since triggers are not simulated like rigid bodies, they don't require much configuration. Triggers can be moved around at runtime and they will fire, when an object enters a trigger because the trigger moved into the object. When a trigger fires, it sends the event message ezMsgTriggerTriggered. The message states which other object was involved, and whether it entered or left the trigger volume. It will also pass along the TriggerMessage string. This can be used to identify which (kind of) trigger was just triggered. Note: Physics triggers only detect overlaps with other physics objects. For such scenarios they are an efficient solution. If, however, you need to query overlaps with other kinds of objects, you should take a look at the spatial system. To achieve more complex trigger behavior, for instance to only activate something after a delay, you can utilize the trigger delay modifier component. Component Properties CollisionLayer: The collision layer to use. TriggerMessage: The string that should be sent along with the ezMsgTriggerTriggered. See Also Trigger Delay Modifier Component Jolt Actors Jolt Shapes Spatial System Marker Component"
  },
  "pages/docs/physics/jolt/collision-shapes/jolt-box-shape-component.html": {
    "href": "pages/docs/physics/jolt/collision-shapes/jolt-box-shape-component.html",
    "title": "Jolt Box Shape Component | ezEngine",
    "keywords": "Jolt Box Shape Component The Jolt box shape component adds a box as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Boxes are relatively efficient for the physics engine to handle. Prefer them over the convex shape component when possible. Component Properties HalfExtents: The width, height and depth of the box shape, from its center position. See Also Jolt Shapes Jolt Actors"
  },
  "pages/docs/physics/jolt/collision-shapes/jolt-capsule-shape-component.html": {
    "href": "pages/docs/physics/jolt/collision-shapes/jolt-capsule-shape-component.html",
    "title": "Jolt Capsule Shape Component | ezEngine",
    "keywords": "Jolt Capsule Shape Component The Jolt capsule shape component adds a capsule as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Capsules are relatively efficient for the physics engine to handle. Prefer them over the convex shape component when possible. For long thin objects, especially static collision geometry, capsules may also be more efficient and yield better results, than box shapes. Component Properties Radius: The radius of the capsule, ie its thickness. Height: The height or length of the capsule. See Also Jolt Shapes Jolt Actors"
  },
  "pages/docs/physics/jolt/collision-shapes/jolt-collision-layers.html": {
    "href": "pages/docs/physics/jolt/collision-shapes/jolt-collision-layers.html",
    "title": "Jolt Collision Layers | ezEngine",
    "keywords": "Jolt Collision Layers Collision layers are a concept used by physics engines to decide which objects should collide with which other objects. For instance, a box thrown into a scene should obviously collide with the scene geometry. There may, however, be things that it should not collide with. For example, you may have triggers that should only be triggered by the player or NPCs, but not by other objects. Collision layers can be used to solve this problem. Collision Layer Matrix Collision layers are evaluated by the physics engine and are therefore the fastest method to reject collisions. Filtering out objects that entered a trigger could be done on a higher level, but having the physics engine filter them out early is best for performance. Due to this, there is a limit of 32 available collision layers. If you require more, you need to rethink your setup and maybe use other methods to handle certain cases. Each layer can be set to collide or not collide with each other layer. Consequently, the available combinations form a matrix of 32x32 entries. Each entry is either enabled or disabled. The matrix is mirrored along the diagonal. If layer A is set to collide with layer B, then of course layer B has to be set to collide with layer A. For convenience, in the editor you can give a name to each layer. The name is irrelevant at runtime, though. Under Project > Plugin Settings > Jolt Project Settings... you can configure the Jolt collision layers, as seen above. Physics Actors and Shapes The collision layer is specified on each actor. All the shapes of a physics actor are in the same collision layer. When two shapes overlap, the physics engine will check their collision layers. If the pair of layers is configured to collide, the two shapes will interact. Otherwise they will pass right through each other. Raycasts and Shape Queries When doing a raycast or other shape query, you also need to provide a collision layer to determine with which physics shapes the ray should collide. Therefore you often need additional collision layers to implement common operations. For example, you may need a layer to handle 'visibility checks'. Those rays should pass through transparent objects, as glass windows should not block the AI from seeing the player. On the other hand, if you want to 'interact' with the scene, you need a different collision layer, one that would not pass through transparent objects but maybe ignore other types of objects. Combinations can quickly add up and therefore you must consider very carefully what filtering must be done by the physics engine already, and what additional filtering could be done in your own code. However, in raycasts and shape queries you can additionally provide other options, for instance to ignore all dynamic or character objects. Therefore, not all filtering functionality has to be exclusively achieved through collision layers. See Also Jolt Integration Jolt Collision Meshes"
  },
  "pages/docs/physics/jolt/collision-shapes/jolt-collision-meshes.html": {
    "href": "pages/docs/physics/jolt/collision-shapes/jolt-collision-meshes.html",
    "title": "Jolt Collision Meshes | ezEngine",
    "keywords": "Jolt Collision Meshes Collision meshes are special meshes that are used by the physics engine to compute physical interactions. Their internal representation is optimized to speed up this task. Additionally, physics engines generally distinguish between two types of meshes: convex meshes and concave meshes. While concave meshes can represent any arbitrary geometric shape, they can only be used for static physics actors, which limits them to be used for the static level geometry. Convex meshes are often an oversimplification of the original mesh. However, they can be used for all physical interactions. Concave Collision Meshes To create a concave collision mesh, use the asset type Jolt Triangle Collision Mesh when importing an asset. The image above shows a mesh imported as a concave collision mesh. As you can see it represents every detail faithfully. Due to this complexity, the model can only be used for static physics actors, meaning you can place it in a level, scale and rotate it, but you may not move it dynamically during the game and it cannot be used to simulate a rigid body. The complexity of a mesh has direct impact on the performance of the game. Especially small details may result in large computational costs when dynamic objects collide with those detailed areas. If you want to optimize performance, you should author dedicated collision meshes with reduced complexity, instead of using the render mesh directly. Concave collision meshes are set directly on the static physics actor component and have no dedicated physics shape component. Triangle meshes can use a different surface for each submesh. Convex Collision Meshes The simulation of dynamic actors is only possible with convex shapes. To create a convex collision mesh, use the asset type Jolt Convex Collision Mesh when importing an asset. To attach a convex mesh to an actor, use the convex mesh shape component. Convex meshes may only use a single surface, even if the mesh is made up of multiple convex pieces. There are multiple modes how to create the convex collision mesh: Convex Hull In the image above the mesh import computed the convex hull. The number of vertices and triangles was also reduced to less than 250 (a requirement by Jolt). Obviously, the mesh lost all of its details and the object will not collide with its surroundings according to its actual geometry, but in many use cases that won't be obvious. This is the most efficient way to use an arbitrary mesh as a collision mesh, as it will always use exactly one, very low poly convex mesh for the physics calculations. Convex Decomposition In the image above the mesh import decomposed the mesh into multiple pieces (seven pieces in this case). Each piece is a convex mesh with less than 250 vertices and triangles. This mode allows you to dictate into how many pieces to split the mesh. The more pieces, the closer the result resembles the original shape. These collision meshes can still be used for dynamic simulation, the Jolt actors simply use multiple convex shapes as their representation. Of course the more pieces such a mesh contains, the less efficient the simulation becomes. Visualizing Collision Meshes Sometimes you want to visualize the collision mesh of an object within a scene. One way is to use the Jolt debug visualizations. However, for some use cases you can also just attach a Collision Mesh Visualizer component. This renders the collision mesh into your scene the same way as in the images above. See Also Jolt Physics Integration Jolt Physics Shapes Jolt Physics Actors Jolt Collision Layers Jolt Debug Visualizations Surfaces"
  },
  "pages/docs/physics/jolt/collision-shapes/jolt-convex-shape-component.html": {
    "href": "pages/docs/physics/jolt/collision-shapes/jolt-convex-shape-component.html",
    "title": "Jolt Convex Shape Component | ezEngine",
    "keywords": "Jolt Convex Shape Component The Jolt convex shape component adds a convex mesh as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Convex mesh shapes are the least efficient shape to handle for the physics engine, but it is also the only shape that allows you to define your own (convex) collision geometry. For many kind of objects this is necessary. The convex shape component references a convex collision mesh, which you need to create first. Note that the editor doesn't visualize convex shape components in any way. The image above was taken by using a collision mesh visualizer component. Component Properties CollisionMesh: The convex collision mesh to use. See Also Jolt Shapes Jolt Actors"
  },
  "pages/docs/physics/jolt/collision-shapes/jolt-cylinder-shape-component.html": {
    "href": "pages/docs/physics/jolt/collision-shapes/jolt-cylinder-shape-component.html",
    "title": "Jolt Cylinder Shape Component | ezEngine",
    "keywords": "Jolt Cylinder Shape Component The Jolt cylinder shape component adds a cylinder as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Cylinders are relatively efficient for the physics engine to handle. However, cylinders are known to not always yield the desired behavior. Generally prefer them over the convex shapes, though if the behavior is too erratic, you can try those instead, and see if the results are better. Whenever possible, prefer to use capsule shapes instead. Component Properties Radius: The radius of the cylinder, ie its thickness. Height: The height or length of the cylinder. See Also Jolt Shapes Jolt Actors"
  },
  "pages/docs/physics/jolt/collision-shapes/jolt-shapes.html": {
    "href": "pages/docs/physics/jolt/collision-shapes/jolt-shapes.html",
    "title": "Jolt Shapes | ezEngine",
    "keywords": "Jolt Shapes A Jolt actor configures how an object behaves in the physics simulation. However, every physical presence also requires to have a 3D shape. The shape of actors is set up using Jolt shape components. Dynamic actors can only be simulated with convex shapes. Therefore concave collision meshes are exclusive to static actors. All shape components represent convex geometry and work with all physics actor types. Shape Components The following shape components are available: Jolt Sphere Shape Component Jolt Box Shape Component Jolt Capsule Shape Component Jolt Cylinder Shape Component Jolt Convex Shape Component Actor Shape Setup The easiest kind of actor shape setup is to simply attach a shape component to the same game object that the actor component is attached to. This way the position of the game object is also the center of the shape, which is often sufficient. For more complex shapes, you can add child nodes below the actor node, attach the shapes to those nodes, and position the nodes as needed. When an actor is initialized for the simulation, it traverses the hierarchy below its owner game object and gathers all shape components. When it encounters another actor component, all shapes below that node are ignored. All shapes that are found this way are added to the actor as one compound shape. This way you can build a single actor that has a complex shape, made up of many parts. You can't add or remove individual shapes during simulation. If you need pieces to be destructible, you need to turn them into separate actors. To still have those actors move in unison, you need to join them using a fixed joint. Friction and Restitution Friction and restitution are the two physical properties that affect a shape's physical behavior the most. See this section for details. See Also Jolt Actors Jolt Collision Layers Surfaces"
  },
  "pages/docs/physics/jolt/collision-shapes/jolt-sphere-shape-component.html": {
    "href": "pages/docs/physics/jolt/collision-shapes/jolt-sphere-shape-component.html",
    "title": "Jolt Sphere Shape Component | ezEngine",
    "keywords": "Jolt Sphere Shape Component The Jolt sphere shape component adds a sphere as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Spheres are very efficient for the physics engine to handle. Therefore you should prefer them over all other shapes, especially convex shapes, when you can approximate the geometry of an object with one or a couple sphere shapes. Component Properties Radius: The radius of the sphere shape. See Also Jolt Shapes Jolt Actors"
  },
  "pages/docs/physics/jolt/constraints/jolt-cone-constraint-component.html": {
    "href": "pages/docs/physics/jolt/constraints/jolt-cone-constraint-component.html",
    "title": "Jolt Cone Constraint Component | ezEngine",
    "keywords": "Jolt Cone Constraint Component The Jolt cone constraint component is a constraint that links two actors in a similar way to a point constraint but additionally constrains the maximum angle that the actors can swing. If additionally the maximum twist around the rotational axis shall be constrained, use a swing-twist constraint. Component Properties Shared Constraint Component Properties ConeAngle: The maximum angle how for the child actor may tilt or swing relative to the parent actor. This sets up a cone shape within which the child actor may be. This can be used to prevent unrealistic rotations for stiff objects. See Also Jolt Constraints Jolt Actors Jolt Point Constraint Component Jolt Swing-Twist Constraint Component"
  },
  "pages/docs/physics/jolt/constraints/jolt-constraints.html": {
    "href": "pages/docs/physics/jolt/constraints/jolt-constraints.html",
    "title": "Jolt Constraints | ezEngine",
    "keywords": "Jolt Constraints A Jolt constraint is a component that links Jolt actors to constrain their movement. Constraints are often also called joints. There are multiple types of constraints that each constrain the actors in different ways. Constraints can be used for simple things like door hinges, up to complex configurations like rag dolls. Joining Actors Constraints can be used to link: Two dynamic actors. This will create a more complex dynamic object that can still freely move throughout the scene, but has multiple parts that can wiggle around. One static actor with one dynamic actor. In this case the dynamic actor is now constrained in its movement relative to the static actor. Since the static actor never moves, the dynamic actor's freedom is severely limited. One dynamic actor with no second actor. This just affixes the dynamic actor with 'the world'. This is effectively the same as joining it with another static actor. One dynamic actor with one kinematic actor. If the kinematic actor moves, the dynamic actor is forced to follow according to the constraint between them. The dynamic actor does not affect the kinematic actor in any way. This setup is common to have full control over the movement of one part (the kinematic actor) and still get physically plausible behavior from another part. There are two ways that a constraint component can link actors. Using Object References You can use the object references on the joint component to specify both the parent actor and the child actor: With this method, the game objects for the actors and the joint don't need to have a specific hierarchy, they can be placed just next to each other. This also allows to create loops of linked actors. On the other hand, you always need to know exactly which actors shall be linked. Note: The Child Actor of a joint always has to be specified this way. Using the Object Hierarchy To determine which Parent Actor to link the joint to, you can utilize the object hierarchy. If no parent actor is specified as an object reference, the joint will automatically traverse its object hierarchy upwards and search for the closest object with a static or dynamic actor. If it finds one, the joint will link to that object as the parent actor. If it does not find any such actor on a parent node, the joint will attach to 'the world', meaning it will be fixed to its current world position. This method can be useful especially when you want to put the joint into a prefab. For example, if you have a prefab for a chandelier that can swing around, you can set up the joint to only reference the actor that represents the chandelier, but the prefab does not contain an actor for the anchor point. If this prefab is placed into a scene, the rule that a joint without a parent actor just gets linked to the world, means that you can easily place these prefabs in a scene, and they will swing around the location where they have been instantiated. However, if your scene contains for example a moveable room, you can also place it there and make sure to attach the chandelier prefab as a child node of the moveable room actor. That means, the joint of the chandelier will now link the chandelier actor with the moveable room actor, and thus whenever the room moves, the chandelier will be physically dragged along as well. Important: This method of linking actors only works properly, if the parent actor is either static or kinematic. It doesn't work for simulated dynamic actors, as the transform update introduces jitter. Constraint Position The position and rotation of a constraint specifies the pivot point. For hinge constraints, point constraints, cone constraints and swing-twist constraints the position of the joint determines around which point the linked actors rotate. For fixed constraints the position still affects how strong forces may become due to lever effects. For distance constraints the joint position determines from where the distance is computed. Only for slider constraints is the position of the joint less relevant, though for computational stability, it should still be somewhere in between the linked actors. Video: Basic Setup Note: In the video we use PhysX joint components, but the concepts are identical for Jolt. Using the ChildActorAnchor Property Using the ChildActorAnchor property is entirely optional and often not needed. Basically this option is used to reference any object, and tell the constraint that the position and orientation of that object should be the anchor point where the constraint acts upon the child actor. For most constraint types this is rarely needed. If you link two actors with a hinge constraint, the position of the constraint already defines where between the actors the hinge will be. However, this setup always assumes that the two actors should start out in the 'default configuration' of the constraint. For example, for a hinge constraint with a stiff spring, the default configuration is when it is fully relaxed, meaning you could not put them into a start configuration where the constraint is already under tension. Similarly, for a distance constraint, the child actor would always need to start out such that the constraint position is exactly at the position of the actor, where it should link to the actor. This can be problematic especially when the distance constraint uses a minimum distance, as the constraint would immediately push away the actor, potentially with high speed. Using a dedicated object to specify the anchor point, both situations can be fixed. The anchor point specifies the position at which the constraint affects the child actor. For most constraint types the rotation of the anchor point is important as well. If the anchor's transform is identical to the constraint's transform, it is redundant and could be left out. However, if it is different from the constraint's transform, this difference tells the constraint how much its start state deviates from the 'default configuration'. For example for a hinge constraint the position of the anchor should always be identical to the constraint's position, otherwise the child actor will jerk towards the hinge constraint at startup. However, the rotation may deviate from the constraint's rotation, which specifies how much the constraint is already rotated at startup. Video: How to use the Anchor Property Note: In the video we use PhysX joint components, but the concepts are identical for Jolt. Shared Constraint Component Properties These properties are shared among all joint types: PairCollision: If disabled, joined actors will not collide with each other. This can be preferable, because then the joined actors may overlap. ParentActor, ChildActor: References to objects with actor components to link with this constraint. See joining actors above. ChildActorAnchor: An optional reference to an object that tells the constraint where it should attach to the child actor. See using the ChildActorAnchor property above. BreakForce, BreakTorque: If either of these values is larger than zero, the constraint is breakable. That means if during the physics simulation the force or torque acting upon the constraint exceeds this threshold, the constraint will be deleted and the bodies won't be joined any longer. When this happens, ezMsgPhysicsJointBroke will be sent as an event message. Constraint Types These types of constraints are currently available: Jolt Hinge Constraint Component Jolt Point Constraint Component Jolt Fixed Constraint Component Jolt Slider Constraint Component Jolt Distance Constraint Component Jolt Cone Constraint Component Jolt Swing-Twist Constraint Component Constraint Stability Joining multiple actors in a chain can quickly result in really bad simulation results. Have a look at dynamic actor simulation stability for ways to improve this. When working with constraints the following aspects help a lot: Make sure that the involved actors are not too light. For smaller objects the automatically computed mass is often too low for the constraint to be stable. Drastically increase linear and especially angular damping on the actors (0.1 to 0.8). Don't create very long chains, try to achieve the desired result with as few constraints as possible. Don't use constraint limits (hinge constraint, swing-twist constraint), they can add significant instability. See Also Jolt Actors"
  },
  "pages/docs/physics/jolt/constraints/jolt-distance-constraint-component.html": {
    "href": "pages/docs/physics/jolt/constraints/jolt-distance-constraint-component.html",
    "title": "Jolt Distance Constraint Component | ezEngine",
    "keywords": "Jolt Distance Constraint Component The Jolt distance constraint component is a constraint that links two actors such that they will keep a minimum and maximum distance. If the actors come closer than the minimum distance, they will repel each other, if they get farther apart than the maximum distance, they will attract each other. The distance joint can be used to fake the behavior of chains and ropes, if no proper simulation and visualization of individual chain links is needed. Component Properties Shared Constraint Component Properties MinDistance: The minimum distance that the two joined actors should keep from each other. If the constraint has no parent actor, the child actor will keep this distance from the constraint position. Note that if the distance constraint does not use the ChildActorAnchor option, a non-zero minimum distance will make the child actor be pushed away right after startup. MaxDistance: The maximum distance between the two joined actors. Without a spring, the two joined actors are unable to get farther apart than this. With a spring, the two actors will be pulled back together with the spring force, when they become farther apart than this. Frequency: Determines how often (per second) the constraint is enforced. Higher values make the constraint stiffer, but can also lead to oscillation. Good values are in range 0.1 to 20. Damping: How much to dampen actors when they overshoot the target position. Lower values make the objects bounce back harder, higher values make them just stop. See Also Jolt Constraints Jolt Actors Jolt Shapes"
  },
  "pages/docs/physics/jolt/constraints/jolt-fixed-constraint-component.html": {
    "href": "pages/docs/physics/jolt/constraints/jolt-fixed-constraint-component.html",
    "title": "Jolt Fixed Constraint Component | ezEngine",
    "keywords": "Jolt Fixed Constraint Component The Jolt fixed joint component is the most simple joint type. It links two actors together, such that they move in unison. If all you want is to merge two shapes, you should just attach them to the same actor. The main use case for fixed joints is to make them breakable. NOTE: Breakable constraints are currently not implemented in Jolt. That means joints can't automatically break due to physical stress. At the moment you can only make joints break by deactivating a constraint programmatically. Component Properties Shared Constraint Component Properties See Also Jolt Constraints Jolt Actors Jolt Shapes"
  },
  "pages/docs/physics/jolt/constraints/jolt-hinge-constraint-component.html": {
    "href": "pages/docs/physics/jolt/constraints/jolt-hinge-constraint-component.html",
    "title": "Jolt Hinge Constraint Component | ezEngine",
    "keywords": "Jolt Hinge Constraint Component The Jolt hinge constraint component is a constraint that links two actors such that they can only rotate around one axis relative to each other. How far the joined objects can rotate can be limited. The hinge can also be powered with a drive, meaning it will rotate on its own with a maximum force. The drive can also be configured to effectively act like a spring, pulling the hinge towards a desired rotation. Component Properties Shared Constraint Component Properties LimitMode: Defines whether the constraint can spin freely, or is restricted by LowerLimit and UpperLimit. NoLimit: The constraint can spin without restriction. HardLimit: The constraint cannot rotate farther than LowerLimit and UpperLimit. If it hits the boundary, it may bounce back. LowerLimit, UpperLimit: The lower and upper allowed rotation angles, if LimitMode is enabled. Friction: How easy it is to rotate the hinge. Higher values make the constraint stiffer. DriveMode: Specifies whether the constraint will apply a force to rotate the actors. NoDrive: The constraint will not rotate on its own. ReachVelocity: The constraint will try to rotate at a speed of DriveTargetValue. ReachPosition: The constraint will try to rotate towards the angle DriveTargetValue. DriveStrength: The maximum force the constraint can apply to try to reach its target. See Also Jolt Constraints Jolt Actors Jolt Shapes"
  },
  "pages/docs/physics/jolt/constraints/jolt-point-constraint-component.html": {
    "href": "pages/docs/physics/jolt/constraints/jolt-point-constraint-component.html",
    "title": "Jolt Point Constraint Component | ezEngine",
    "keywords": "Jolt Point Constraint Component The Jolt point constraint component is a constraint that links two actors with a ball and socket constraint, meaning the actors can rotate around the constraint's pivot point, but cannot move apart. The point constraint is very simple and therefore also quite stable. If you can get away with its limited functionality, prefer to use it over more complex constraints. If the maximum swing or twist angle shall be constrained, use a cone constraint or a swing-twist constraint. Component Properties Shared Joint Component Properties See Also Jolt Constraints Jolt Actors Jolt Cone Constraint Component Jolt Swing-Twist Constraint Component"
  },
  "pages/docs/physics/jolt/constraints/jolt-slider-constraint-component.html": {
    "href": "pages/docs/physics/jolt/constraints/jolt-slider-constraint-component.html",
    "title": "Jolt Slider Constraint Component | ezEngine",
    "keywords": "Jolt Slider Constraint Component The Jolt slider constraint component is a constraint that links two actors such that they can only slide along one axis relative to each other. Optionally, how for the joined actors can slide can be limited. Component Properties Shared Constraint Component Properties LimitMode: Specifies whether the distance of sliding is limited. NoLimit: The actors can slide unlimited far. Since they will still collide with other objects, there may be no need to limit the slide distance through the joint. HardLimit: When the actors reach the end of the joint range, they will be stopped. LowerLimit, UpperLimit: How far the actor can deviate from the start position in either direction. Friction: How easy it is to slide along the constraint. Higher values make the joint stiffer. DriveMode: Specifies whether the constraint will apply a force to push the actors. NoDrive: The slider will not push the actors. ReachVelocity: The constraint will try to push at a speed of DriveTargetValue. ReachPosition: The constraint will try to push towards the relative position DriveTargetValue. DriveStrength: The maximum force the constraint can apply to try to reach its target. See Also Jolt Constraints Jolt Actors Jolt Shapes"
  },
  "pages/docs/physics/jolt/constraints/jolt-swing-twist-constraint-component.html": {
    "href": "pages/docs/physics/jolt/constraints/jolt-swing-twist-constraint-component.html",
    "title": "Jolt Swing-Twist Constraint Component | ezEngine",
    "keywords": "Jolt Swing-Twist Constraint Component The Jolt swing-twist constraint component is a constraint that links two actors in a similar way to a cone constraint but additionally constrains how much the actors may twist around their rotational axis. The swing-twist constraint is more complex and thus also less stable than other constraints. If possible prefer to use a cone constraint or a point constraint. Component Properties Shared Constraint Component Properties SwingLimitY, SwingLimitZ: These two angles make the constraint limit how far the child actor may tilt relative to the parent actor. This sets up a cone shape within which the child actor may be. This can be used to prevent unrealistic rotations for stiff objects. Friction: How easily the constraint rotates. Higher values make the constraint stiffer. LowerTwistLimit, UpperTwistLimit: How much the child actor may rotate around the main axis. See Also Jolt Constraints Jolt Actors Jolt Cone Constraint Component Jolt Point Constraint Component"
  },
  "pages/docs/physics/jolt/jolt-debug-visualizations.html": {
    "href": "pages/docs/physics/jolt/jolt-debug-visualizations.html",
    "title": "Jolt Debug Visualizations | ezEngine",
    "keywords": "Jolt Debug Visualizations To debug physics issues it can be very valuable to visualize certain aspects of the Jolt simulation. Debug visualizations are enabled through CVars. Visualize Jolt Geometry These CVars enable rendering of the Jolt collision geometry: Jolt.Visualize.Geometry: Enables visualization of physics collision geometry. Jolt.Visualize.Exclusive: Disables rendering of regular geometry. Jolt.Visualize.Distance: Configures the distance up to which Jolt geomtry gets extracted from objects. The collision geometry is rendered using these color codes: light blue = static geometry dark blue = kinematic yellow = dynamic bodies green = query shapes (or hitboxes) pink = ragdolls and ropesJolt Rope Component red = soft bodies transparent = triggers Debug Draw Constraints The debug draw CVars enable wireframe overlays which are mainly useful to debug issues with constraints. This visualization has a high performance impact and thus should only be used in very small test scenes. These debug draw CVars are available: Jolt.DebugDraw.Bodies: Enables visualization of physics bodies. Jolt.DebugDraw.Constraints: Enables basic visualization of constraints. Jolt.DebugDraw.ConstraintFrames: Enables more detailed visualization of constraints. Jolt.DebugDraw.ConstraintLimits: Enables visualization of constraint limits. See Also Jolt Integration Jolt Physics Settings Component"
  },
  "pages/docs/physics/jolt/jolt-overview.html": {
    "href": "pages/docs/physics/jolt/jolt-overview.html",
    "title": "Jolt Physics Integration | ezEngine",
    "keywords": "Jolt Physics Integration Jolt Physics is an open source physics engine. It computes the physical interactions between objects using rigid body dynamics. Physics engines are a vital part in most 3D games, to make objects collide and interact with each other convincingly. An important feature are also raycasts and shape queries which are used to detect objects and analyze the state of the world. Enable Jolt Support Support for Jolt is enabled by default on all platforms. It can be disabled in the CMake config. Working with Jolt The most important Jolt functionality is exposed through components, as well as through TypeScript. When you write custom C++ code, you can access the most important functionality, like raycasts and shape queries, through the abstract ezPhysicsWorldModuleInterface, which is implementation independent. If you need to access Jolt features that are not exposed in EZ, you can cast that interface to ezJoltWorldModule and directly work with the JPH::PhysicsSystem. For Jolt details, refer to its documentation. Feature Overview You use components to tell Jolt which objects should be considered for its simulation, and how. In Jolt, objects participating in the simulation are called bodies but in EZ they are usually referred to as actors. How to set up actors is described here. Reading up on actors is the best starting point. Actors are made up of shapes, such as spheres, boxes, capsules and meshes. Shapes are described here. Actors can be physically linked, to constrain their movement. This is how you would set up a door hinge for example. Linking two actors is accomplished using constraints. To make a player or NPC walk through a physically simulated scene, you need something that computes how the character collides with walls, climbs stairs, slides down slopes, and so on. This functionality is provided by a so called character controller. Often games have invisible areas that either need to be reached as a goal, or that activate something. Such areas are called triggers. Several non-Jolt components either use the available physics engine, or even expose new functionality. For example the raycast placement component does a raycast (using the abstract physics interface) and exposes the hit position to the user by moving a linked object there. The area damage component does a shape query and both damages and pushes the found physical objects. Video: How to create a physics object See Also Jolt Physics Jolt Architecture Jolt Actors"
  },
  "pages/docs/physics/jolt/jolt-settings-component.html": {
    "href": "pages/docs/physics/jolt/jolt-settings-component.html",
    "title": "Jolt Physics Settings Component | ezEngine",
    "keywords": "Jolt Physics Settings Component The Jolt physics settings component is used to configure general Jolt simulation options. You can only have one such component in a scene, it is an error to have two or more. If no such component is present, all Jolt settings use default values. Component Properties ObjectGravity: The gravity that is applied to all dynamic actors. This property sets both the direction and strength of the gravity. CharacterGravity: A separate gravity value that is used for characters controllers. In many games the gravity for characters is higher than what's used for regular objects. SteppingMode: The stepping mode determines with what time steps Jolt is updated. This is most relevant when your game doesn't run at a fixed framerate: Variable: Jolt will be stepped every frame with the time delta from the previous frame. This mode will forward any frame rate variations to Jolt unfiltered, which means the time step can vary drastically. This mode has the least overhead, but can also result in an unstable simulation when the framerate varies too much. If your game doesn't use dynamic actors much and you mainly use it for raycasts, character movement and overlap queries, this can be entirely sufficient. Fixed: In this mode Jolt is always stepped with the time delta for the FixedFrameRate. If too little time has passed between frames, the Jolt update is skipped entirely, once the delta has been reached, Jolt is stepped. If the time between two frames is very long, up to MaxSubSteps are done to update Jolt. This mode is the most reliable, producing the most stable and deterministic results, since a variable framerate doesn't introduce any variation in how Jolt is updated. This mode is most suitable when your game runs at a locked framerate. This mode can be problematic, if you do have a variable framerate, especially when a frame can take a very long time. In that case the physics simulation will do up to MaxSubSteps simulation steps to catch up with the passed time. If that is not sufficient, the Jolt update will continue catching up during the next frame. As a result, the speed at which simulated objects move may appear erratic until the simulation has fully caught up with the passed time. SemiFixed: This mode is a compromise between Variable and Fixed. It prefers to use fixed time steps, to achieve good simulation stability. At high framerates it will do shorter update steps, but may also skip the Jolt update until enough time has passed. At low framerates, it will do up to MaxSubSteps per frame, but it will use those to always fully catch up with the time that passed between the frames. FixedFrameRate: The framerate to use for the 'fixed' timesteps. A higher framerate means the simulation will be more stable, but also cost more update steps and therefore performance. MaxSubSteps: The maximum number of simulation steps to do between two frames. This is to introduce an upper bound on the performance cost of the Jolt update during one frame. MaxBodies: For performance reasons, Jolt pre-allocates certain resources once at startup. Therefore you can't have more active bodies than this. The default value should be sufficient for the vast majority of use cases, but if necessary, you can increase the value here. See Also Jolt Integration"
  },
  "pages/docs/physics/jolt/ragdolls/jolt-hitbox-component.html": {
    "href": "pages/docs/physics/jolt/ragdolls/jolt-hitbox-component.html",
    "title": "Jolt Hitbox Component | ezEngine",
    "keywords": "Jolt Hitbox Component The Jolt hitbox component is used to add collider shapes to an animated mesh. The component must be attached next to another component that defines the skeleton to use, for instance an animated mesh component or a skeleton component. It will then use that skeleton to create the physics shapes. Consequently, the configuration of the hitbox shapes is set up through the skeleton asset. The hitboxes are usually used to be able to shoot an animated character. Although they share the collider setup with the Jolt ragdoll component, hitboxes and ragdolls are separate features that can be used independently of each other. For example, while a character is alive, it would use hitboxes, so that raycasts can determine where it would be hit, but it would not use a ragdoll component yet. Once a character dies, a ragdoll component would be activated to make it fall to the ground. The hitbox component could now be deactivated (which also makes sense for performance reasons), since it's functionality may not be needed anymore. Component Properties Query Shapes Only: If true, the shapes that get created act the same way as query shape actors. That means the shapes can be detected via raycasts and other shape queries (e.g. the projectile component will be able to hit it), but otherwise they don't participate in the physical simulation. If set to false, full kinematic shapes are used, which means the shapes will push all dynamic actors aside. This is rarely desired, usually one would rather use a character controller or a single kinematic actor in the form of a capsule to represent the animated mesh, but in some exceptional cases it might be useful. Update Threshold: How often the hitboxes are updated to follow the animation. At 0, they are updated every frame. If perfect alignment with the animation is not necessary, it is better for performance to use a larger time step. See Also Jolt Ragdoll Component Jolt Query Shape Actor Component Skeleton Asset Skeletal Animations"
  },
  "pages/docs/physics/jolt/ragdolls/jolt-ragdoll-component.html": {
    "href": "pages/docs/physics/jolt/ragdolls/jolt-ragdoll-component.html",
    "title": "Jolt Ragdoll Component | ezEngine",
    "keywords": "Jolt Ragdoll Component Note Ragdolls are a work-in-progress feature. They are working, but the exact functionality may change in the future. The Jolt ragdoll component is used to physically simulate limp bodies. Ragdoll Configuration Ragdolls only work with skeletons that have a proper bone collider and joint setup. The most important bones need to have collider shapes. Additionally, bones that should be anatomically connected, need to have joints set up. Bones also must adhere to a physically plausible hierarchy, meaning that leg bones should be child bones of a hip bone, feet bones must be child bones of leg bones and so on. Unfortunately many assets don't strictly follow this rule, which often makes them unsuitable for use as a ragdoll. The ragdoll component works with uniform scaling, so you can create differently sized characters or objects. It does not work with non-uniform scaling. It is common to add a ragdoll component to a character, but set the component to inactive, and only activate the component when the character goes limp. Breakable Objects The ragdoll component can be used for a simple breaking effect. For this you need to build a mesh out of broken pieces and give each piece a bone. In the rest pose the mesh should look like one piece. Now you can use a visual script to determine under what conditions the object should shatter and then activate the ragdoll component. In this case the skeleton only needs to define shapes for the bones, but no joints between them. Thus each fragment will fall individually and the object looks like it breaks apart. Use the properties CenterPosition, CenterVelocity and CenterAngularVelocity to make the pieces fly away more convincingly. Important This feature is only experimental and very limited in functionality. Component Properties SelfCollision: Whether the individual bones of a ragdoll shall collide with each other. If disabled, they will pass through each other and only the joint constraints will prevent unnatural motion. Wether self collision works well or not on a given character highly depends on how the colliders for the bones are set up. StartMode: In which pose the ragdall should start: WithBindPose: The ragdoll starts immediately and uses the default bind pose (or rest pose) of the skeleton. WithNextAnimPose: The ragdoll waits for the next animation pose from and then starts from there. This requires a simple animation component or animation graph to be active. WithCurrentMeshPose: The ragdoll starts immediately with the current pose. This does not require another component to regularly provide new poses and thus can also be used with a skeleton pose component. GravityFactor: How much gravity to use. Mass: How heavy the ragdoll should be. StiffnessFactor: The overall stiffness of the joints. Each joint has an individual stiffness as defined in the skeleton asset, but when scaling characters up or down, it may be necessary to also scale the stiffness. OwnerVelocityScale: A ragdoll may get enabled while a character is moving, for example while it is running. The owner object velocity is then transferred to the ragdoll to have it continue falling into the direction, rather then suddenly stop and just fall down. This factor allows to tweak how much of that momentum to keep (or even exaggerate). CenterPosition: An experimental feature mainly meant for breakable objects (ragdolls with no joints). Specifies an offset where the center of the object should be, to apply an outwards force from. CenterVelocity, CenterAngularVelocity: What linear and angular velocity to set at start outwards from the CenterPosition on each bone. This makes it possible to build breakable objects that break apart when the ragdall gets activated. See Also Skeletal Animations Jolt Hitbox Component Skeleton Asset"
  },
  "pages/docs/physics/jolt/special/jolt-character-controller.html": {
    "href": "pages/docs/physics/jolt/special/jolt-character-controller.html",
    "title": "Character Controller | ezEngine",
    "keywords": "Character Controller A character controller is a special object in the physics engine that is used to move a character throughout a scene and make it collide with other geometry. A character controller is typically an upright capsule that abstractly represents the space that a character occupies. The character controller provides the following functionality: Move throughout a scene, collide with and slide along walls Fall to the ground, slide down steep slopes Climb up shallow slopes Step over small obstacles Climb stairs Jump Stand and crouch with different capsule sizes Push dynamic objects Get pushed by kinematic objects Ride on kinematic platforms On top of these basic features, the character controller implements many details of movement. For example, while jumping or falling, a game may allow the player some degree of control. Such details are very game specific, though, and there is no one-size-fits-all solution. Consequently, the character controller functionality is split up into multiple classes, and you are encouraged to implement your own logic: ezJoltCharacterControllerComponent: A base class for Jolt character controllers. It gives access to the most important functionality and also adds some convenience functionality. ezJoltDefaultCharacterComponent: An implementation of ezJoltCharacterControllerComponent that is provided as an example and as a decent starting point. It implements behavior similar to old-school first-person shooter games, such as Half-Life 2. Depending on how significantly different behavior you want, you can either derive from this class and override some parts, or copy the entire code and rewrite everything as desired. Example The player object is often the most complicated object in a game. The character controller only provides the locomotion aspect, but this is often coupled tightly to the overall game logic. For example, the player may move slower or be disallowed to jump while carrying an object. Many of these aspects can be handled by an overall player logic script. Other aspects, like the details of the characters velocity while sliding down a slope or jumping through the air, have to be implemented directly inside a character controller component. The Testing Chambers sample contains a prefab called Player.ezPrefab, which demonstrates how to build your own player object. The top level node contains a Default Character Controller component. You could replace this with a custom character controller component, to test out entirely different movement behavior. Note that the player object also uses an input component to funnel input into a script, which implements high level game logic, like weapon selection. See Also Character Controller"
  },
  "pages/docs/physics/jolt/special/jolt-cloth-sheet-component.html": {
    "href": "pages/docs/physics/jolt/special/jolt-cloth-sheet-component.html",
    "title": "Jolt Cloth Sheet Component | ezEngine",
    "keywords": "Jolt Cloth Sheet Component The Jolt cloth sheet component simulates a square patch of cloth as it hangs and swings in the wind. It is meant for decorative purposes such as flags but it also interacts with other physics objects. Note that this interaction isn't very precise and prone to tunneling as well as getting tangled up inside dynamic objects. Jolt cloth sheets are affected by wind and contrary to the cloth sheet component they also interact with physics objects and collide with scene geometry. Component Properties Size: The physical size of the cloth sheet in the world. Segments: How detailed to simulate the cloth. CollisionLayer: The collision layer to use. Damping: How quickly the cloth loses energy while swinging. Higher values make it come to rest more quickly, low values make it swing for a longer time. WindInfluence: How strongly wind should make the cloth swing. GravityFactor: How strongly gravity pulls on the cloth. Flags: These define at which corners and edges the sheet of cloth is attached to the world. Material: The material used for rendering the cloth. Make sure to set it to two-sided for cloth that can be seen from both sides. TextureScale: Scale for the texture UV coordinates. Color: An additional tint-color for rendering. See Also Cloth Sheet Component Jolt Rope Component Wind"
  },
  "pages/docs/physics/jolt/special/jolt-grab-object-component.html": {
    "href": "pages/docs/physics/jolt/special/jolt-grab-object-component.html",
    "title": "Jolt Grab Object Component | ezEngine",
    "keywords": "Jolt Grab Object Component The Jolt grab object component enables a character controller to pick up physical items to carry around, drop or throw. The component is typically attached to the same object as the camera component. When triggered, it uses a raycast along its X axis to determine which physical object to potentially pick up. When it finds a non-kinematic dynamic actor, it checks whether a grabbable item component is available. If so, the information from that component is used to determine the best anchor at which to hold the object, otherwise it uses the object's bounding box to approximate a grab point. When it finds a suitable grab point, it attaches a constraint to an object that is specified to be the pivot point (see AttachTo property). That object has to have a kinematic actor and a dummy shape. The joint will then pull the grabbed item towards it and try to align its orientation according to the grabbed anchor. The grabbed item can then be dropped, or thrown away. All actions must be triggered from code, either C++ or TypeScript. The grabbed item still physically interacts with the environment. If such collisions hold the object too far back, the grab object component may decide to 'break' the joint and drop the object. In this case a ezMsgPhysicsJointBroke event message is sent. Component Properties MaxGrabPointDistance: The maximum distance from this object for an individual grab point to be considered as a candidate. CollisionLayer: The collision layer to use for raycasting to detect which object to pick. SpringStiffness, SpringDamping: The stiffness and damping of the internally used constraint. Affects how stiff the object is held. Careful: This also determines how much force the held object can apply to other objects when you push against them. High values mean that the held object can push objects, that the character controller itself may not be able to push. BreakDistance: If the held object deviates more than this distance from the anchor point it is attached to, the hold will break. In this case a ezMsgPhysicsJointBroke event message is raised. Set to zero to disable this feature. AttachTo: A reference to another game object, to which the held object will be attached to. The target object must have a kinematic Jolt actor (and a dummy Jolt shape), such that a constraint can be attached. The reference may point to this component's owner object. However, using a different object allows you to place the held object in a more suitable location. GrabAnyObjectWithSize: If this is non-zero, objects that have no grabbable item component can be picked up as well, as long as their bounding box is smaller than this value. See Also Grabbable Item Component Jolt Constraints Jolt Actors"
  },
  "pages/docs/physics/jolt/special/jolt-rope-component.html": {
    "href": "pages/docs/physics/jolt/special/jolt-rope-component.html",
    "title": "Jolt Rope Component | ezEngine",
    "keywords": "Jolt Rope Component The Jolt rope component is used to physically simulate ropes, cables and chains. Ropes can be attached to walls as decorative elements (cables, wires), or they can even be attached to dynamic physics objects to link them together. This can be used as a gameplay feature. If all you need is a decorative rope, that doesn't react to physical stimuli (except wind), prefer to use a fake rope component, as that has a much lower performance overhead. Setting Up a Rope A rope requires two anchor points between which it hangs. One anchor point is the rope object position itself, for the other one typically uses a dummy game object. The Anchor object reference is used to select which one to use. In the object hierarchy it typically looks like this: The position of the anchors can be moved in the 3D viewport to position the rope as desired. The approximate shape of the simulated rope will be shown as a preview. Use the Slack property to make the rope sag. Run the scene to see the final shape and behavior. Rendering With just the rope simulation component, you won't be able to see the rope, at all. You also need to attach a rope render component to the same game object. Examples The Testing Chambers project contains a dedicated Ropes scene with many examples. Simulation Stability See Dynamic Actors - Simulation Stability. Overall the same guidelines to prevent stability issues apply here as well. Component Properties Anchor1, Anchor2: A reference to an object whose position determines one end of the rope. If only one anchor is specified, the position of the rope component's owner object is used as the other end. Anchor1Constraint, Anchor2Constraint: How the rope is attached to each anchor: None: The rope is not attached to the object, at all, and once the simulation starts, it will fall down at that end. Point: The rope is attached with a point constraint and thus can freely rotate around that end. Fixed: The rope won't be able to rotate around that end point. The orientation of the anchor is used to specified the direction of the rope there. Cone: The rope can rotate around that anchor, but only within a cone of MaxBend opening. Similar to Fixed, the orientation of the anchor defines the starting direction of the rope. If an anchor object is attached to a dynamic actor, the rope will pull that actor. Otherwise, the rope will be fixed at that static location. If the rope is not attached at one or both ends it is free to move away from there. Mass: The total mass of the rope. It will be distributed equally among all pieces. Pieces: How many individual pieces the rope is made up of. More pieces look prettier, but cost more performance and may decrease the simulation stability. Slack: How much slack the rope has. A value of zero means the rope is hung perfectly straight between its anchors. Positive values make the rope sag downwards. Negative values are also allowed, they make the rope hang upside down. This is useful to create a longer rope that shouldn't spawn inside the ground. The rope can thus be placed above the ground and it will simply fall down after creation. Thickness: How thick the simulated rope is. This may be very different from how thick it is rendered. A thinner rope will have more simulation issues, such as tunneling through other geometry. BendStiffness: Determines how hard it is to deform the rope. MaxBend, MaxTwist: These angles restrict how much each individual pieces in the rope can bend or twist relative to its neighboring piece. Low angles mean the rope is very stiff. A very flexible rope would use values above 45 degrees. CollisionLayer: The collision layer defines with which other objects the rope collides. Surface: The surface defines how slippery or bouncy the rope is. GravityFactor: How much gravity affects the rope. SelfCollision: Whether the rope should be able to collide with itself. ContinuousCollisionDetection: If enabled, the physics simulation tries harder to prevent the rope from passing through other objects. This costs additional performance. See Also Fake Rope Component Rope Render Component Jolt Actors"
  },
  "pages/docs/prefabs/prefabs-overview.html": {
    "href": "pages/docs/prefabs/prefabs-overview.html",
    "title": "Prefabs | ezEngine",
    "keywords": "Prefabs When you build a game you need many different objects that you can place in your world. The complexity of objects can range from very simple objects with just one node and component, up to thousands of nodes and components. Some will be placed in scenes, others need to be instantiated dynamically at runtime. These prefabricated objects are commonly referred to as prefabs. Generally, prefabs in EZ are the same as scenes. Therefore the workflow for creating prefabs is mostly identical. Video: How to use Prefabs Templates and Instances When talking about prefabs, you need to distinguish between prefab templates and prefab instances. A prefab template is the \"original\" object. For each prefab there is always exactly one template, which is stored in a dedicated file. When you want to change a prefab, you edit the template. Once you place a prefab in a scene, that object is a prefab instance. You can create as many instances as you want, and they will all reference the same template. The idea of prefabs is, that when you modify the template, all instances will automatically update to reflect these changes. This way you can set up placeholder templates early in your project and start instantiating them right away. Over time you can then flesh out the prefabs, without having to manually update your scenes. Instantiation Process A prefab is basically just a chunk of a level. It contains game objects (entities / nodes) with components. For example a box prefab would contain a single game object, a mesh component, a physics actor component and a physics box shape. Together these make up an object that looks like a box and behaves like a box. Now if you want to have such a box in your scene, you need to add a copy of this structure in your level. To have multiple boxes in your scene you would need to copy the same structure multiple times into the scene, and you need to adjust their positions for each object to end up in the desired place. This is what prefab instantiation is doing for you. It integrates a copy of the prefab template into your scene. Since each object is a full copy, they can then change their state independently. When a prefab is instantiated, it is decoupled from its template, as it is now a separate copy. Therefore, changing the template will not affect existing instances. To have existing instances show the changes that were made to their template, they must be deleted and recreated from the template. For engine prefabs (see below) the editor does this automatically for you, so while editing a scene, changes to prefabs will always show up immediately. However, while you run your scene, instances cannot be replaced, as they would then lose all their simulation state. For editor prefabs (see below), the process of updating an instance to incorporate the latest template changes is more complicated, which is why the editor will only do this upon request. Prefab Types ezEngine distinguishes between two types of prefabs: Editor Prefabs and Engine Prefabs. The difference between the two types is, when and where the prefab templates get instantiated. For editor prefabs, the editor instantiates the prefab template and shows all the nodes and components of the prefab in its scene graph. For engine prefabs, the instantiation happens in the engine runtime, here all the editor sees is a single node with a prefab reference component. The editor has no information about the prefab, other than its position. The image above shows the same prefab instantiated in two ways. One of the barrels is an engine prefab. As you can see in the scene graph, there is only a single node for this object. The other instance is an editor prefab and the scene graph shows the full node hierarchy for it. Engine prefabs are by far the preferred way to use prefabs. The editor needs to handle less state, the prefabs get automatically re-instantiated every time the template is updated and even when you run a scene in an external application like ezPlayer. Therefore, you can modify a prefab and just launch your stand-alone game, without the need to re-export any of your scenes, and the prefabs will show up in their latest state already. Since the prefab is only instantiated (ie. copied into your world) at runtime, this also takes up less space on disk and is generally more efficient. Editor prefabs should generally be avoided, except for very rare, special cases. As you can see in the image above, the entire structure of the prefab is available in the editor scene graph. That means you can modify the instance. For example you could switch out a mesh, change a color or even add or delete nodes. Because of this extra flexibility, the editor cannot delete and re-instantiate a prefab when the template changes. Instead, it has to try to merge both your changes and the changes to the template. This process is much slower, and error prone, which is why the editor never does this automatically. Instead, you have to trigger this manually by selecting Tools > Update Prefabs. The possibility to modify a prefab instance may sound appealing. However, engine prefabs allow you to expose parameters, which you can then set on the prefab reference component. This way you can control exactly which aspects of a prefab you want the user to be able to change. It is possible to convert back and forth between engine prefabs and editor prefabs in the editor, through the context menu entries under Prefabs > .... When doing so, all customizations to a prefab instance will be lost, though. Working with Prefabs In the editor you typically instantiate a prefab by dragging and dropping it from the asset browser into the scene. By default this will create an engine prefab. If you hold Shift before starting to drag the prefab asset, it will be instantiated as an editor prefab. If the prefab has exposed parameters, you can set these on the prefab reference component. Spawning Objects at Runtime Much game functionality requires to spawn objects dynamically at runtime. This is only possible with engine prefabs. Things like the spawn component store a reference to a prefab asset. They can then be triggered at any time to create a new instance of that prefab. Surfaces also reference prefabs, to provide dynamic effects like footsteps or bullet impacts. Prefab Instance Root Node Prefab templates are allowed to have multiple top-level nodes. When a prefab is referenced in a scene, the node that holds the prefab reference component acts as a root node (or group node) for all nodes that will be instantiated from the template. All instantiated nodes will be attached to this parent node. In the prefab asset, this parent node is accessible. By default, new prefabs have a root node called which acts as a proxy node for the node on which the prefab template will be instantiated on. This way you can add components directly to this entity. If you don't need access to this object, you can give the proxy object a different name, then it shows up as a regular game object and will become a child object of the prefab root node, when the prefab is instantiated. All of this is especially important to keep in mind when writing custom (script) code that searches for a node within a hierarchy. You can give a name to the prefab reference node (the one that instantiates a prefab), and therefore find a specific instance of the prefab by that name. From there on, you can continue searching for nodes by name, and thus find a specific sub-node from the prefab template. Another situation where this is important, is when a prefab needs to create components on the prefab instance entity. For example when setting up physical objects, it can be useful to add the dynamic actor component to the node. This way constraints can attach directly to a prefab. For instance, you can have two instances of a box prefab, and add a constraint (or a rope) between them. As long as the dynamic actor exists directly on the prefab-root object, the constraint is able to attach to both boxes. If the actor component were on a sub-node instead, the constraint would not be able to find it, and wouldn't work. See Also Scene Editing Spawn Component Exposed Parameters Object References"
  },
  "pages/docs/projects/data-directories.html": {
    "href": "pages/docs/projects/data-directories.html",
    "title": "Data Directories | ezEngine",
    "keywords": "Data Directories Data directories are a vital concept in ezEngine. They define where on disk the game data resides, which data is accessible to your project and how. Data directories can be used to organize your assets and share content across multiple projects. For the typical use case, you can treat a data directory simply as a folder, which is mounted in your game, making all files in that folder accessible. Your game can mount an arbitrary number of data directories. Files outside of data directories are not accessible by the game engine. The editor displays all available assets from the mounted directories in the asset browser. When browsing for a file, the editor actively refuses to use files that are not inside a mounted data directory. Otherwise this file would not be readable by the engine. The engine only operates with relative paths, it never stores absolute file paths. Thus, the data directory can be moved to a different location. This is what makes it easily possible to copy the game to a different computer in a different location. As long as the startup code is able to locate the data directories, the rest of the engine will work without a change. Mounting Data Directories Using the Editor Typically you should use the editor to configure which folders will be accessible as data directories. Go to Project > Project Settings > Data Directories...: Here you can add, remove and reorder them. Setting up the data directories of a project should be one of the very first steps after creating a new project. You may notice the >sdk and >project prefixes. These are special directory indicators, meaning those paths are relative to the SDK directory (where the engine is installed) or the project directory (where your project is located). These indicators are automatically inserted when you browse for a folder. They enable the engine to find the same folder again, even if you move the project, or you need to stream the data using FileServe (TODO). From Custom Code You can also mount data directories from code. Usually you would do this either in a custom application (TODO) class or in a game state. Either way, you should do this early at game startup. For more more information, see the filesystem documentation. See Also FileSystem FileServe (TODO) Projects Application (TODO) Game States"
  },
  "pages/docs/projects/plugin-selection.html": {
    "href": "pages/docs/projects/plugin-selection.html",
    "title": "Plugin Selection | ezEngine",
    "keywords": "Plugin Selection Plugins are used to add functionality both to the runtime (the actual game) as well as to the editor. For example, sound, physics simulation, AI and GUI functionality are all added through plugins. Which of these features are required, of course, depends on the project. Therefore the set of active plugins is configured for each project. Modify the Set of Active Plugins Under Project > Plugin Settings > Plugin Selection you can open the plugin selection dialog: The list on the left lists all available plugins. Using the Template dropdown at the top, you can choose a default selection. The right-hand side shows a short description of the selected plugin. Enable the checkmark for a plugin to make it active in this project. NOTE: Some plugins are mutually exclusive. In such a case, once one of them is activated, the others are greyed out. Similarly, some plugins depend on each other, in which case the dependent plugin is automatically enabled and can't be deselected. 'Enable Reload' If you Enable Reload for a plugin, the engine will load a copy of the runtime plugin DLL, rather than the original DLL. This allows to edit and compile the DLL while the editor is running. Typically, this should only be active for a single, game-specific plugin. See Hot Reloading C++ Game Plugins in the Editor for details. Custom Plugin Entries If you have a custom plugin that should show up in this list, you need to add an ezPluginBundle file. ezPluginBundle Files Each ezPluginBundle file contains information for one plugin. Since the editor and the engine runtime are different processes, though, it is possible that one logical plugin actually consists of multiple DLLs. For example it is common that one DLL must be loaded into the runtime process (the actual game), another one into the editor process to add custom UI elements and a third one into the editor engine process for custom, editor-specific rendering. How ezPluginBundle files work is documented inside this file: Code/EditorPlugins/Assets/EditorPluginAssets/Assets.ezPluginBundle The editor searches for ezPluginBundle files in the binary folder where Editor.exe is located. All files that are found add an entry to the dialog. Typically the original ezPluginBundle file is stored next to the source code of a plugin and copied to the binary folder in a post-build step. Examples for this can be found in the CMakeLists.txt files of existing plugins, for example in Code/EnginePlugins/XBoxControllerPlugin/CMakeLists.txt: add_custom_command(TARGET ${PROJECT_NAME} POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${CMAKE_CURRENT_SOURCE_DIR}/XBoxController.ezPluginBundle\" $<TARGET_FILE_DIR:${PROJECT_NAME}> WORKING_DIRECTORY ${CMAKE_SOURCE_DIR} ) NOTE: An ezPluginBundle file is automatically created and referenced for you, when you use the C++ Project Generation. See Also Engine Plugins C++ Project Generation Hot Reloading C++ Game Plugins in the Editor Sample Game Plugin"
  },
  "pages/docs/projects/project-settings.html": {
    "href": "pages/docs/projects/project-settings.html",
    "title": "Project Settings | ezEngine",
    "keywords": "Project Settings Project settings are options that are configured once and affect the entire application. Most project settings can be configured from the editor, though some (currently) can only be configured by writing certain configuration files manually. In the editor you will find the project settings under Project > Project Settings > .... Data Directories Project > Project Settings > Data Directories... opens a dialog to set up the data directories. Input Configuration Project > Project Settings > Input Configuration... opens a dialog to configure input actions. Tags Project > Project Settings > Tags... opens a dialog to configure which tags are available in the project. Window Configuration Project > Project Settings > Window Configuration... opens a dialog to configure the default window configuration when running a scene. These settings allow you to configure basic window settings for Play-the-Game mode and when running an exported scene in ezPlayer. A proper game would typically implement this logic in a custom application (TODO) and should allow the user to choose settings such as the resolution. The window configuration dialog is mainly for use during development. There are two separately stored configurations: Project Default: This configuration will be stored in the project folder and thus should be checked into source control to be shared with others. User Specific: This configuration is only stored locally for the active user and not in the project directory. Therefore it cannot be checked into source control. It is meant for users who want to use settings different from the project default. For instance, when you have multiple monitors, you may want the exported scene to always appear on a specific one. This configuration must be enabled to override the default one. Apart from the window position and size, the window configuration also controls the behavior of the mouse. If Clip Mouse Cursor is enabled, the mouse won't be able to leave the window area. This should be preferred for games that hide the mouse and only use relative mouse movement. Asset Profiles Project > Project Settings > Asset Profiles... opens a dialog to edit asset profiles. Plugin Selection Which optional functionality should be available is configured on a per-project basis. See the plugin selection chapter for details. If plugins provide additional editor options, they are typically found under Project > Plugin Settings > .... Loading Plugins from Code A custom application (TODO) or game state can load plugins directly from code if necessary. For example the ezInspectorPlugin is automatically loaded for you by stock EZ applications, when building the code for development. FMOD If the FMOD Integration is enabled, Project > Plugin Settings > FMOD Project Settings... will be available to configure the speaker mode and which master sound bank to use. Jolt If the Jolt Physics Integration is enabled, Project > Plugin Settings > Jolt Project Settings... will be available to configure the collision layers. Video See Also Projects Plugin Selection Data Directories"
  },
  "pages/docs/projects/projects-overview.html": {
    "href": "pages/docs/projects/projects-overview.html",
    "title": "Projects | ezEngine",
    "keywords": "Projects The term project refers to one game or application, its global settings, and all its data. The editor distinguishes between projects, and allows you to configure various options of each one. On the runtime side, however, the concept of a project does not exist, the current state of the runtime represents the project. Therefore, when you switch to a different project in the editor, the engine will in fact be shut down completely and restarted with different settings (editor and engine are two separate processes). Video: How to create a project Creating a Project You can use ezEngine entirely without the editor. In that case, you do not need to create a project, at all. Your application (TODO) is your project and you set up things like the fileSystem, the plugins and so on, entirely from code. It is more convenient, though, to maintain your project through the editor. To create a new project, open the editor's dashboard (Project > Show Dashboard) and select New from the top-right corner: The dialog will ask you to select a new folder for your project: The name of the folder represents the name of your project. This name is stored nowhere else, you can rename your project later simply by renaming the folder. Basic Setup Now you have a new, blank project. The first thing you should do is to check the project settings. Specifically, if you want to share assets between multiple projects, you need to put those assets into a dedicated folder and then add that folder to your project as a data directory. The second thing you should check is which plugins you want to use, so that you have all desired features available. Create a Scene Select File > Create... and create a document of type ezScene. The new scene will be filled with some default objects and you should see something like this: If you don't see the asset browser, make sure to open it. You can now edit your scene. When you need more assets to play with, you need to import them into your project. Once you have something in your scene that could do something, you can test your scene. A good starting point for that is to simply attach a Rotor component to a mesh. A fun next step is to let objects fall down using physics (hint: you need a Dynamic Actor component and a Box Shape component) Project-wide options Plugins may add project wide options. Not all options may be exposed through editor UI, there are a few things that can (at the moment) only be configured through config files or directly from code. Most options are stored in OpenDDL format or other human-readable files, and you can edit them directly. Some options to be aware of are: Data directories Engine plugins Collision layers Input Configuration Tags Window Configuration Asset profiles See Also Data Directories Plugin Selection"
  },
  "pages/docs/projects/tags.html": {
    "href": "pages/docs/projects/tags.html",
    "title": "Tags | ezEngine",
    "keywords": "Tags A tag is a semantic label. On its own a tag has no functionality. Tags are grouped in categories. When an object supports tags, that means it allows you to add or remove all the tags from one specific category. Tags are mainly used by game objects (ezGameObject). Game objects use the tag category Default. Apart from a handful of built-in tags, you are free to add your own tags. Tags are mainly used for searching and filtering. A common use case is to query the game world for objects in an area, and only return those that have or don't have certain tags. For example you can add a tag 'usable' and then mark up all objects in your game that the player may interact with. This can then be used to highlight all usable objects in the player's vicinity. In the image below the selected game object only has My Custom Tag assigned, all other tags are unassigned. Tags Configuration Project > Project Settings > Tags... opens a dialog to configure which tags are available in the project. Using New Category you can set up multiple tag categories. However, which categories are used where, is defined by code. Game objects for example, use tags from the Default category. Unless you have custom components which use a different tag category, there is no use in adding a new category. Important: Be careful how you name your tags, they cannot be renamed. Once tags are used and saved in scene files, you would need to manually modify scene and prefab files to change their name. Object Visibility Tags are used by the renderer to decide which objects to include or exclude from certain views. For this, camera components have two properties: IncludeTags and ExcludeTags. If any include tag is set on the camera, it will only show objects that have at least all those tags set. If any exclude tag is set, the camera will not render any object that has at least one of those tags set. You can use this to make objects invisible in certain views and only render them in certain other views. This mechanism is also used to decide which objects cast shadows. Only objects with the CastShadow tag will be included in the shadow map generation. Note: The editor uses its own render view, for which you currently cannot edit the include and exclude tags. You have to launch Play-the-Game mode for the engine to render the scene using your custom camera setup, to see the final result. You can, however, select a camera object and look at its preview window, to see the effect of the tag setup. The image below shows two objects. The blue object on the left has the Invisible tag set, as well as the CastShadow tag removed. The selected camera object has the Invisible tag set as an ExclusionTag. Both the red and the blue object are visible in the editor, but the camera preview in the top left of the viewport only shows the red object. Limitations Tags are always set directly on one object. They are not inherited to child objects and usually the code that evaluates tags does not check parent objects for their tags. Thus, to exclude an object from a view, the tag has to be set directly on the render object. Setting it on parent node or a prefab instance, for example, won't work. Similarly, if you want to make an object invisible in one view, you probably also need to disable shadow casting on it. If, however, it should be visible in another view, it cannot cast a shadow in that view, as the shadow maps are shared between the views. See Also Project Settings Jolt Physics Integration Lighting"
  },
  "pages/docs/runtime/application/application.html": {
    "href": "pages/docs/runtime/application/application.html",
    "title": "Application | ezEngine",
    "keywords": "Application See Game States The classes ezApplication, ezGameApplicationBase and ezGameApplication are fully functional, but currently undocumented. See Also Common Application Features Game States"
  },
  "pages/docs/runtime/application/common-application-features.html": {
    "href": "pages/docs/runtime/application/common-application-features.html",
    "title": "Common Application Features | ezEngine",
    "keywords": "Common Application Features All applications that are built on top of ezGameApplication provide a number of useful features for developers. In-game console Press F1 to toggle the in-game console. See its documentation for further details. Reload Resources Press F4 to instruct the engine to reload all resources (TODO). This can be useful, if for example, you are working on a shader and want to see the result of your changes inside the game, without having to restart the game. Reloading resources works for all assets that are used directly by the engine (e.g. Shaders). For some assets it will work, after the editor processed the input assets. For example for textures and materials and many others. Some resources, though, are not reloadable, e.g. things that get instantiated at runtime, such as prefabs. Show Frames Per Second Press F5 to toggle the display of the FPS counter. Take a Profiling Capture Press F8 to take a capture of the profiling data. See the profiling documentation for details. Take a RenderDoc Capture Press F11 to take a RenderDoc capture. See the RenderDoc integration documentation for details. Take a Screenshot Press F12 to take a screenshot. Screenshots are stored in the appdata data directory (see the log output). On Windows this can be found by typing %appdata% into the Windows Explorer. See Also Application (TODO) ezPlayer Profiling RenderDoc Integration"
  },
  "pages/docs/runtime/application/game-state.html": {
    "href": "pages/docs/runtime/application/game-state.html",
    "title": "Game States | ezEngine",
    "keywords": "Game States Most game code is implemented by writing custom components. However, components always work in the context of an object, be it a single game object or an entire prefab. The most that a single component can be responsible for, is to do high level logic for a level, by acting as a global message handler. However, for a full game you need a layer of control that is outside the world, where you can do logic like what level to load, what to do when the player dies or reaches their goal, how to display a main menu for the game settings and level selection, and so on. Most of these things would be possible with world components alone, but it would be cumbersome. Especially switching from one level to another is difficult, if some of your overall game logic has to be transitioned as well. Game States are this extra layer. A game state sits between the application (TODO) layer and the world. A game state is in so far optional, in that the engine will create an instance of ezFallbackGameState, if no custom game state is available. The diagram above shows, that the editor skips the game state in simulate mode. In practice that means that the editor will not allocate any game state when the scene is being edited or only simulated. Only when you enter play-the-game mode, will it create a game state, which can then take over full control for the windows, input and the main camera. Game State Responsibilities The typical things that a game state controls are: Spawning one or multiple windows Setting up the main render pipeline Creating a world and loading a scene into it Unless it's run in the editor, where it is handed an existing world Setting up input devices and bindings Processing main input (not component specific input) Setting up and controlling the main camera Spawning the player prefab Displaying game UI Providing a main menu Saving and restoring global state (progression, high-scores, etc) For example when you have a player start point component in a scene, the component itself doesn't do anything, it just holds some data. Instead, when you enter play-the-game mode, the active game state can (but is not required to) use the information from these components to spawn a player prefab. Similarly, most scenes have a camera component whose usage hint is set to 'Main Camera' (this may be part of the player prefab). This camera defines what part of the scene will be shown on screen. At least that's how it appears. In reality it is the game state that controls the camera for the main render target. It's simply a feature of the ezFallbackGameState, that it searches the world for an appropriate camera component and applies that to the main camera. If it doesn't find any such camera component, it provides simple WASD fly-camera controls. You can even cycle through the different camera components in a scene using Page Up and Page Down. As you can see, by implementing a custom game state, you can gain control over many things that otherwise appear to be built-in. Game State Instantiation It is the responsibility of the ezGameApplication to instantiate a game state. By default this is done right at application startup, but if you write your own application (TODO) you could handle this differently. For example the editor only instantiates the game state for play-the-game mode. The application knows what game states are available through the reflection information. When a game state is needed, all available ones are instantiated and 'asked' (via ezGameStateBase::DeterminePriority()) whether it is the right one to handle the situation. The game state that is the best fit will be kept and gets control. The idea is, that there are typically only few game states available anyway. Usually you have the built-in ezFallbackGameState and then you have one other game state implementation from your custom game plugin. The latter one will take precedence. You could have multiple game states, for example when you have multiple game plugins loaded simultaneously, but then they would need to somehow figure out which one should get activated (e.g. through command line arguments). Sample Game State For an example game state have a look at the Sample Game Plugin. The game state in the sample derives from ezFallbackGameState to benefit from its utilities, such as the default fly camera. In fact, it is a good idea to look at the source of ezFallbackGameState as an example, as well. For all the details, read the API Docs for ezGameStateBase. See Also Custom Code Application (TODO)"
  },
  "pages/docs/runtime/configuration/actor-system.html": {
    "href": "pages/docs/runtime/configuration/actor-system.html",
    "title": "Actor System | ezEngine",
    "keywords": "Actor System The actor system is fully functional but currently undocumented. The actor system is about creating windows and render targets for the right hardware. It's name may sound more interesting, than it actually is :D See Also"
  },
  "pages/docs/runtime/configuration/interfaces.html": {
    "href": "pages/docs/runtime/configuration/interfaces.html",
    "title": "Singleton Interfaces | ezEngine",
    "keywords": "Singleton Interfaces Singletons are classes of which there should only be a single instance throughout the lifetime of the process. Although EZ uses the singleton pattern quite extensively for built-in classes, such as ezTaskSystem and ezResourceManager, those classes don't use dedicated singleton infrastructure. Instead, they only expose static functions, and there is no need for any instance. Accessing such singletons is trivial, as you can always call their functions directly. However, there is another type of singleton, which does require special handling. There are cases where you want to define an interface to make certain functionality available, but you may have different implementations. Only one implementation should ever be active, though. Concrete examples are the integrations of third party libraries. For example there is an ezFrameCaptureInterface. This class defines an interface through which ezGameApplicationBase can do a capture of the rendered frame, which can be used for debugging graphics issues. However, how such a frame capture could be taken, depends on the platform, the installed tools, the used graphics API and so on. This functionality may be available or not and the exact implementation that is needed can differ drastically. Therefore, we want to be able to dynamically load the necessary implementation and make it available through the abstract interface. For the ezFrameCaptureInterface we have an implementation by our RenderDoc integration. In the future we might have a second implementation for PIX or some other platform specific tool. Using the singleton infrastructure, we can simply load an engine plugin that contains an implementation, and from that plugin register our implementation for that interface. Other code can then query for an instance of this interface and, if available, use it without knowing anything about the implementation, and without the need to link against that library. Implementing Singletons This section shows all the pieces needed for a singleton. You can find the sample code in the Sample Game Plugin. Interface Base Class First, you need to have a virtual base class that declares the actual interface. /// \\brief Pure virtual interface for demonstrating the singleton work flow /// /// This declaration would typically be in a shared location, that all code can #include class PrintInterface { public: virtual ~PrintInterface() = default; virtual void Print(const ezFormatString& text) = 0; }; This is the class through which other code will later access the functionality, so it must be in a shared location. Interface Implementation Next, you need one or more implementations of your interface. You can, of course, have zero implementations, if all you want to provide is the option for future extensibility, and your code should generally be able to handle the fact that no implementation is currently loaded. /// \\brief Implementation of the PrintInterface, just forwards the text to ezLog::Info() /// /// This would typically be in a different plugin than the interface and would be allocated by that plugin on startup. class PrintImplementation : public PrintInterface { EZ_DECLARE_SINGLETON_OF_INTERFACE(PrintImplementation, PrintInterface); public: PrintImplementation(); virtual void Print(const ezFormatString& text) override; private: // needed for the startup system to be able to call the private function below EZ_MAKE_SUBSYSTEM_STARTUP_FRIEND(SampleGamePluginStartupGroup, SampleGamePluginMainStartup); void OnCoreSystemsStartup() { /* we could do something important here */ } }; Note the EZ_DECLARE_SINGLETON_OF_INTERFACE macro. This adds one part of the required functionality. For one, this class adds a function to query the one and only instance of your class (GetSingleton()). Also, it prevents you from creating two instances of this class, as that would violate the singleton contract. Finally, you need to add this to you cpp file: EZ_IMPLEMENT_SINGLETON(PrintImplementation); PrintImplementation::PrintImplementation() : m_SingletonRegistrar(this) // needed for automatic registration { } The macro again inserts vital code for your singleton to work. The constructor also has to follow the pattern shown above. You can now implement the desired behavior for the overridden functions. Instantiating Singletons The EZ singleton infrastructure does not automatically create an instance of singleton classes. It is up to you whether, when and how you create your instance. The most common way to do this, is to leverage the startup system to hook into the engine startup process at the right time. For details, read that chapter, but here is what you would typically do. At startup you instantiate your singleton implementation: ON_CORESYSTEMS_STARTUP { // allocate an implementation of PrintInterface s_PrintInterface = EZ_DEFAULT_NEW(PrintImplementation); s_PrintInterface->OnCoreSystemsStartup(); s_PrintInterface->Print(\"Called ON_CORESYSTEMS_STARTUP\"); } And at shutdown you make sure to clean it up again: ON_CORESYSTEMS_SHUTDOWN { s_PrintInterface->Print(\"Called ON_CORESYSTEMS_SHUTDOWN\"); // clean up the s_PrintInterface, otherwise we would get asserts about memory leaks at shutdown s_PrintInterface.Clear(); } Accessing Singletons There are two ways that you can access your singleton instance. In a piece of code that knows for certain that it will only run in conjunction with a specific singleton implementation, you can access it directly: PrintImplementation::GetSingleton()->Print(\"Called ON_HIGHLEVELSYSTEMS_SHUTDOWN\"); This is the most efficient way. However, use cases for this should be relatively rare. The more common situation is, when you want to get the implementation for an interface. To do so, you need to go through ezSingletonRegistry: ezSingletonRegistry::GetSingletonInstance<PrintInterface>()->Print(\"Called ON_HIGHLEVELSYSTEMS_STARTUP\"); Here we don't need to know anything about the implementation and therefore have no link dependency on the library that provides it. This is how most code would access a singleton implementation. Be aware that this requires a more expensive lookup, so locally cache the result, if you want to do multiple function calls on it. See Also Startup System Engine Plugins"
  },
  "pages/docs/runtime/configuration/startup.html": {
    "href": "pages/docs/runtime/configuration/startup.html",
    "title": "Startup System | ezEngine",
    "keywords": "Startup System Initializing an engine and shutting it properly down again, is a surprisingly difficult task. There are many steps involved, some of which have hard requirements on their ordering. Also, some functionality can only be initialized when at least a window, and potentially even a graphics API is available, which is not the case for command line tools. Once plugins are added to the mix, which can be loaded and unloaded at any time, it becomes impossible to manually set up this process. Therefore, EZ uses a dedicated startup system, to handle this complexity automatically for you. Startup System Concept The concept of the startup system is simple. For every 'thing' in the engine you write code how to initialize it and shut it down again. 'Things' in the startup system are referred to as subsystems. You then define what other subsystems you depend on, so that your startup code should run after your dependencies, and your shutdown code should run before your dependencies. All of this is then (automatically) given to the startup system, and when it comes time to fully boot up the engine, that system sorts all subsystems by their dependencies and executes them in the right order. Conversely, it executes all shutdown code in the reverse order. Two Phase Startup A lot of code can be initialized easily in all applications. However, some code strictly requires a window or graphics API to work with and could never be initialized successfully in a command line application. Therefore, the startup system splits the engine initialization into two phases: core systems startup (phase 1) and high level systems startup (phase 2). For command line applications, we would only ever run phase 1. In a proper game, we would first run phase 1, then create our window and rendering API and finally run phase 2. This way, when we don't need things like a renderer or the input system, we simply exclude all high level systems from being initialized. Dependencies Some subsystems depend on other subsystems to be initialized. Therefore the startup system requires you to provide a name for every subsystem and also a group. The name can be arbitrary but has to be unique. The group name obviously does not need to be unique, as multiple subsystems can be part of the same group. When you declare a dependency on another subsystem, you can then either specify it by its direct name, or you can also just declare a dependency on an entire group. The latter is very common, as it is often easier, and you rarely have very strict dependencies on a single subsystem. Startup System Usage In practice, to use the startup system, you need to add a block of code to some cpp file. You can copy this code from Foundation/Configuration/Startup.h and then just fill out the parts that you require. An example is given in the Sample Game Plugin: // clang-format off EZ_BEGIN_SUBSYSTEM_DECLARATION(SampleGamePluginStartupGroup, SampleGamePluginMainStartup) // list all the subsystems that we want to be initialized first BEGIN_SUBSYSTEM_DEPENDENCIES \"Foundation\", // all subsystems from the 'Foundation' group (this is redundant, because `Core` already depends on `Foundation`) \"Core\" // and all subsystems from the 'Core' group END_SUBSYSTEM_DEPENDENCIES ON_CORESYSTEMS_STARTUP { // allocate an implementation of PrintInterface s_PrintInterface = EZ_DEFAULT_NEW(PrintImplementation); s_PrintInterface->OnCoreSystemsStartup(); s_PrintInterface->Print(\"Called ON_CORESYSTEMS_STARTUP\"); } ON_CORESYSTEMS_SHUTDOWN { s_PrintInterface->Print(\"Called ON_CORESYSTEMS_SHUTDOWN\"); // clean up the s_PrintInterface, otherwise we would get asserts about memory leaks at shutdown s_PrintInterface.Clear(); } ON_HIGHLEVELSYSTEMS_STARTUP { // we can query 'an implementation of PrintInterface' through the ezSingletonRegistry ezSingletonRegistry::GetSingletonInstance<PrintInterface>()->Print(\"Called ON_HIGHLEVELSYSTEMS_STARTUP\"); } ON_HIGHLEVELSYSTEMS_SHUTDOWN { // we could also query 'the one instance of the PrintImplementation singleton' PrintImplementation::GetSingleton()->Print(\"Called ON_HIGHLEVELSYSTEMS_SHUTDOWN\"); } EZ_END_SUBSYSTEM_DECLARATION; Here we give our subsystem the name SampleGamePluginMainStartup and we put it into the group SampleGamePluginStartupGroup. Both names could be used by other subsystems to reference this as a dependency. We then specify that this subsystem should be initialized only after all the Foundation and Core subsystems have been booted. Both groups contain many subsystems, but we don't need to care about those details. Now when the application starts running, at some point it will run all the ON_CORESYSTEMS_STARTUP code blocks (in a sorted order). Here, we use that hook to set up our singleton. Later, the game will execute the ON_HIGHLEVELSYSTEMS_STARTUP block, and at shutdown it will first execute ON_HIGHLEVELSYSTEMS_SHUTDOWN and finally ON_CORESYSTEMS_SHUTDOWN shortly before the application closes. Command line applications would not execute the high level startup code blocks. Also, when a plugin is loaded or unloaded, the system ensures to call all the right startup and shutdown functions for subsystems from those plugins. How to know about dependencies A practical problem you may be wondering about, is how you would know the names of potential subsystem dependencies, or how you would even know what subsystems exist. In practice, this is rarely a problem. Most subsystems only depend on the Foundation or the Core group of subsystems. If you have any other dependencies, you are typically quite aware of them, and know where in the code they are set up and thus, where you can look up their names. However, you can also use ezInspector to discover all the available subsystems, their names, and see what other subsystems they depend on: See Also ezInspector Singleton Interfaces Sample Game Plugin"
  },
  "pages/docs/runtime/filesystem.html": {
    "href": "pages/docs/runtime/filesystem.html",
    "title": "FileSystem | ezEngine",
    "keywords": "FileSystem Accessing files is one of those things that you probably need to do very early in your project. Although the POSIX functions fopen, fread, etc. are available on most platforms, they are not always the best choice. ezEngine provides multiple layers of abstractions for accessing files, or more generally, data streams that look like files. Overview The following is a list of abstractions that you should be aware of. Each one will be explained in more detail below. Streams (ezStreamReader, ezStreamWriter) are the basis for all reading and writing of abstract data streams. These are the two classes that you will use whenever something that acts like a file is passed around, even though it does not need to represent an actual file. Low Level File Abstraction (ezOSFile): This is the abstraction layer that implements reading and writing to actual files on disk in a mostly platform independent manner. This is what you typically should not need to use, at all. However, when something is not possible through the higher level abstractions, you might need to do it through this interface. High Level File System (ezFileSystem, ezFileReader, ezFileWriter): Through ezFileSystem you can configure a virtual file-system. ezFileReader and ezFileWriter should be what you typically use to open 'files' in that virtual file-system. ezFileReader and ezFileWriter implement the respective stream interfaces ezStreamReader and ezStreamWriter, so once you have opened a file through these classes, you can pass them to any function that works on streams. Streams A 'stream' is simply a series of bytes. This data stream can come from a physical file, through a network or it can be generated fully procedurally. Streams typically come in two forms: Ones that you can read from and ones that you can write to. In ezEngine those two types of streams are represented with the interface classes ezStreamReader and ezStreamWriter respectively. The stream interfaces are reduced in functionality to what all types of data streams can provide. Reading is always done from the current read position. Writing will always append data at the end of the stream. You cannot seek to an arbitrary position (when reading you can skip ahead) and you cannot re-read the same data or overwrite previously written data. Although this can be limiting in a few scenarios, it is absolutely sufficient for the vast majority of use cases. For the few cases where you really need to set the read or write position, you can revert to another abstraction layer, e.g. on an ezOSFile you have more control. Inside the core engine we typically do not care about files, at all. Therefore you will not find many functions that take a file-name string. In the few cases that this is so, those functions are usually just convenience functions that internally open a file and then pass along the file as a stream. Instead most engine functions work directly on streams. This decouples the engine from working on the concept of files and enables to work on data from any kind of source. For example, this makes it easy to embed a file in an archive or to read it from a network, or even both combined. Memory Streams A very frequently used implementation of streams is the 'memory stream'. This represents a stream of data that exists in RAM only. The data is stored in an ezMemoryStreamStorage object. You can have multiple readers through the ezMemoryStreamReader class and you could theoretically have multiple writers through the ezMemoryStreamWriter class (though having more than one will probably not work in any useful way). ezEngine often uses memory streams to store incoming data for fast access later. For example the ezResourceManager reads files from disk on a separate thread into a memory stream. This allows to do the slow file reading in parallel without blocking the main thread. Once all data is read into the memory stream, the main thread can then read the data directly from memory and have a guaranteed latency. The interface for reading from file or the memory stream is identical, so the code that actually interprets the file content does not need to know about this optimization at all. Other Types of Streams ezEngine comes with a variety of stream implementations: Compressed Streams: Through ezCompressedStreamReaderZstd and ezCompressedStreamWriterZstd you can easily add compression to your data streams. These classes take another stream as input or output, so you can pass it a file or memory stream to work upon. Chunk Streams: A chunk stream is basically a stream that is divided into distinct parts which are fully separated from each other. This allows to handle one stream (e.g. a file) as if it were actually multiple streams, which is useful when you want to package multiple files into one. The useful feature of these streams is that code often reads a stream until it ends (e.g. nothing more can be read). When you package multiple files into one stream, this behavior can end badly. A chunk stream enables you to prevent code from reading further than a specific point (a virtual 'end-of-stream' position). Chunk formats are also useful when you only want to read or update certain parts of a file, without knowing how the rest of the file format works. This is possible because the size of each chunk is stored in the stream which allows to skip or read an entire chunk and pass it through. Please note that you can combine different types of streams. For example you can write to a file by using ezFileWriter, pass that stream to ezChunkStreamWriter and then even use ezCompressedStreamWriterZstd to compress individual chunks in the file. Using Standard Types with Streams The stream interfaces only provide the functions ReadBytes and WriteBytes. For most standard types ezEngine provides overloaded << and >> operators. E.g. you can read and write ints, floats, ezVec3, ezString, etc. like this: write_stream << ezVec3(1, 2, 3); write_stream << \"some string\"; ezVec3 v; ezString s; read_stream >> v; read_stream >> s; Using Complex Types with Streams To serialize and deserialize data that is in EZ containers, you can also use functions like ezStreamReader::ReadArray(), ezStreamReader::ReadMap(), ezStreamWriter::WriteArray() and ezStreamWriter::WriteMap(). Those functions will try to serialize the container information and all the elements. For that to work, there must either be << and >> operaters overloaded for the element type, or the element types must have member functions with the following signature: ezResult Serialize(ezStreamWriter& stream) const; ezResult Deserialize(ezStreamReader& stream); Using these functions allows to return success or failure, which is not possible with the shift operators. If any element fails to de-/serialize, the whole operation (e.g. ezStreamWriter::WriteArray()) will fail and terminate early. Note: ezStreamWriter::WriteArray() and similar functions are provided for convenience. However, when it is important to have full control over file versioning and backwards compatibility of a file format, it may be preferable to serialize containers manually. Low Level File Abstraction Reading and writing files is implemented through ezOSFile. This class internally uses platform specific functions such as fopen on Linux and CreateFile on Windows. You should typically NOT use this class, there are higher level abstractions built upon ezOSFile that you should prefer. Since ezOSFile provides actual file access, it also has some file specific features, for instance you can set the file read or write position back and forth and you can get the OS specific file handle, such that you can use OS specific functions on that file yourself. Additionally there are static functions for deleting files, creating directory structures, querying whether a file exists, and so on. ezOSFile is a thin abstraction over the operating system, it does not yet implement a higher level interface. Therefore it deliberately does not implement the stream interfaces, so you cannot pass an ezOSFile instance to a function that takes an ezStreamReader or ezStreamWriter. Usually you only need to use ezOSFile for some of the static functions that implement more infrequently used features, such as ezOSFile::CopyFile or ezOSFile::GetFileStats. These might not be available in higher level abstractions. Please be aware that ezOSFile always requires platform specific absolute paths. There is no concept of a 'current working directory' or some root directory, to which relative paths could be used. ezOSFile will check all incoming paths and assert that they are absolute. Therefore, if you need to use ezOSFile, make sure to always convert any relative path to an absolute path before you pass it to ezOSFile. Other Low Level File Operations Apart from ezOSFile, there are a few classes that implement additional file system operations that are only thin abstractions over the operating system. ezFileSystemIterator is a class that allows to iterate over all files and folders in some directory. In a game this should rarely be necessary, but in tools this can be very useful. Unfortunately it is not guaranteed that this feature can be implemented on all platforms, and each platform might have different features regarding wild-card usage and so on. Therefore this class is only available when the preprocessor define EZ_SUPPORTS_FILE_ITERATORS is defined as EZ_ON. ezFileStats is a struct that provides information about a file or folder. One way to retrieve file stats is through ezOSFile::GetFileStats(). Again, this feature cannot be implemented on all platforms, so it is only available when EZ_SUPPORTS_FILE_STATS is defined as EZ_ON. High Level File System ezEngine comes with a high level file system that is very flexible and powerful but might seem a bit confusing at first for people that are not used to working with abstractions on this level. A game engine is in many aspects simply a framework that manages resources. It ensures that resources can be found and accessed, are freed when not needed any more and otherwise are handled efficiently and often in an abstract manner that makes it easy to work with them. Working with files and everything that is similar enough so that it could be handled like a file, is the lowest level at which this 'managing of resources' begins. ezEngine does this by working with a virtual file system. The central class to configure this file system is ezFileSystem. Data Directories The most important concept that you need to understand is that of 'data directories'. A data directory is basically a mount point that is added to the virtual file system. You can have an arbitrary number of those, but usually you will only need very few. A data directory is in the most common use case some folder on your harddisk. By adding it to the file system (using ezFileSystem::AddDataDirectory) you make that folder visible for the application. When you work with the virtual file system, you do NOT use ezOSFile. Instead, you will most likely use ezFileReader and ezFileWriter. Those classes represent a single open file in the virtual file system. They implement the stream interfaces mentioned above. When you open a file through these classes, you should use relative paths. Absolute paths will only work if a data directory above the given path is also mounted. When you open a file through a relative path, all data directories are searched (in the reverse order in which they were added) for the file. So as long as the file exists in one data directory, it can be opened. If it exists in multiple data directories with the same relative path, you will typically get the file from the data directory that was added last. The use case here is, that you often want to have some 'base' data directory that contains general files and than additional data directories that add project or even level specific files to the file system. You will typically add the 'base' data directory right at startup of your game. Later you add your project specific data directories. Since you often want your files in the project data directory to take precedence over the 'base' data directory, it makes sense for the file system to search the directories in the reverse order in which they were added. One use case for multiple data directories with the same files in it is localization. Suppose you have a folder with all your sounds, including speech that is recorded in English. When you want to add another language, you can just create a folder 'German' that only contains the sound files which need localization. When the user wants German language support, you simply mount the German sounds folder AFTER the other data directory. Now whenever a sound file gets loaded that is both in the German sounds data directory and the default data directory, the German sound will be preferred. Everything that is not there falls back to the default data directory. Thus when some sound file was forgotten to be included in the German localization, the game will at least play the sound in English. Advanced Data Directories Although the most common use case is to mount folders as data directories, ezEngine's concept of data directories goes much further. Basically anything can be 'mounted' as a data directory. For example you could mount a zip file and then use it like a read-only folder. Or you could mount a folder on a remote PC and have the data sent over a network. This is possible because data directories are an abstract concept. To create your own type of data directory, you need to implement three classes, derived from ezDataDirectoryType, ezDataDirectoryReader and ezDataDirectoryWriter respectively. Therefore, when you add a data directory to ezFileSystem, the engine needs to know how exactly to mount this type. This is what 'data directory factories' are necessary for. So for example, if you want to support simple folders and zip files as data directories, you need to register one factory for 'folders' and one factory for 'zip archives'. Now when you mount a data directory with the name \"My/Test/Archive.zip\", the engine will ask each factory, whether it can handle this path. The 'folder factory' will detect that this is a file and not a folder and therefore decline. The 'zip factory' detects that this is a zip file and therefore creates a data directory which provides the functionality to read files from the archive as if it were a real folder. For example you could then open a file with the path \"My/Test/Archive.zip/Some/Compressed/File.txt\" through ezFileReader and it would just work as if the file was located in a real folder. That is why you always need to register these factories first, before you can add any data directories to the virtual file system. If you want to implement your own data directory type, please have a look at ezDataDirectory::FolderType. Setting up the File System Setting up the file system is very easy to do, once you know what is involved and why. The most basic configuration looks like this: // register a factory that can handle simple folders // this is actually already done automatically through the ezStartup system // ezFileSystem::RegisterDataDirectoryFactory(ezDataDirectory::FolderType::Factory); // mount the application directory (where the binary is located) as a data directory ezFileSystem::AddDataDirectory(ezOSFile::GetApplicationDirectory()); That's all. Now you can read and write files in that folder like this: ezFileWriter FileOut; if (FileOut.Open(\"SubFolder/Test.txt\") == EZ_SUCCESS) { // \"SubFolder\" will be created automatically, if it does not exist yet FileOut << \"This is a string\"; FileOut.Close(); // will also be called automatically when FileOut goes out of scope } ezFileReader FileIn; if (FileIn.Open(\"SubFolder/Test.txt\") == EZ_SUCCESS) { ezString s; FileIn >> s; FileIn.Close(); // will also be called automatically when FileIn goes out of scope EZ_ASSERT_DEV(s == \"This is a string\", \"The read string is incorrect: '%s'\", s.GetData()); } Obviously you can now add further data directories through ezFileSystem::AddDataDirectory(). There are several additional features to configure the system for your exact use case. Please have a look at the API documentation for further details. Resolving Paths One thing that comes up once in a while is that you need to convert a path from relative to absolute or vice versa. For example, should you actually need to use an ezOSFile function and therefore you require the absolute path to a file, of which you only know the relative path (in some data directory), you need a way to query this information. This is what ezFileSystem::ResolvePath() is for. Basically you give it some path, and it will return under which absolute path and under which relative path the file was found. It also needs to know whether you want to read or write the file. If you want to read it, ie. the file is supposed to already exist, it will search for the file in all data directories. If you want to write it, it will return under which path the file would end up. See Also"
  },
  "pages/docs/runtime/reflection-system.html": {
    "href": "pages/docs/runtime/reflection-system.html",
    "title": "Reflection System | ezEngine",
    "keywords": "Reflection System The ezEngine reflection system allows to inspect structs and classes at runtime. It is used primarily for communication with tools and serialization. The reflection system is macro-based, meaning that it is not generated automatically but needs to be written manually for each type, member, etc that needs to be known at runtime. Types There are four distinct types that can be represented by reflection: classes, structs, enums and bitflags. Each is represented by the ezRTTI class that stores the type information. Classes Classes are separated into two types: dynamic and static reflected. Dynamic classes derive from ezReflectedClass which allows you to determine its type using ezReflectedClass::GetDynamicRTTI(). So with a pointer to an ezReflectedClass you can access its type information. A static reflected class does not derive from ezReflectedClass so it is not possible to get the RTTI information in a common way. However, if you know the type of a variable you can use the template function ezGetStaticRTTI to retrieve the ezRTTI instance of a specific type. Alternatively, you can also search for a type by name using ezRTTI::FindTypeByName(). ezReflectedClass* pTest = new ezDynamicTestClass; const ezRTTI* pRtti = pTest->GetDynamicRTTI(); const ezRTTI* pRtti2 = ezGetStaticRTTI<ezDynamicTestClass>(); const ezRTTI* pRtti3 = ezRTTI::FindTypeByName(\"ezDynamicTestClass\"); Declaring a dynamic class involves deriving from ezReflectedClass, adding the EZ_ADD_DYNAMIC_REFLECTION(SELF, BASE_TYPE) macro into the class body and adding a EZ_BEGIN_DYNAMIC_REFLECTED_TYPE(Type, Version, AllocatorType) block into a compilation unit. //Header class ezDynamicTestClass : public ezReflectedClass { EZ_ADD_DYNAMIC_REFLECTION(ezDynamicTestClass, ezReflectedClass); }; //Cpp EZ_BEGIN_DYNAMIC_REFLECTED_TYPE(ezDynamicTestClass, 1, ezRTTIDefaultAllocator<ezDynamicTestClass>) EZ_END_DYNAMIC_REFLECTED_TYPE Declaring a static class is very similar to declaring a dynamic class. However, you need to declare the type outside the class via EZ_DECLARE_REFLECTABLE_TYPE(Linkage, TYPE) and use EZ_BEGIN_STATIC_REFLECTED_TYPE(Type, BaseType, Version, AllocatorType) in a compilation unit. If a class has no base class, use the dummy class ezNoBase instead. // Header class ezStaticTestClass { }; EZ_DECLARE_REFLECTABLE_TYPE(EZ_NO_LINKAGE, ezStaticTestClass); // Cpp EZ_BEGIN_STATIC_REFLECTED_TYPE(ezStaticTestClass, ezNoBase, 1, ezRTTIDefaultAllocator<ezStaticTestClass>); EZ_END_STATIC_REFLECTED_TYPE Structs Structs are identical to static reflected classes so you can use the exact same macros. Enums Enums are limited to structured enums, i.e. those used by the ezEnum class. Declaration is similar to static classes, but you use EZ_BEGIN_STATIC_REFLECTED_ENUM(Type, Version) instead in the compilation unit code. // Header struct ezExampleEnum { typedef ezInt8 StorageType; enum Enum { Value1 = 1, // normal value Value2 = -2, // normal value Value3 = 4, // normal value Default = Value1 // Default initialization value (required) }; }; EZ_DECLARE_REFLECTABLE_TYPE(EZ_NO_LINKAGE, ezExampleEnum); // Cpp EZ_BEGIN_STATIC_REFLECTED_ENUM(ezExampleEnum, 1) EZ_ENUM_CONSTANTS(ezExampleEnum::Value1, ezExampleEnum::Value2) EZ_ENUM_CONSTANT(ezExampleEnum::Value3), EZ_END_STATIC_REFLECTED_ENUM The enum constants can either be declared via EZ_ENUM_CONSTANTS() or EZ_ENUM_CONSTANT(Value) inside the begin / end block of the enum declaration. An enum type can be identified by its base type which is always the dummy ezEnumBase. Bitflags Bitflags are limited to structured bitflags, i.e. those used by the ezBitflags class. Declaration is similar to static classes, but you use EZ_BEGIN_STATIC_REFLECTED_BITFLAGS(Type, Version) instead in the compilation unit code. // Header struct ezExampleBitflags { typedef ezUInt64 StorageType; enum Enum : ezUInt64 { Value1 = EZ_BIT(0), // normal value Value2 = EZ_BIT(31), // normal value Value3 = EZ_BIT(63), // normal value Default = Value1 // Default initialization value (required) }; struct Bits { StorageType Value1 : 1; StorageType Padding : 30; StorageType Value2 : 1; StorageType Padding2 : 31; StorageType Value3 : 1; }; }; EZ_DECLARE_REFLECTABLE_TYPE(EZ_NO_LINKAGE, ezExampleBitflags); // Cpp EZ_BEGIN_STATIC_REFLECTED_BITFLAGS(ezExampleBitflags, 1) EZ_BITFLAGS_CONSTANTS(ezExampleBitflags::Value1, ezExampleBitflags::Value2) EZ_BITFLAGS_CONSTANT(ezExampleBitflags::Value3), EZ_END_STATIC_REFLECTED_BITFLAGS(); The bitflags constants can either be declared via EZ_BITFLAGS_CONSTANTS() or EZ_BITFLAGS_CONSTANT(Value) inside the begin / end block of the bitflags declaration. A bitflags type can be identified by its base type which is always the dummy ezBitflagsBase. Properties Properties are the most important information in a type as they define the data inside it. The properties of a type can be accessed via ezRTTI::GetProperties(). There are different categories of properties, each deriving from ezAbstractProperty. The type of property can be determined by calling ezAbstractProperty::GetCategory(). Properties are added via the property macros inside the EZ_BEGIN_PROPERTIES() / EZ_END_PROPERTIES() block of the type declaration like this: EZ_BEGIN_STATIC_REFLECTED_TYPE(ezStaticTestClass, ezNoBase, 1, ezRTTIDefaultAllocator<ezStaticTestClass>) { EZ_BEGIN_PROPERTIES { EZ_CONSTANT_PROPERTY(\"Constant\", 5), EZ_MEMBER_PROPERTY(\"Member\", m_fFloat), EZ_ACCESSOR_PROPERTY(\"MemberAccessor\", GetInt, SetInt), EZ_ARRAY_MEMBER_PROPERTY(\"Array\", m_Deque), EZ_ARRAY_ACCESSOR_PROPERTY(\"ArrayAccessor\", GetCount, GetValue, SetValue, Insert, Remove), EZ_SET_MEMBER_PROPERTY(\"Set\", m_SetMember), EZ_SET_ACCESSOR_PROPERTY(\"SetAccessor\", GetSet, SetInsert, SetRemove), } EZ_END_PROPERTIES } EZ_END_STATIC_REFLECTED_TYPE(); Constants Constants are declared via EZ_CONSTANT_PROPERTY(PropertyName, Value). The value is stored within the property so no instance of the class is necessary to access it. To access the constant, cast the property to ezAbstractConstantProperty and call ezAbstractConstantProperty::GetPropertyType() to determine the constant type. Then either cast to ezTypedConstantProperty of the matching type, or if the type is not known to you at compile time, use ezAbstractConstantProperty::GetPropertyPointer() to access its data. Members There are two types of member properties, direct member properties and accessor properties. The first has direct access to the memory location of the property in the class while the later uses functions to get and set the property's value. Direct member properties are declared via EZ_MEMBER_PROPERTY(PropertyName, MemberName) while accessor properties are declared via EZ_ACCESSOR_PROPERTY(PropertyName, Getter, Setter). The getter and setter functions must have the following signature: Type GetterFunc() const; void SetterFunc(Type value); Type can be decorated with const and reference but must be consistent between get and set function. The available macros are the following: EZ_MEMBER_PROPERTY(\"Member\", m_fFloat1), EZ_MEMBER_PROPERTY_READ_ONLY(\"MemberRO\", m_vProperty3), EZ_ACCESSOR_PROPERTY(\"MemberAccessor\", GetInt, SetInt), EZ_ACCESSOR_PROPERTY_READ_ONLY(\"MemberAccessorRO\", GetInt), To access an instance's member variable value, cast the property to ezAbstractMemberProperty and call ezAbstractMemberProperty::GetPropertyType() to determine the member type. Then either cast to ezTypedMemberProperty of the matching type, or if the type is not known to you at compile time, use ezAbstractMemberProperty::GetPropertyPointer() or ezAbstractMemberProperty::GetValuePtr() and ezAbstractMemberProperty::SetValuePtr() to access its data. The first solution will only return a valid pointer if the property is a direct member property. Arrays Array properties are very similar to member properties, they just handle arrays instead of single values. Direct array properties are declared via EZ_ARRAY_MEMBER_PROPERTY(PropertyName, MemberName) while accessor array properties are declared via EZ_ARRAY_ACCESSOR_PROPERTY(PropertyName, GetCount, Getter, Setter, Insert, Remove). The accessor interface functions must have the following signature: ezUInt32 GetCount() const; Type GetValue(ezUInt32 uiIndex) const; void SetValue(ezUInt32 uiIndex, Type value); void Insert(ezUInt32 uiIndex, Type value); void Remove(ezUInt32 uiIndex); The available macros are the following: EZ_ARRAY_ACCESSOR_PROPERTY(\"ArrayAccessor\", GetCount, GetValue, SetValue, Insert, Remove), EZ_ARRAY_ACCESSOR_PROPERTY_READ_ONLY(\"ArrayAccessorRO\", GetCount, GetValue), EZ_ARRAY_MEMBER_PROPERTY(\"Hybrid\", m_Hybrid), EZ_ARRAY_MEMBER_PROPERTY(\"Dynamic\", m_Dynamic), EZ_ARRAY_MEMBER_PROPERTY_READ_ONLY(\"Deque\", m_Deque), To access an instance's array, cast the property to ezAbstractArrayProperty and call ezAbstractArrayProperty::GetElementType() to determine the element type. From here you can use the various functions inside ezAbstractArrayProperty to manipulate an instance's array. Sets Set properties are very similar to member properties, they just handle sets instead of single values. Direct set properties are declared via EZ_SET_MEMBER_PROPERTY(PropertyName, MemberName) while accessor set properties are declared via EZ_SET_ACCESSOR_PROPERTY(PropertyName, GetValues, Insert, Remove). The accessor interface functions must have the following signature: void Insert(Type value); void Remove(Type value); Container<Type> GetValues() const; The available macros are the following: EZ_SET_ACCESSOR_PROPERTY(\"SetAccessor\", GetValues, Insert, Remove), EZ_SET_ACCESSOR_PROPERTY_READ_ONLY(\"SetAccessorRO\", GetValues), EZ_SET_MEMBER_PROPERTY(\"Set\", m_SetMember), EZ_SET_MEMBER_PROPERTY_READ_ONLY(\"SetRO\", m_SetMember), To access an instance's set, cast the property to ezAbstractSetProperty and call ezAbstractSetProperty::GetElementType() to determine the element type. From here you can use the various functions inside ezAbstractSetProperty to manipulate an instance's set. Flags Types as well as properties have flags that quickly let you determine the kind of type / property you are dealing with. For types, ezRTTI::GetTypeFlags() lets you access its ezTypeFlags::Enum flags which are automatically deduced from the type at compile time. Properties can have flags as well, ezAbstractMemberProperty::GetFlags(), ezAbstractArrayProperty::GetFlags() and ezAbstractSetProperty::GetFlags() let you access the ezPropertyFlags::Enum flags of the handled property type. The only difference here is that besides automatically deduced flags there are also user-defined flags that can be added during declaration of the property by using ezAbstractMemberProperty::AddFlags and the variants on the other property categories: EZ_ACCESSOR_PROPERTY(\"ArraysPtr\", GetArrays, SetArrays)->AddFlags(ezPropertyFlags::PointerOwner), Limitations No two types can share the same name. Each property name must be unique within its type. Only constants that are a basic type (i.e. can be stored inside an ezVariant) will be available to tools. A pointer to a type cannot be its own type, the only exception to this is const char*. See Also"
  },
  "pages/docs/runtime/resource-management.html": {
    "href": "pages/docs/runtime/resource-management.html",
    "title": "Resource Management | ezEngine",
    "keywords": "Resource Management Resource management is fully functional, but currently undocumented. See Also"
  },
  "pages/docs/runtime/world/component-managers.html": {
    "href": "pages/docs/runtime/world/component-managers.html",
    "title": "Component Managers | ezEngine",
    "keywords": "Component Managers A component manager is a world module whose purpose it is to create, store and update components of a single type. For every component type, there is exactly one component manager to handle that type. Simple Component Managers There are two types of simple component managers: Ones that don't update their components, at all. Ones that call a simple Update() function once per frame on their components. No Update Component Manager A component manager that doesn't update its components is declared like this: using SendMsgComponentManager = ezComponentManager<class SendMsgComponent, ezBlockStorageType::Compact>; We can simply instantiate the ezComponentManager template and not override anything. Therefore this component manager doesn't have any update function and so the component type that it manages is never updated. That doesn't mean that the component type in question can't do things periodically. In fact the SendMsgComponent does update its state regularly, but it triggers its own update through messaging, which is more efficient for components that only need to wake up every once in a while. Simple Update Component Manager Many component types need to be updated every frame, but it is sufficient if the component manager just calls a simple Update() function. Creating a component manager for this scenario looks like this: using DisplayMsgComponentManager = ezComponentManagerSimple<class DisplayMsgComponent, ezComponentUpdateType::WhenSimulating, ezBlockStorageType::FreeList>; That is literally all. The template ezComponentManagerSimple will take care of the required update function setup. All you need to do then, is to add a (non-virtual) Update() function to the component type, which the component manager will call for all active components each frame. The ezComponentUpdateType option determines whether the component manager will call the Update() function only while the world simulation is running (during a game) or also when it is not running, meaning when editing a scene. For things that should show up even while looking at a paused scene in the editor, you need to use ezComponentUpdateType::Always. Non-Simple Component Managers The vast majority of component managers are very simple, but they can also be much more complex. This is mostly the case when the manager needs to synchronize state between components and other systems. Another reason to write a more complex component manager is efficiency. If the manager can track which components need updating and which ones can be ignored, it can skip the update for many components. Or it can update only a number of components each frame to amortize costs. To write a more complex component manager you basically just register your own update functions and then do whatever needs to be done there. See the chapter about world modules for how to do that. Note: When you write your own update function, don't forget to skip inactive components. Otherwise deactivating a component or object hierarchy has no effect on your component type. See ezComponentManagerSimple::SimpleUpdate() for an example. Component Storage Both component managers above were configured with a ezBlockStorageType option. This determines what happens when a component gets deleted from the world. If the component manager is set to ezBlockStorageType::FreeList, the unused memory block will be put into a free-list and reused when a new component is allocated. In the mean time, the component manager needs to skip these unused memory blocks, every time it iterates over all components. For components that have very short lifespans or are frequently created and destroyed, this can be more efficient. The main reason to use this, though, is for components that can't be relocated in memory. If a component would crash when it is copied to a different memory location, then using the free-list option prevents this. If the component manager is set to ezBlockStorageType::Compact, then an unused memory block will be filled right away by relocating the last valid component to that freed up slot. This prevents memory fragmentation, which wastes performance when iterating over large arrays of components, of which many elements are unused. For components which are mostly long lived, this option gives better performance. If in doubt, both options are fine. The ezComponentManagerSimple defaults to ezBlockStorageType::FreeList as this mode has fewer restrictions. See Also World Modules Components"
  },
  "pages/docs/runtime/world/components.html": {
    "href": "pages/docs/runtime/world/components.html",
    "title": "Components | ezEngine",
    "keywords": "Components For an introduction what a component is and how it fits into the overall picture, see The World / Scenegraph System. This documentation focuses on the C++ ezComponent class. The functionality exposed through other mechanisms, such as TypeScript, may be more limited in scope, but ultimately maps to the C++ implementation. Components are the fundamental building blocks with which to make the engine do things. Components act as glue between systems like the renderer and the user. They expose the available functionality to the editor and they control when and how each system is used. This document describes how components work. Owner A component is always attached to a game object. This 'owner' can be queried with ezComponent::GetOwner(). There is a brief moment where a component is not attached to an owner, which is when it is being prepared for destruction. If you run into such a situation use ezComponent::IsActiveAndInitialized() to filter them out. Component Manager Every component has a component manager. You can access it with ezComponent::GetOwningManager(). To get the component manager for a specific component type, you need to query the world. See ezWorld::GetOrCreateComponentManager() and ezWorld::GetOrCreateManagerForComponentType(). Creating Components The most convenient way to create a component of a specific type is to call the static function CreateComponent() on the component manager for that type. ezMeshComponent* pMesh; pWorld->GetOrCreateComponentManager<ezMeshComponentManager>()->CreateComponent(pObject, pMesh); Deleting Components To delete a component, just call ezComponent::DeleteComponent() on it. Note that deleting individual components is relatively rare, it is more common to just delete the entire object. Also be aware that deleted components are immediately deinitialized. They will still exist in a semi-usable state until the end of the frame, but if other code tries to access the component within the same frame, it may see it in an 'unexpected' state. If necessary, that code can check ezComponent::IsActiveAndInitialized() to prevent working with just deleted components. You can also delete a component only through its handle, if you have the corresponding component manager. Component Handles When you need to reference components across frames, you should always store handles to them, never pointers. See the chapter about object lifetime for details. To convert a handle to a (temporary) pointer, use ezWorld::TryGetComponent(). Querying Components from Game Objects When you have a game object you can get a list of all attached components with ezGameObject::GetComponents(). However, typically you want to get a component of a specific type. Use ezGameObject::TryGetComponentOfBaseType() for that: ezMeshComponent* pMesh = nullptr; if (pObject->TryGetComponentOfBaseType(pMesh)) { pMesh->DeleteComponent(); } Iterating over all Components You can iterate over all components of one type by calling ezComponentManager::GetComponents(). This returns an iterator with which you can efficiently access all components managed by that component manager. Be aware that some components may not be active, so you should skip those. You can also access all components on a game object using ezGameObject::GetComponents(). Component Reflection Block All component types must use reflection. Only reflected members show up as properties in the editor. An example block looks like this: EZ_BEGIN_COMPONENT_TYPE(DebugRenderComponent, 2, ezComponentMode::Static) { EZ_BEGIN_PROPERTIES { EZ_MEMBER_PROPERTY(\"Size\", m_fSize)->AddAttributes(new ezDefaultValueAttribute(1), new ezClampValueAttribute(0, 10)), EZ_MEMBER_PROPERTY(\"Color\", m_Color)->AddAttributes(new ezDefaultValueAttribute(ezColor::White)), EZ_ACCESSOR_PROPERTY(\"Texture\", GetTextureFile, SetTextureFile)->AddAttributes(new ezAssetBrowserAttribute(\"CompatibleAsset_Texture_2D\")), EZ_BITFLAGS_MEMBER_PROPERTY(\"Render\", DebugRenderComponentMask, m_RenderTypes)->AddAttributes(new ezDefaultValueAttribute(DebugRenderComponentMask::Box)), EZ_ACCESSOR_PROPERTY(\"CustomData\", GetSampleCustomDataResource, SetSampleCustomDataResource)->AddAttributes(new ezAssetBrowserAttribute(\"CompatibleAsset_CustomData\")), } EZ_END_PROPERTIES; EZ_BEGIN_ATTRIBUTES { new ezCategoryAttribute(\"SampleGamePlugin\"), // Component menu group } EZ_END_ATTRIBUTES; EZ_BEGIN_MESSAGEHANDLERS { EZ_MESSAGE_HANDLER(ezMsgSetColor, OnSetColor) } EZ_END_MESSAGEHANDLERS; EZ_BEGIN_FUNCTIONS { EZ_SCRIPT_FUNCTION_PROPERTY(SetRandomColor) } EZ_END_FUNCTIONS; } EZ_END_COMPONENT_TYPE The properties section lists all the members that should be editable. Components can have 'virtual' properties, that don't exist as members, but use accessors (functions). Properties can have attributes to configure how they show up in the editor. The attributes section can additionally specify type specific properties. For example, here we tell the editor where in the component menu this component should appear. The message handler section is important to enable messaging. The functions section is used to expose certain member functions to the reflection system, such that script bindings, such as TypeScript can call these functions. At the moment there is no documentation that lists all the available options. It is best to get inspiration by looking at the code for existing components. Component Activation There are three important states for components: Whether they are initialized Whether they are active Whether they are simulating You can hook into changes to these states by overriding ezComponent::Initialize() / ezComponent::Deinitialize(), ezComponent::OnActivated() / ezComponent::OnDeactivated() and ezComponent::OnSimulationStarted(). The most important function to override is ezComponent::OnSimulationStarted(). This is almost always the function where you want to set up your component. It is called when the component is fully initialized, active and the world is actively simulating (the game is running). In the editor, it is only called after you start running a scene, not while you are editing. Since most game code should not do anything while the scene is being edited, you typically don't need to set up anything before this time. Components can be 'active' or 'inactive'. This can be used to switch them on and off at will. The active flag on game objects affects this, but components can also be deactivated individually with ezComponent::SetActiveFlag(). When a component is not active, its component manager will typically not update it anymore. If you want to properly support switching components on and off at any time, you often need to be careful to restore state properly. ezComponent::OnActivated() and ezComponent::OnDeactivated() will be called every time a component's active state changes. Additionally, if the world is being simulated, ezComponent::OnSimulationStarted() will also be called after each call to ezComponent::OnActivated(). It should be extremely rare that you need to override ezComponent::Initialize() or ezComponent::Deinitialize(). For all the details on the activation functions, refer to the API Docs. Caution: A common mistake is to override a function like ezComponent::OnActivated() but to not call its base class implementation (SUPER::OnActivated()). It is good practice to always do so. Forced Activation If for some reason a component must access another component during its own setup, and requires that other component to be set up first, you can enforce this by calling ezComponent::EnsureSimulationStarted() on the other component. An example is a physics joint component. To set up a joint, the component needs to access two rigid body components. Both must be already set up themselves, otherwise the joint component cannot link the two. Therefore, when the joint component is being set up, it calls ezComponent::EnsureSimulationStarted() on both rigid body components, to make sure it can access valid data. User Flags ezComponent::SetUserFlag and ezComponent::GetUserFlag can be used to store up to 8 bits of user flags. This should only be used internally, to reduce memory consumption. Dynamic and Static Components In the component reflection block you have to specify whether a component is 'dynamic' or 'static': EZ_BEGIN_COMPONENT_TYPE(DemoComponent, 3 /* version */, ezComponentMode::Dynamic) This information tells the editor whether this component type attempts to modify the owner's transformation (position, rotation, scale). If any dynamic component is attached to a game object, the entire object will be marked as dynamic and will have its transform updated every frame. If only static components are attached, the game object can be marked as static as well, and costs less performance. See Static vs. Dynamic Objects. Serialization and Versioning When editing a scene or prefab, the editor will serialize components purely based on reflection information. That means only the properties that are marked up through reflection and are therefore visible to the user are serialized. This format is robust to change (and allows for patches), but is not efficient. For the runtime format, that a shipping game should use, scenes are exported. This is a binary serialization format and every component has full control over what data it writes and how it encodes the data. When you run a scene in ezPlayer the editor will serialize the scene to the binary format, and the player will deserialize it. If a component doesn't properly serialize all its data, the results can range from misconfigured components to crashes during loading. To implement proper serialization, you need to override ezComponent::SerializeComponent() and ezComponent::DeserializeComponent(). During serialization you simply write data to a stream, as you like: void DemoComponent::SerializeComponent(ezWorldWriter& inout_stream) const { SUPER::SerializeComponent(inout_stream); auto& s = inout_stream.GetStream(); s << m_fAmplitude; s << m_Speed; } Don't forget to call SUPER::SerializeComponent() to include the data of the base class. When you deserialize a component, you need to handle versioning. Every component type has a version number, which is specified in the component reflection block: EZ_BEGIN_COMPONENT_TYPE(DemoComponent, 3 /* version */, ezComponentMode::Dynamic) The version number should be increased every time the serialization format of the component type has to change. During deserialization you can query the version number with which this component data was written. You than have to handle converting older formats as appropriate: void DemoComponent::DeserializeComponent(ezWorldReader& inout_stream) { SUPER::DeserializeComponent(inout_stream); const ezUInt32 uiVersion = inout_stream.GetComponentTypeVersion(GetStaticRTTI()); auto& s = inout_stream.GetStream(); s >> m_fAmplitude; if (uiVersion <= 2) { // up to version 2 the angle was stored as a float in degree // convert this to ezAngle float fDegree; s >> fDegree; m_Speed = ezAngle::MakeFromDegree(fDegree); } else { s >> m_Speed; } } Custom Components You can extend the engine with custom components: Custom Components with C++ Custom Components with TypeScript For examples, have a look at the Sample Game Plugin. See Also Custom Code The World / Scenegraph System Game Objects Sample Game Plugin"
  },
  "pages/docs/runtime/world/game-objects.html": {
    "href": "pages/docs/runtime/world/game-objects.html",
    "title": "Game Objects | ezEngine",
    "keywords": "Game Objects For an introduction what a game object is and how it fits into the overall picture, see The World / Scenegraph System. This documentation focuses on the C++ ezGameObject class. The functionality exposed through other mechanisms, such as TypeScript, may be more limited in scope, but ultimately maps to the C++ implementation. Creating Game Objects You create new game objects by calling ezWorld::CreateObject(). This function takes a ezGameObjectDesc. This is used to initialize the game object, all options can be changed later on, however things like changing the 'dynamic' state can be expensive to switch. Deleting Game Objects Use ezWorld::DeleteObjectDelayed() to remove an object from the world. This will also delete all child nodes and attached components. This function puts the object into a deletion queue and only deallocates it at the end of the frame, so code that still tries to access the object in the same frame will not be affected. The chapter about object lifetime explains this in more detail. Object Transforms Game objects have a position, rotation and scaling. These values are separated into local and global. The local transform represents the offset to the parent. If a game object has no parent, the local and global transform are identical. You can query or modify either, though in the editor the property grid only allows you to set the local transform. The global transform is computed from the local transform and the global transform of the parent (recursively). For dynamic objects (see below) the global transform is recomputed regularly. Static objects will not be updated after their initial placement. Video: How to parent objects Static vs. Dynamic Objects Static game objects are objects that are considered to never move. Dynamic objects, however, can move around the scene arbitrarily. Internally the engine separates these two types of objects into different memory regions. The object transform for dynamic objects is updated every frame. That means from a performance perspective it makes no difference whether a dynamic object was moved in a frame or not. The transforms for static objects, however, are only updated when it is needed (after creation). If you try to move a static object, you will see warnings in the log in development builds, and the object will not move. When you build a scene in the editor, you generally don't need to worry about this. Each component type is flagged to either be dynamic (meaning it may modify its owner's position) or static. From the attached components, the editor will automatically detect whether a game object must be created as static or dynamic. However, in some cases you may know that an object will end up having dynamic components later. So to prevent a costly switch from static to dynamic, you can force a game object to be dynamic, by selecting this mode from the properties. The renderer also caches rendering state for static objects. This does not mean that a static object cannot change rendering state dynamically, but when it does, the code must ensure to properly invalidate the render caches. If switching from static to dynamic fixes a rendering issue, some attached component doesn't handle this cache invalidation correctly. Performance Considerations For performance reasons it is always best to keep all objects static that don't need to be moved. Due to the render data caching, this can save even more CPU time. However, the code paths to update dynamic objects are still quite heavily optimized. A current CPU can easily handle updating 100.000+ dynamic objects at interactive framerates. Active Flag Game objects have an 'active flag'. By default all objects are marked active. If the active flag is removed, all components on that object get deactivated. That means they will not get updated further and in general the object is treated as if it is not part of the world anymore. The active flag propagates down to child objects. Using ezGameObject::GetActiveFlag() you can check the state of the active flag on a given game object. However, even if the flag is set, the game object can still be deactivated, if any one of its parents has been deactivated. You can check this with ezGameObject::IsActive(). The active flag is useful to hide an entire object hierarchy. For example the player object in the Testing Chambers project has multiple weapons attached. Only one of them is active at a time, and therefore visible. Lifetime and Referencing Game Objects When deleting a game object, it typically stays alive till the end of the frame, to make writing robust code easier. You should, however, never store pointers to game objects across frames, as objects can be relocated in memory. Instead, always use handles (ezGameObjectHandle) to store references to game objects. The chapter about object lifetime explains this in more detail. Components can also reference objects from their properties. These references are also based on handles. Tags Game objects can have tags. These are used to control things like whether the object will cast shadows. However, they are mostly at your disposal to tag objects with game play relevant information. Iterating over Game Objects You can iterate over all objects in a world using ezWorld::GetObjects(). This will return the objects in an arbitrary order, but is the more efficient way. You can also traverse the object hierarchy with ezWorld::Traverse(). This allows you to list the objects either in a depth-first or a breadth-first order. When you have a specific game object, you can also iterate over its children with ezGameObject::GetChildren(). Finding Objects There are multiple ways to find specific objects, or objects relative to some parent node. Global Keys You can assign a game object a global key. This is a string that should be unique across all objects within the world. That includes all game objects from all prefab instances, so you must be very careful with this. If the same global key is used twice, one of them will be ignored. You can query for a game object by global key using ezWorld::TryGetObjectWithGlobalKey(). Global keys can be useful to find unique objects, like the one player object (in a single player game), or level specific items. Finding Child Objects Within an object hierarchy, you can use the name of objects to search for certain child nodes. These functions are available: ezGameObject::FindChildByName() ezGameObject::FindChildByPath() ezGameObject::SearchForChildByNameSequence() ezGameObject::SearchForChildrenByNameSequence() For details please consult the API Docs on those functions. Coordinate System The coordinate system of the world is configurable. To make it easier to not hard code assumptions about which axis represents what direction, the game objects provide functions to query the local axis: ezGameObject::GetGlobalDirForwards() ezGameObject::GetGlobalDirRight() ezGameObject::GetGlobalDirUp() These functions return the respective directions in global space considering the worlds coordinate system and the objects own global rotation. Messaging You can send messages to all components attached to an object, or the entire hierarchy below an object. You can also send event messages, which will 'bubble up' the hierarchy until they find a component to handle it. See the chapter about messaging for details. Team ID All game objects store a 16 bit team ID. This value can be used to identify which team or faction an object belongs to. The team ID has no functionality by itself, you can use it or ignore it. The one feature that the team ID has, is that it is automatically propagated for you when components create objects or instantiate prefabs. This way, when a player with team ID 3 shoots, the bullet prefab that gets instantiated by the spawn component will automatically be assigned team ID 3 as well. Thus when that bullet hits another player, your code can easily attribute a kill to a team, or filter out friendly fire. Although it would be possible to implement something similar entirely with custom components, only by having this in the basic game object, is it possible to trace this information even through built in components, meaning you don't need to reimplement basic functionality like the spawn component or the projectile component. See Also The World / Scenegraph System Components Object Lifetime Messaging"
  },
  "pages/docs/runtime/world/object-lifetime.html": {
    "href": "pages/docs/runtime/world/object-lifetime.html",
    "title": "Object Lifetime | ezEngine",
    "keywords": "Object Lifetime The lifetime of game objects and components is tightly controlled by the world that they belong to. Neither are objects reference counted, nor garbage collected. You have full control over the destruction of objects, but by default 'deleted' objects are not destroyed before the end of the frame, to make writing robust code easy. The lifetime of objects is directly linked to the object hierarchy. If a game object gets deleted, that also deletes all child nodes and all attached components. Referencing Objects In C++ you can of course always hold pointers to anything. Within a single frame, it is fine to reference game objects and components by pointers. However, once the next frame starts, you have to assume that those pointers are invalid. Not only can objects be deleted, but even live objects can be moved around in memory. This 'compacting' is an optimization and can happen to any object between frames. Therefore, instead of keeping pointers to objects, you should always use handles. Specifically ezGameObjectHandle for ezGameObject references, and ezComponentHandle for ezComponent (and derived) types. Handles act like weak pointers. Once you have a handle to an object, you can keep it around forever. When you need to access the actual object, you call ezWorld::TryGetObject() or ezWorld::TryGetComponent(). If the object is still alive at that time, you get back a pointer. That pointer is guaranteed to stay valid until the end of the frame, so you don't need to call the TryGet... function again. As a rule of thumb, you should never have ezGameObject* or ezComponent* types as class members. Pointers to these types should be limited to local function variables. Deleting Game Objects To delete a game object, call ezWorld::DeleteObjectDelayed(). This will put the object into a deletion queue, and will remove the object at the end of the frame. This guarantees that all code that tries to access the object within this frame will work correctly. You can also call ezWorld::DeleteObjectNow(). This will indeed delete the object right at that instant. The only situation where it is ok to call this, is in tools where you modify a world in a single threaded way and you know that no other code can ever access objects. Here, having an object not destroyed immediately may be undesirable. Deleting Components To delete a component, get its component manager and call DeleteComponent() on it. The component won't be deallocated right away, that is deferred till the end of the frame. However, it will be deactivated and deinitialized immediately. Therefore, if other code tries to access such a component, it will get valid memory, but it may see a deinitialized component. Such a situation can be detected by calling ezComponent::IsActiveAndInitialized() on the target. If you delete individual components during a frame (and not entire objects), code that accesses those components should be prepared to deal with deinitialized components. See Also Game Objects Components"
  },
  "pages/docs/runtime/world/spatial-system.html": {
    "href": "pages/docs/runtime/world/spatial-system.html",
    "title": "Spatial System | ezEngine",
    "keywords": "Spatial System Every world has a spatial system. Spatial systems are responsible for sorting game objects by their position and size. They are utilized to efficiently find all objects within a volume, such as a box, a sphere or a view frustum. This is mainly used by the renderer to do frustum culling, but is also available to all other code. Obviously the spatial system needs to keep track of moving objects and update its index accordingly. Spatial System Setup ezSpatialSystem is the base class for all spatial systems. During the construction of an ezWorld, a custom implementation can be provided through the ezWorldDesc. By default ezSpatialSystem_RegularGrid is used, which is optimized to handle arbitrary situations with good performance. Implementing a custom spatial system can make sense when you have a highly specialized use case. For example, if you have a strictly tile-based 2D game, where you know that all sprites are below a fixed size, and you always have a dense grid without holes, you can write a spatial system that takes advantage of this knowledge and therefore outperforms the default implementation. However, unless you determine that the spatial system is a clear performance bottleneck, and you have domain specific knowledge that could be a big advantage to speed things up, there is no reason to consider writing your own. Since there is exactly one spatial system per world, it usually means that the choice of a system is made for a type of game. In theory, though, one could use different systems for different types of levels, as well. Accessing the Spatial System In C++ code you get access to the world's spatial system through ezWorld::GetSpatialSystem(). When using other languages bindings the spatial system may not be exposed directly. For example, when using TypeScript, the most useful functions are exposed directly through ez.World, for example ez.World.FindObjectsInBox() and ez.World.FindObjectsInSphere(). Spatial Data Categories Every piece of spatial data is associated with a category. For example, rendering data is either in the category \"RenderStatic\" or \"RenderDynamic\". This is mainly used to separate spatial information into distinct groups, so that during a spatial query, data that is irrelevant can be filtered out quickly. For efficiency reasons, categories are represented with bitmasks internally, which is why there can only be up to 32 categories. You should assume that the core engine uses at least 5 categories already. Configuring Spatial Data Categories In C++ code you register a spatial data category through ezSpatialData::RegisterCategory(). This will return a category object which can be used for spatial queries later: ezSpatialData::Category RtsSelectableComponent::s_SelectableCategory = ezSpatialData::RegisterCategory(\"Selectable\", ezSpatialData::Flags::None); When using the editor, there are components, such as the marker component, which allow you to select a category from a predefined list. This list is project specific. When you click on such a dropdown box, the last entry allows you to open an editor to configure the available categories: The Invalid Category Some components 'add' their bounds to a ezMsgUpdateLocalBounds using ezInvalidSpatialDataCategory. This means that they want to specify their bounds, but do not want to add anything to the spatial system. This is useful for components that do have a perceived size, such as physics shapes, which should be visible when selecting these objects in the editor, but where there is no benefit of inserting this into the spatial system. Exposing Game Objects to the Spatial System The spatial system only knows about game objects, it does not differentiate by components. However, which game objects are inserted into it and under which categories, is handled by components. The world sends the message ezMsgUpdateLocalBounds to all components when it determines that an update is necessary. This can also be triggered manually by calling ezGameObject::UpdateLocalBounds() when spatial data, such as which category to use, has been modified. Components can handle this message and add spatial information to it. For 3D objects one would use something like the bounding sphere of a mesh, but it is also possible to use more abstract spatial data. For example the RTS sample has an RtsSelectableComponent which is attached to all units that should be selectable by the player. Although the RtsSelectableComponent doesn't have a visual representation, it reacts to ezMsgUpdateLocalBounds to add spatial data, which can then be used to efficiently look up units under the mouse cursor. void RtsSelectableComponent::OnUpdateLocalBounds(ezMsgUpdateLocalBounds& ref_msg) { ezBoundingBoxSphere bounds; bounds.m_fSphereRadius = m_fSelectionRadius; bounds.m_vCenter.SetZero(); bounds.m_vBoxHalfExtends.Set(m_fSelectionRadius); ref_msg.AddBounds(bounds, s_SelectableCategory); } Don't forget to register the message handler in the reflection block: EZ_BEGIN_MESSAGEHANDLERS { EZ_MESSAGE_HANDLER(ezMsgUpdateLocalBounds, OnUpdateLocalBounds) } EZ_END_MESSAGEHANDLERS; Querying the Spatial System Once you have spatial data inserted into the system, you can use it to efficiently query for objects within a volume. When calling functions such as ezSpatialSystem::FindObjectsInSphere() you have to provide a bitmask of categories. That's because you can request to get objects from multiple categories at the same time. You can get this bitmask by calling ezSpatialData::Category::GetBitmask() on a category object. void RtsGameState::InspectObjectsInArea(const ezVec2& vPosition, float fRadius, ezSpatialSystem::QueryCallback callback) const { ezBoundingSphere sphere = ezBoundingSphere::MakeFromCenterAndRadius(vPosition.GetAsVec3(0), fRadius); ezSpatialSystem::QueryParams queryParams; queryParams.m_uiCategoryBitmask = RtsSelectableComponent::s_SelectableCategory.GetBitmask(); m_pMainWorld->GetSpatialSystem()->FindObjectsInSphere(sphere, queryParams, callback); } In other language bindings you may instead need to pass in a list of all the desired categories by name. Spatial System vs. Physics Engines Both the spatial system, as well as physics engines allow you to do spatial queries. There are cases where a problem can be solved using either system, but generally they are meant to complement each other. If you want to query for things that already need to have a physical representation, and therefore will be handled by the physics engine anyway, it is best to leverage the physics engine to query for such objects. For example a shockwave effect that is supposed to push objects away, only makes sense to be applied to physically simulated objects. Therefore, querying which objects are close-by, to figure out what objects to apply the effect to, should be done through the physics engine, and there is no reason to even have information about these objects in the spatial system. On the other hand, things like the RtsSelectableComponent (see above) could be achieved by setting up fake physics actors so that they can be found with physics queries. The performance cost for doing so would be unnecessary high though, as the physics engine would perform additional maintenance that is ultimately not needed, and it may waste precious resources such as collision layers. Here, using the spatial system makes much more sense. Note: If you require doing raycasts or queries against meshes, you will need to use the physics engine, as the spatial system only works with very basic shapes. Spatial System vs. Tags The spatial data categories are very similar to tags. The difference is, that tags are set up on game objects and they don't have any spatial quality. A game object can have many tags, but not be registered spatially and therefore cannot be found through spatial queries. On the other hand, because of this, tags have nearly no performance overhead, whereas spatial data must be updated whenever an object moves. Ultimately, both systems can be used to solve many of the same problems. When you need to be able to inspect an area and find all objects of a certain kind, you should use spatial data, for example through a marker component. If, however, you need to semtantically label objects, but do not require to find them spatially, prefer tags to not waste performance. See Also Marker Component The World / Scenegraph System"
  },
  "pages/docs/runtime/world/world-messaging.html": {
    "href": "pages/docs/runtime/world/world-messaging.html",
    "title": "Messaging | ezEngine",
    "keywords": "Messaging For an introduction what a message is and how it fits into the overall picture, see The World / Scenegraph System. This documentation focuses on the C++ ezMessage class. The functionality exposed through other mechanisms, such as TypeScript, may be more limited in scope, but ultimately maps to the C++ implementation. Messages can be sent from any code. They can only be received by components, though, as the messaging system is implemented by ezWorld. Declaring a Message A message has to be derived from ezMessage and contain a helper macro that implements some message specific functionality: struct ezMsgSetText : public ezMessage { EZ_DECLARE_MESSAGE_TYPE(ezMsgSetText, ezMessage); ezString m_sText; }; In some cpp file you then also need to implement the message and set up basic reflection information: EZ_IMPLEMENT_MESSAGE_TYPE(ezMsgSetText); EZ_BEGIN_DYNAMIC_REFLECTED_TYPE(ezMsgSetText, 1, ezRTTIDefaultAllocator<ezMsgSetText>) EZ_END_DYNAMIC_REFLECTED_TYPE; This is all that is needed to send and receive the message in C++ code. Note: The code above does not add reflection for each message member, as that is not necessary to make this message work. However, if you want to send and receive this message from non-C++ code, for example from TypeScript, then reflecting the members is necessary for the language binding to work. Be aware though, that language bindings may not support all types of reflected members and would ignore those. Message Handlers For each message type that your component is supposed to receive, you need to add a function that takes the respective component types as the only argument: void DisplayMsgComponent::OnSetText(ezMsgSetText& msg) { m_sCurrentText = msg.m_sText; } void DisplayMsgComponent::OnSetColor(ezMsgSetColor& msg) { m_TextColor = msg.m_Color; } Finally, you also need to register these functions as message handlers, in the component's reflection information: EZ_BEGIN_MESSAGEHANDLERS { EZ_MESSAGE_HANDLER(ezMsgSetText, OnSetText), EZ_MESSAGE_HANDLER(ezMsgSetColor, OnSetColor) } EZ_END_MESSAGEHANDLERS; Now this component is ready to receive messages of those types. Sending Messages To send a message, first create an instance on the stack (don't heap allocate them) and then call one of the SendMessage() functions: ezMsgSetText textMsg; textMsg.m_sText = m_TextArray[idx]; pGameObject->SendMessageRecursive(textMsg); Message Routing There are several ways a message can be sent. Which function you use determines which components may see the message and also how efficient the delivery will be. You can send messages either through an ezGameObject, through an ezComponent or through an ezWorld. It is differentiated between sending a message (direct) and posting a message (delayed). Messages sent through functions on ezComponent will always only be received by exactly that component and no one else. Messages sent through ezGameObject are broadcast to all components on that object. If one of the Recursive variants is used, the messages are additionally delivered to all components on all child objects. When sending messages through ezWorld, you identify the target through a handle (ezGameObjectHandle or ezComponentHandle). In this case the world takes care of resolving the handle for you. If the target object does not exist anymore, the message won't be handled by anyone. If you call SendMessage(), the message is delivered immediately. That also means that all message handlers will access the same message object. This can be used to query information, as the message handler can write data back to the message. If the message is meant to be sent to multiple receivers, the code must take care to properly append or aggregate the results. Messages are never delivered multi-threaded, though. If you call PostMessage(), the message is delivered delayed. These messages are queued and delivered when their time has come. If the target does not exist anymore at that time, the message is discarded with no effect. Posted messages can't be used to retrieve a result. Internally, posted messages will be copied, so you still don't need to allocate them on the heap. Apart from a time delay, when posting a message you also have to specify a phase in which to deliver the message (see ezObjectMsgQueueType). This is used for special cases, where you want to tightly control at what time during the world update the message should arrive. For most cases using ezObjectMsgQueueType::NextFrame is the right choice. Event Messages Regular messages are used to 'instruct' components to do something. For example to switch something on, or to apply a physical force to it. These things can be implemented differently by different component types, but generally the calling code assumes them to do something. When messages are broadcast (instead of sending them to a single component directly), all targeted components get the message. Event messages on the other hand, are used to 'inform' an object hierarchy that something happened inside that hierarchy. An example would be ezMsgDamage which is used to inform an object that it received damage. The difference is mainly in the routing. Regular messages are either sent directly to the recipient or to all its children. Event messages are delivered to a node or its closest parent node that has a message handler for this type of message. The idea is, that for complex objects you typically want to have a single script at the top of the hierarchy that deals with all everything that's happening below. For example an NPC may have many different child nodes, but if an ezMsgDamage is sent to any of them, the script at the top should decide what to do about it. Any message can be sent as an event by using ezGameObject::SendEventMessage(), however, messages that are meant to be always treated as events should derive from ezEventMessage, so that they include additional information. Finally, there is the ezEventMessageHandlerComponent interface, which is only implemented by very few component types. Out of the box, only by ezTypeScriptComponent and ezVisualScriptComponent. When an ezEventMessageHandlerComponent is attached to a node, it will receive all event messages below that node hierarchy, no matter whether it has a message handler for it or not. It therefore prevents event messages from leaving the hierarchy. If an event message is supposed to 'bubble up' further, the message handler component must either forward the message manually or be configured to pass-through all unhandled event messages. Example: Gas Cylinder This prefab has two physics shapes. One for the body, one for the cap. If the body receives too much damage, the cylinder is supposed to explode. If the cap receives enough damage, the cylinder is supposed to start ventilating burning gas, fly off and explode after a while. To implement this, we attach a script component to the very top of the hierarchy. The script contains a message handler for ezMsgDamage. When a projectile hits the cylinder, it calls ezGameObject::SendEventMessage() on the node whose physics shape it hit. The event message contains the information through which game object and which component it was sent. The message is then delivered to the closest parent node that handles event messages, which in this case is the GasCylinder node, because it has the script attached. The script can then differentiate what child node was hit and implement the desired game logic: Declaring Event Messages Messages that are always sent as events should derive from ezEventMessage, however, this is not mandatory. Sending Event Messages You can send messages as events to every object using ezGameObject::SendEventMessage(). This will determine the closest parent to handle events on the fly and deliver the message accordingly. For components that regularly send events to the same object hierarchy (their own), such as trigger components, it is more efficient to have a member of type ezEventMessageSender<>. Sending a message through this object will cache the receiving target and be more efficient the second time. If a component that is an ezEventMessageHandlerComponent itself wants to send an event message further up the hierarchy, it has to send the event to its own parent node. Caution: Event messages are also just regular messages and can be sent that way using SendMessage() or PostMessage(). If you accidentally use those functions, rather than SendEventMessage() or PostEventMessage(), your message will not get delivered as intended. Global Event Message Handlers If an event is sent to a hierarchy that does not handle it, it is ultimately delivered to a global event handler. A global event handler is simply an event handler component that has the HandleGlobalEvents property enabled. A global event handler can be useful as a catch-all level script. This way you can place buttons around a level, and have a single script that receives the message when one of them is pressed. Each button has its own script to implement its logic (when you can press it, how it changes its appearance and so on), but the button script then just raises a generic \"button pressed\" event on its own parent node. If those buttons don't have an event handling parent node, the message is delivered to the level script, which can then handle the logic of all those buttons. Be careful though when using multiple global event handler components. Every type of message is only delivered to a single handler, so as long as each global handler takes care of a different type, it will work as expected. Message Serialization For regular messages you don't need to implement any serialization, as they are short lived within the same process. However, if you intend to record messages or send them across a network, you can utilize ezMessage::PackageForTransfer() and ezMessage::ReplicatePackedMessage(). To make these functions work, you need to override and implement ezMessage::Serialize() and ezMessage::Deserialize(). See Also The World / Scenegraph System Sample Game Plugin Forward Events To Game State Component"
  },
  "pages/docs/runtime/world/world-modules.html": {
    "href": "pages/docs/runtime/world/world-modules.html",
    "title": "World Modules | ezEngine",
    "keywords": "World Modules World modules are systems that are used to update certain aspects of a world. There can be only one instance of each world module for each world. A good example for a world module is the ezJoltWorldModule. This module is responsible for updating the physics world every frame. To do so, it hooks into two update phases of the world, once early in the frame, where it kicks off the physics simulation in a parallel task, and once late in the frame, where it fetches the results of the simulation and applies them to the world. Components represent individual pieces in the world. World modules represent large systems that provide the foundation for the components to work. World modules are frequently needed when integrating third party systems that require per frame updates to function. Creating and Instantiating World Modules You create a new world module class by deriving from ezWorldModule. You never instantiate world modules yourself. Instead, call ezWorld::GetOrCreateModule(). This will allocate the desired world module if necessary. Consequently, if no code path ever calls ezWorld::GetOrCreateModule(), the respective world module will never be instantiated. Therefore, the lifetime and existence of a world module is often coupled to some component. Once a component is added to a world, its respective component manager (which also is a world module) is automatically instantiated. If those components request access to another world module, that will be instantiated, as well. Only few systems require a world module, without having some component type that would request its instantiation. For example, there is no need to instantiate a physics world module, if the scene doesn't contain any physics component. If you do need a system that is always running, consider putting it into a game state. And if you determine it really does need to be a world module, a custom game state may be the right place to do the initial call to ezWorld::GetOrCreateModule() to instantiate the system. The more common approach, though, is to have a custom component type, which ensures to set up a world module. You would then put a single component of this type into each world. This also allows you to have properties on the component, with which you can configure the world module. Example: Wind World Module Code can query for the ezWindWorldModuleInterface using ezWorld::GetWorldModule<ezWindWorldModuleInterface>(). If a world module that implements this interface exists, the function will return a valid pointer. Things like particle effects can then ask the system for a wind value at their location, to apply wind to particles. Wind can be implemented in different ways. From full 3D fluid simulations with turbulence, over simpler models, down to entirely basic models with just a randomly changing wind vector. What implementation you want may depend on your scene. Therefore, you choose the wind module by adding a corresponding component to the level. Out of the box you can have either no wind, or very simple wind. By adding an ezSimpleWindComponent to a scene, that component will make sure a wind module of type ezSimpleWindWorldModule is instantiated. Through the component's properties you can configure how the wind behaves. If you want different wind behavior, you can add your own implementation of ezWindWorldModuleInterface through a plugin. You would then add your own wind component, which instantiates and configures your custom wind module. Update Functions The main feature of world modules is that they can hook into the world update and execute code at specific points. To do so, they need to register update functions using ezWorldModule::RegisterUpdateFunction(). This should be done during ezWorldModule::Initialize(). To register an update function, you need to fill out an UpdateFunctionDesc. This takes a delegate to the actual function that should be called, and requires you to give a unique name to that function. This way, other world modules can refer to your update function by name. This is useful, when you have dependencies between world modules. Say you need to run one part of the physics update, then a specific animation update and finally another part of the physics update. You can do so, by registering three update functions and set up dependencies. The world will then execute the update functions in the required order. Update Phases An important aspect of the update functions is in which update phase of the world they are executed. These are the steps in which the world is updated: Pre-async phase: The corresponding update functions are called synchronously in the order of their dependencies. Async phase: The update functions are called in batches, asynchronously on multiple threads. There is no guarantee in which order they are called. It is not allowed to access any data other than the components' own data during this phase. Post-async phase: Another synchronous phase like the pre-async phase. Object deletion: Dead objects and components are removed. Transform update: The global transformation of all dynamic objects is updated. Post-transform phase: Another synchronous phase like the pre-async phase, after the global transformation has been updated. The choice in which phase to run an update function affects performance, how you can access other components, and how recent some data is that you read. Many things must be updated in a single-threaded way. These would typically be done in the pre-async phase. Since everything runs single-threaded here, you can access other components, both to read and to modify them. If you have something that operates solely on the data of a single component and would be safe to be executed for multiple components at the same time, you should put this into the async phase. Your update function will automatically be distributed across multiple threads to speed things up. If you do have an async update, you may need to finalize or clean up some data afterwards, but in a single threaded way. Use the post async phase for that. In all of these phases you can modify the owner game object's local transform, but when you read the global transform you will get the value from the previous frame. For most use cases this is sufficient, but in a few cases you must have the absolutely latest global transform, to prevent things from lagging a frame behind. For those cases you use the post transform phase. Here you can read the latest global transform value that will be used by the renderer. You can still modify the local transform here, but it won't have an effect until the next frame. See Also Component Managers The World / Scenegraph System"
  },
  "pages/docs/runtime/world/world-overview.html": {
    "href": "pages/docs/runtime/world/world-overview.html",
    "title": "The World / Scenegraph System | ezEngine",
    "keywords": "The World / Scenegraph System When you build a scene in the editor or through code, the structure of all the objects is stored in something that is commonly referred to as a scenegraph. In EZ the scenegraph is implemented by the class ezWorld, which is why the terms scenegraph and world are used interchangeably in our documentation. ECS In EZ we use a variation of an Entity Component System (ECS). It doesn't matter whether you are familiar with ECSs, but if you are, the main difference of our implementation to a pure ECS is, that in EZ there is always exactly one system to handle each component type. You can have additional systems (see World Modules), however, this is not as common as in other engines. The main classes involved are ezWorld, ezGameObject, ezComponent and ezWorldModule / ezComponentManager. ezWorld Each ezWorld represents the entire state of a scene. Worlds hold all game objects and all world modules, which in turn hold all components. Each world has its own simulation state, such as a clock and a random number generator. Through the world modules, worlds also hold their own state for other simulation aspects, such as physics. You can have multiple worlds in parallel and they will be completely separated. This is for example the case when you have multiple documents open in the editor. Worlds are described in more detail in this chapter. ezGameObject ezGameObject is our entity class. The terms entity, game object and node are used interchangeably. Every game object has a position, rotation and scale. It may be attached to a single parent game object and it may have multiple game objects attached as children. The game object hierarchy is a directed acyclic graph (DAG). Game objects by themselves do not have any behavior. Instead, each game object can have an arbitrary number of components attached. The object's transform (position, rotation, scale) is local to its parent node, but it also holds a global transform, which is computed by concatenating the transformations of all parent nodes. Every time a game object or any of its parent nodes is moved, this global transform is updated. Game objects are described in more detail in this chapter. ezComponent Components can be attached to game objects. They bring their own data and functionality. Components are used to implement behavior. For example light source components are used to tell the renderer how to light the scene, physics components are used to make objects collide with each other and AI components let creatures run around. By attaching components to game objects, you configure how that game object behaves. Components can interact with or depend on each other. For example a physics actor component would make an object fall to the ground, but it also needs a physics shape component to know whether the object should behave like a box, a sphere or something else. Components are described in more detail in this chapter. ezWorldModule / ezComponentManager World modules are the systems of the ECS pattern. Worlds are updated in multiple phases. Some phases are multi-threaded, others aren't. World modules can hook into these phases and make sure that they are called at the right time. World modules implement things like stepping third party code (e.g. physics). The most common type of world modules are component managers. Each component type has its own component manager, which is responsible for updating those components. The manager can leverage knowledge from other sources for determining which components need updating, and it can easily update components in a multi-threaded fashion, if it is save to do so. World modules are described in more detail in this chapter and component managers in this chapter. Object Lifetime The EZ scenegraph does not use any kind of reference counting or garbage collection, however it does provide weak reference semantics through handles, to enable you to delete objects exactly when you need them to be removed, while still being able to detect whether an object is still alive. See the object lifetime chapter for details. Custom Components A large part of writing your own game, is to write your own components. If you need maximum control and performance, you need to write your components in C++. You can also write components in TypeScript. Their functionality is very similar but a bit more limited. It is possible to use both and communicate between Typescript and C++ components using messages. Messaging When a component gets updated, it can access other components and call functions on them. Of course that requires that the other component type is known at compile time. In practice, that is often not the case. Take the projectile component as an example. Whenever a projectile hits something, it should apply damage to the hit object. However, what it hit was just the physical representation of an object (e.g. a physics actor). The physics object doesn't have a concept of 'receiving damage' and therefore calling some 'OnDamage' function on the physics component makes no sense. Instead, on the object that has the physics component, there may be another component which knows how it would react to damage, so we want to send the information there. That component may be a custom component, though, which the projectile component knows nothing about, so there is no way to call a function on that. To solve this problem, you can send messages to components. A message is a class derived from ezMessage and it can contain arbitrary data. Each component registers message handlers for all the types of messages that it wants to receive. When our projectile component now hits some object, it simply sends a damage message to that object. The engine will then deliver that message to all components which have a matching message handler. The message can be delivered right away, in which case a result can be written back to the message, or with a delay. Using messages decouples code, as components that know nothing of each other can still communicate and interact. The message system is also highly optimized for best performance. Messages are described in more detail in this chapter. Spatial System The world also sorts objects into a spatial system, to enable efficient queries for which objects are within a certain area. Although this is the basis for frustum culling in the renderer, it is also available to other systems. See this chapter for details. See Also Worlds Game Objects Components World Modules Component Managers Messaging Custom Code"
  },
  "pages/docs/runtime/world/worlds.html": {
    "href": "pages/docs/runtime/world/worlds.html",
    "title": "Worlds | ezEngine",
    "keywords": "Worlds For an introduction what a world is and how it fits into the overall picture, see The World / Scenegraph System. This documentation focuses on the C++ ezWorld class. The functionality exposed through other mechanisms, such as TypeScript, may be more limited in scope, but ultimately maps to the C++ implementation. Game Objects Game objects are allocated, destroyed and accessed through the world. For these details, see the chapter about game objects. Components Components are not directly managed by a world. Instead, worlds manage world modules and component managers, which in turn manage components. For details, see the chapter about components. World Modules World modules are bigger systems that manage aspects like particle effects, the Jolt integration, wind and so on. Component managers are a special type of world modules that take care of updating the various component types. Simulation State Each world has its own simulation state, to not affect other worlds. Simulation Enabled Every world can be actively simulated, or paused. ezWorld::SetWorldSimulationEnabled() is used to toggle this. Clock Each world has its own ezClock which can be retrieved through ezWorld::GetClock(). The clock is used to speed up or slow down the simulation or to set a fixed update rate. The clock keeps track of the 'game time', so when a component needs to know the current time, it should query this from the world's clock. Random Number Generator When a component needs a random number, it should query this from the world via ezWorld::GetRandomNumberGenerator(). Components or better, component managers can of course also have their own random number generator, for example when they need multi-threaded access to it, or when they want to control the seed value for determinism. The particle systems, for example, do this to achieve deterministic results. Coordinate System The default coordinate system in EZ is: +X is 'forwards' +Y is 'right` +Z is 'up' That makes it a left-handed coordinate system. You can query these axis in the space of a game object, if you need to. The coordinate system can be changed through ezWorld::SetCoordinateSystemProvider(). The ezCoordinateSystemProvider can potentially return a different coordinate system at different locations, so you could implement a provider that changes the coordinate system to follow a sphere. Warning: Although components are supposed to not hard-code assumptions about which axis is 'forward', etc, using a non-default coordinate system is not well tested. Using a dynamic coordinate system even less so. Read / Write Access Control Some aspects of the world are updated in a multi-threaded fashion. For instance, rendering generally happens in parallel to other updates. To prevent you from accessing the world in a problematic way, you need to lock the world for reading or writing when you work with it. From within a component update function you don't need to worry, you always have write access to the world while components are being updated. However, if for example you want to load a level or otherwise set it up procedurally at launch, you need to lock it for write access: EZ_LOCK(pWorld->GetWriteMarker()); pWorld->CreateObject(...) In developer builds the world will check that you have properly locked it when you try to do certain operations. Therefore, the best way to know where to add such locks, is simply to run your code without a lock and see whether the engine asserts. If so, you can just traverse your callstack to find a reasonable place to insert the lock. World Update To step your world, call ezWorld::Update(). The time delta that will be applied depends on whether the world simulation is enabled and how your world clock is configured. See Also Game Objects Components World Modules Object Lifetime Messaging"
  },
  "pages/docs/scenes/advanced-object-transform.html": {
    "href": "pages/docs/scenes/advanced-object-transform.html",
    "title": "Advanced Object Transforms | ezEngine",
    "keywords": "Advanced Object Transforms This page describes advanced methods to modify scene objects. Duplicate The Duplicate dialog enables you to quickly create a vast number of copies of the selected objects, while adjusting their position and rotation. The duplicate dialog can be opened with CTRL+D. Although the dialog has many options, in practice you typically only need to specify the number of desired duplicates and the Translation Step. The translation step is a fixed position offset that is added onto each new instance, placing the objects along a line. The Random Translation Offset can be used to randomly add some variation to each object's position. In the example above, an additional random rotation is applied to each object. The result of this operation is shown below: You can also use Revolve around Axis. If this is enabled, the selected object is the center point and all duplicates will be created at distance Radius, revolving around the center, using an arc difference of Angle Step for each duplicate. If you additionally set a Rotation Step, the objects will line up as in the the example below: Video: How to duplicate objects Delta Transform The Delta Transform dialog can be opened with CTRL+M. It can be used to move, rotate or scale objects precisely by typing in the desired amount of change. Its true power, however, lies in adding random variation to objects, to make them look more natural. The image below shows plants in a perfectly regular grid: Using the delta transform dialog, we can make this look much better. The drop down in the top left selects the transform mode. (random) means that the change on each object will be truly random. (deviation) means each object will be randomly adjusted using the given value as wiggle room, however, the amount of change follows a Normal distribution, meaning that extreme changes will appear far less often than moderate changes. This should be preferred for things that are supposed to look natural. The drop down in the top right specifies whether the transform changes are done in World Space, or in Local Space, and whether they should be evaluated for each object individually, or only for the last selected item (as the pivot). The undo button in the bottom left allows you to quickly try and undo various options. Here, we modify the position, by applying a deviation of up to 0.5 along the X and Y axis. For this operation it could be done both in world space, as well as in local space for each item. As a result, the plants look less like they are on a grid. You could click Apply multiple times, if you want to randomize the result even further. Next, we randomly rotate all objects around the Z axis. Here we don't use a deviation, but a random change, and a rotation of [-180; +180] degree, as each object should indeed be truly randomly rotated. Note that this change is applied in Local Space - Each Item. As a result, the plants now don't have a uniform rotation anymore: Finally, we apply a 'natural' tilt to all objects. This is a deviation away from the Z axis and simulates that not all objects (especially plants) are perfectly straight. Now our small patch of plants looks much less artificial: You could go even further and apply a random scale to each object, as well. See Also Editing Gizmos Scene Editing"
  },
  "pages/docs/scenes/editor-camera.html": {
    "href": "pages/docs/scenes/editor-camera.html",
    "title": "Editor Camera | ezEngine",
    "keywords": "Editor Camera This article describes how to use the editor camera. Camera Controls In the following LMB refers to the left mouse button, RMB to the right mouse button and MMB to the middle mouse button. Perspective Views LMB: Move forwards/backwards and sideways MMB: Move in the ground plane Wheel: Move forwards/backwards RMB: Activate fly camera and look around WASD: Fly around Q and E: Fly straight up or down SHIFT: Move faster LMB and RMB: Pan CTRL + Wheel: Change the camera's movement speed. This can also be changed using the Camera Speed slider in the toolbar. C: Move the camera to the pointed at position F: Frame the currently selected objects Pressing F once will only pan the camera towards the selected objects Pressing F a second time will additionally zoom in on them SHIFT + F: Same as F but frames the object in all views simultaneously This can also be triggered by double-clicking an item in the Scenegraph ALT + LMB: Orbit around the last framed object ALT + RMB: Dolly (same as move forwards/backwards just with inverted mouse) ALT + MMB: Pan (inverted) Context menu > Align Camera with Object: Orients the camera with the selected object Orthographic Views RMB: Pan the selected view Wheel: Zoom F and SHIFT + F: Frame object, same as in perspective view Show/Hide Objects To temporarily focus on certain objects, it is possible to make objects invisible. H: Hide the selected objects CTRL + H: Show all hidden objects SHIFT + H: Hide all objects that are not selected Note that 'hide unselected' may hide lighting nodes, which can turn your level very dark. You can either activate ambient lighting in your scene, or switch the render mode to 'Diffuse Color', if necessary. The hidden state of objects is not saved in the scene. Also Play-the-Game mode and ezPlayer always show all objects. Similarly, the hidden state only excludes objects from rendering, not from simulation. Favorite Cameras You can store up to ten favorite editor camera positions using CTRL + 0-9. You can then jump back to that position by pressing the respective number key. These camera positions are saved per user, per scene. If you open the editor on the same computer at a later time, the camera positions are available again. They are not saved with the project, though, so you cannot share these positions with others. Use level cameras for that. The favorite camera actions can also be found in the menu Scene > Favorite Cameras > ... Level Cameras If you have a camera component in your scene, you can assign it a shortcut number in its properties. You can then jump to that location using ALT + 0-9. If multiple camera components use the same number, it is undefined to which one the editor camera will move. Moving the editor camera to a camera component does not change the selected object. You can also create a camera component in the scene at the current editor camera location and assign it a shortcut, by pressing CTRL + ALT + 0-9. Since level cameras are simply objects in the scene, they will be saved in the scene and therefore are shared with others. This can be used both for game play relevant cameras, as well as to just save some useful locations. Camera components that are not actively used for rendering, have no performance impact. The level camera actions can also be found in the menu Scene > Favorite Cameras > ... Field of View The editor camera uses a fixed field of view (FOV). The FOV can be changed in the preferences. Video See Also Editor Settings Editing Views"
  },
  "pages/docs/scenes/exposed-parameters.html": {
    "href": "pages/docs/scenes/exposed-parameters.html",
    "title": "Exposed Parameters | ezEngine",
    "keywords": "Exposed Parameters Some asset types support so called exposed parameters. That means that an asset, such as a particle effect, a prefab or a script may provide parameters, that can be set through a corresponding component, such that each instance of the asset acts differently. As an example, particle effects may expose a parameter that allows you to adjust their color. Now every time you add that particle effect to a scene, you can select the color for that particular instance, through the particle effect component. Similarly, scripts can expose parameters, which allow you to configure the starting conditions of the script. Using exposed parameters to make an asset more versatile is usually less work and more efficient, than to build multiple variants of an asset. Exposed Prefab Parameters In prefabs you can expose most types of properties of any component. When a prefab has any exposed parameters, they show up in the prefab reference component like this: The X button to the right of each parameter lets you reset their value to the default. To expose a property inside a prefab as a parameter, just right-click on the label of a property and choose Expose as Parameter: You will be asked under what name the property shall be exposed. This allows you to select a more meaningful name for the purpose of the parameter. You are also allowed to use the same name for multiple exposed parameters, which means that the user only sees one parameter, but it may be bound to multiple internal properties. Every prefab document has a Prefab Settings panel. This lists all the currently exposed parameters: Here you can remove or rename an exposed parameter. Exposing Exposed Parameters It is perfectly valid to expose a property as a prefab parameter, which itself is already an exposed parameter from a nested prefab, a particle effect or a script. Exposing Object References If a prefab contains an object that requires an object reference to operate on, you can expose the object reference, and thus allow users of your prefab to let each prefab instance operate on different objects. See Also Object References Particle Effect Component TypeScript Component Prefabs"
  },
  "pages/docs/scenes/gizmos.html": {
    "href": "pages/docs/scenes/gizmos.html",
    "title": "Editing Gizmos | ezEngine",
    "keywords": "Editing Gizmos This page describes the most common tools to interact with objects in a scene. Standard Gizmos The standard gizmos are Translate (W), Rotate (E), Scale (R) and Drag-To-Position (T). They can be disabled using the Q key. Click and drag a gizmo to modify the selected objects. The Drag-To-Position gizmo moves the selected object to the position that you point at. If you select one of the six handles, instead of its center, it will additionally align the selected axis with the surface normal of what you point at. The translate and the rotate gizmo may operate either in local space (object space) or in global space (world space). You can toggle the space either using the world icon from the toolbar, or by selecting the tool again. For instance, press W once to enable the translate gizmo, press W again to toggle the space. Modifiers Hold ALT while dragging a gizmo to disable snapping. Hold SHIFT before clicking a gizmo to duplicate the object in place. This works for all but the scale gizmo. Hold CTRL while translating an object to move the camera in conjunction. All modifiers can be combined. Gizmos in Orthographic Views When the Translate, Rotate or Scale gizmo is active, holding LMB and moving the mouse will modify objects. The perspective of the selected view (top-down, front, right) determines along which axis the object will be translated or rotated (always in global space). Scale will always be uniform. Note: The 3D gizmos are not displayed in orthographic views, just left-clicking anywhere in the view will perform the selected action. Snap Settings Press End or the respective icon from the toolbar to open the snap settings: These affect not only the gizmos, but also the positioning of assets dragged from the asset browser into the scene. Grid The grid can be toggled with the G key or the grid icon from the toolbar. If enabled, the grid shows up for the transform gizmo and in greyboxing mode. For the translate gizmo you can choose in which plane the grid is shown, by clicking one of its quads. Afterwards you can still move along the orthogonal axis by dragging the respective axis handle. The density of the grid shows the current position snap value. If position snap is disabled, the grid will not show. Manipulators Manipulators are component and property specific gizmos. Properties of a component that can be changed with a manipulator, are highlighted in blue (off) or violet (on). You can click the property label to toggle the manipulator mode. Once a manipulator is enabled, all Standard Gizmos are disabled. You can now also use the Q key to toggle manipulator mode off and on. Manipulators in Orthographic Views Most manipulators will not be available in orthographic views. Visualizers Some components use visualizers to make some of their aspects more obvious, such as the cone of the spotlight in the image below. These visualizers are only drawn for selected objects and can be toggled using the V key. When visualizers are enabled, the editor will also display a yellow bounding box around each selected object. Video See Also Selecting Objects Greyboxing Scene Editing"
  },
  "pages/docs/scenes/greyboxing.html": {
    "href": "pages/docs/scenes/greyboxing.html",
    "title": "Greyboxing | ezEngine",
    "keywords": "Greyboxing Greyboxing (or whiteboxing) is the process of blocking out a rough level concept to test ideas, before working on the details. The EZ editor provides a dedicated tool to quickly create the most commonly needed geometric shapes to block out a level, such as boxes, stairs, ramps and columns. Video: How to use the greyboxing tool Creating Shapes To activate the greyboxing tool, press the B key or click the brick icon in the toolbar. The statusbar will now display how to proceed. Hold CTRL. If the grid is enabled, it will show in which plane you will draw. Now left-click and draw a rectangle. Release the left mouse button to finish the rectangle. Now move the mouse up and down to choose the height of the box. Left-click once more to finish the shape. From the object properties you can now select a different shape. Activate the manipulator to easily adjust the shape. Creating Shapes in Air By default you will draw the next shape starting at the picked position under your mouse cursor. If you want to draw a series of platforms in air, you can reuse the height of the previously drawn shape by holding CTRL and SHIFT before drawing the next rectangle. Materials You can change the material of a greybox shape either through its properties, or by dragging and dropping a material from the asset browser onto the shape. Additionally, if you have a material selected in the asset browser while creating a new shape, it will automatically get that material assigned. Static Collision As long as a greyboxing object has the GenerateCollision property set, it will automatically get a collision mesh with the default collision layer (0). It's assigned material determines which surface is used for physical interactions. Dynamic Collision The greyboxing shape is not meant to be used for dynamic collisions. That means, if you attach a dynamic Jolt actor, it will not get the necessary physical setup to behave correctly. Instead, its collision mesh will simply be disabled entirely. You can therefore use greybox shapes for dynamic objects, but you need to add the required physics shapes yourself. Occlusion Greyboxing geometry can act as occluders for occlusion culling. By default this is enabled for all greyboxing geometry, but it should be disabled for small objects and objects that are unlikely to occlude much. Also consider disabling it for more detailed geometry. If you need an invisible occluder, use an occluder component instead. See Also Scene Editing Editing Gizmos Advanced Object Transforms"
  },
  "pages/docs/scenes/object-references.html": {
    "href": "pages/docs/scenes/object-references.html",
    "title": "Object References | ezEngine",
    "keywords": "Object References Some components are supposed to work with other objects, but those objects are not in their hierarchy as a parent or child node, but may instead be any arbitrary object that the user wants to select. Such use cases are supported through object references, meaning that a component can have properties that store a reference to another object (within the same scene or prefab document). A very basic example is the DrawLineToObject component, which literally just draws a line between itself and a referenced object. Object Reference UI In the editor UI an object reference property looks like this: Left-clicking the arrow button will switch to object selection mode and turn the mouse cursor into a crosshair. Click on any object in the 3D viewport to select it as the referenced object. Important: You can also right-click the arrow button to bring up a menu with additional options. Be aware that you can right click any node in the tree view or in the viewport and select Copy Object Reference, then use Paste Object Reference in the menu above to set the reference. This is sometimes easier than clicking on an object in the viewport. Object References in Custom Components When you create a custom component you may want to have an object reference property. The easiest way to achieve this, is to get inspiration from existing code, such as ezLineToComponent. Object references are a 'non-trivial' (complicated) feature. They need to be remapped within instances of prefabs, they work differently in the editor and the runtime, and it is possible to use them as exposed parameters. Consequently, you have to stick to a certain pattern to make them work. Expose them as a string property and use custom functions as accessors. Also decorate them with ezGameObjectReferenceAttribute: EZ_ACCESSOR_PROPERTY(\"Target\", GetLineToTargetGuid, SetLineToTargetGuid)->AddAttributes(new ezGameObjectReferenceAttribute()), The 'getter' accessor function is actually never called, but since EZ_ACCESSOR_PROPERTY expects a valid function, you need to have at least one dummy function that you can pass in. In your 'setter' function you need to query a 'reference resolver' from the world and use that to map the string to an actual game object handle. Here you also need to pass in information about the component and the property. This is mainly needed by the editor to handle undo/redo correctly. void ezLineToComponent::SetLineToTargetGuid(const char* szTargetGuid) { auto resolver = GetWorld()->GetGameObjectReferenceResolver(); if (resolver.IsValid()) { // tell the resolver our component handle and the name of the property for the object reference m_hTargetObject = resolver(szTargetGuid, GetHandle(), \"Target\"); } } const char* ezLineToComponent::GetLineToTargetGuid() const { // this function is never called return nullptr; } Finally, during component serialization you just use ezWorldWriter::WriteGameObjectHandle() and ezWorldReader::ReadGameObjectHandle() to save and restore the actual game object handle. Limitations Object references are not possible across scene layers. See Also Scene Editing Scene Layers"
  },
  "pages/docs/scenes/scene-editing.html": {
    "href": "pages/docs/scenes/scene-editing.html",
    "title": "Scene Editing | ezEngine",
    "keywords": "Scene Editing A scene or level is a document like all assets. To edit a scene there are multiple tools available. This page gives a broad overview what tools there are and in what order to try them out. Basics Open a Project and Scene You can start editing an existing project and scene, or you can create your own. See the article about projects for how to do so. We suggest to start by looking at the Testing Chambers project, even if you then use an empty scene, as there are some useful prefabs in that project. If you don't have a scene open yet, the easiest way is to set the Filter in the asset browser to Scene and then double click one of the scenes that belong to that project. First Steps The very first thing you should familiarize yourself with is how to move the editor camera around the scene. The chapter about camera controls lists all the available options. Next, try out selecting objects. With one or multiple objects selected, you can try out the editing gizmos. Add Objects The easiest way to add objects to a scene is to drag and drop existing items from the asset browser into the scene. You would mostly do this for meshes and prefabs, but it works for many asset types. If you want to get your own assets into the editor, you should read up on how to import assets. Create Objects You can, of course, also build objects from the ground up by creating empty game objects and components. Game objects mostly specify the position of an object, components give objects behavior. The easiest way to create a new game object is to point somewhere with the mouse cursor and press Ctrl + Shift + X. This will create a new game object at the position you pointed at. It also attaches a Shape Icon component, whose sole purpose is to make that new object visible and selectable. You can remove that component as soon as you have no need for it anymore. You can also right-click anywhere in the scene tree and select Create Empty Child Object. When you copy an object (Ctrl + C), you can also point anywhere and paste (Ctrl + V) to create a duplicate at that position. Editing Object Hierarchies The way objects are parented to each other is often very important. There are multiple ways to change the hierarchy: Drag and drop: In the Scenegraph panel, you can drag and drop a node onto another node to parent it to that. Right-click -> Detach: Selecting Detach from the context menu, will make a node a top-level game object that has no parent. Right-click -> Attach to This: Selecting Attach to This from the context menu, will make the selected node a child of the object that you pointed at in the viewport. Paste As Child: When you copied an object, you can select a desired parent node and then use Paste As Child from the context menu, to paste a new object and attach it to the selected object right away. Video: How to parent objects Blocking Out a Scene To get some geometry into your scene, you can use the greyboxing tool. The nice thing about greyboxing geometry is, that it automatically sets up colliders, as well, thus once you try to play your game, you won't fall through the geometry. Play You can now try to run your scene. If you use the Testing Chambers sample project, and you created an empty scene, make sure to add a Player Start Point and make it reference the Player prefab. This way when you use Play-the-Game mode, you get a first person shooter game play experience. Advanced Now that you know the basics, you can explore some of the more advanced functionality. Views Apart from the single 3D perspective view, there are also orthographic views and different types of render modes. This article describes all the details. Materials Materials are what is used to give objects a texture. There is much more to this and the chapter about materials lists all the details, but for the time being you can get away with just the most simple material setup. For instance, when you import a mesh, it may add materials automatically for you. Usually you just need to make sure that the paths to the referenced textures are correct. Physics The fun really starts once you get to play around with physics. Read up on the Jolt Physics integration for this. For a basic setup you always need at least an actor component (static for scene geometry, dynamic for stuff that should fall down) and a shape component to give the object a physical shape. Advanced Editing Once you have one physically correct falling box, you may want to have one hundred. Using these tools, you are only a few clicks away from that. Scene Layers Once your scene starts to become bigger, you may want to use scene layers for better organization. Video Next Steps Now that you are familiar with how to create, edit and test a basic scene, there are many other things to explore. Decals and particle effects can make your scene more interesting. Proper sounds add a lot of atmosphere. And finally, by writing custom code, either in C++ or through scripting, you will bring your own game idea to life. See Also Asset Browser Editor Camera Editing Gizmos Greyboxing Running a Scene"
  },
  "pages/docs/scenes/scene-layers.html": {
    "href": "pages/docs/scenes/scene-layers.html",
    "title": "Scene Layers | ezEngine",
    "keywords": "Scene Layers By default all objects that you create in a scene end up in the same overall structure and are saved in the same .ezScene file. Since scenes quickly become full of objects, it can become difficult to keep things organized in a useful way. Additionally, since all objects are stored in the same file, it is not possible to have multiple people edit the same scene simultaneously. Instead, you have to make sure that each scene is always only edited by one person at a time, and then synchronized to others, before they can make a change. Scene layers are a concept that is meant to help both with generally organizing a scene, as well as splitting it up in a way that enables people to collaboratively work on it to some degree. Managing Scene Layers Scene layers are managed through the Layers panel: Here a scene is divided up into 4 different layers. Each layer is stored on disk as a dedicated .ezSceneLayer file, except for the main layer, which is stored in the .ezScene file itself. Layers are added and deleted through the context menu: Note: Once a layer is created, it can't be renamed (other than renaming the file on disk manually). Another option is to create a new layer, and move all objects there, then delete the old one. Main Layer Every scene has exactly one main layer, the one that is represented by the .ezScene file itself. The main layer always has the name of the scene itself. It is the only layer that can't be unloaded and it stores references to all the other layers. Thus adding or removing a layer is always a modification of the main layer as well. Active Layer The layer that is selected in the Layers panel is considered to be the active layer. The Scenegraph panel displays the list of objects that are part of the active layer. When you select an object in the viewport, the active layer is automatically set to be the one that contains the selected object. Layer Visibility The eye icon next to each layer indicates whether a layer is visible. Click the icon or use the context menu to toggle the layer's visibility. When a layer is set to invisible, all objects in it are hidden during editing. This is effectively the same as hiding all objects, though it is more convenient when you put objects that often need to be hidden, into their own layer. Load / Unload Layers Every layer is either loaded or unloaded. By default all layers are loaded. Layers can be unloaded through the context menu or by clicking the folder icon next to them in the Layers panel. If you unload a layer, all its objects get removed from the scene. It is then not possible to select or edit any of them, and selecting that layer won't make it active. Unloading layers thus can be used as a way to not only hide objects, but to properly remove them (temporarily). This can be beneficial in large scenes, to improve performance. Moving Objects between Layers When you create a new object, it is always put into the active layer. To move an object to another layer, drag and drop it from the Scenegraph panel onto another layer in the Layers panel. Undo/Redo across Layers Every layer has its own undo stack, meaning the list of operations that were executed while it was active. Clicking undo will change only the active layer, never any other layer, and it will not switch to another layer either. Operations such as moving an object from one layer to another, are effectively two operations. A delete operation on the active layer, and an add operation on the target layer. Thus undoing the move operation right away, will only undo the delete operation, and thus you end up with a duplicate. Similarly, undoing the operation only on the target layer will only undo the add operation and thus delete the object altogether. To properly undo operations that operate across layers, you need to undo one step in all affected layers. The editor doesn't do this automatically for you, since you might have done additional operations on those other layers already and thus it can't guarantee to do the right thing. Exporting Scenes with Layers When exporting a scene all data is exported into a single file. Currently there is no runtime concept of layers (although this may be added later, since this can be useful for streaming large worlds). However, only the objects from loaded layers are put into the exported scene file. Unloaded layers don't contribute to the result. This can be utilized to export only a part of a scene to speed up loading times during testing. Saving Scenes with Layers Since every layer is a separate file, each one also has its own modified flag (the star next to its name, indicating that it was changed). The regular Save Document (Ctrl+S) action only saves the active layer. You can use Save All (Ctrl+Shift+S) to save all documents. This will save all layers in a scene, but also all other documents. Object References across Layers Creating object references across scene layers is not possible (and also not planned to ever be allowed). Objects that should reference each other must either be in the same scene layer, or one of them has to be a prefab and expose the reference property in a useful way. Multi-User Editing with Layers Layers don't magically solve the problem of editing the same scene concurrently with multiple people. However, they give you a tool to do so, as long as everyone involved sticks to some rules. To enable multi-user editing, you need to decide how to best split up a scene, such that everyone can work on a mutually exclusive set of layers, meaning that no layer is ever modified by two people at the same time. One option is to have one layer per person, and everyone only modifies the things that they worked on previously. Another option is to divide a level into groups of object types, such as terrain, NPCs, vegetation and so on, and always have only one person work on each aspect. Finally, another option is to divide the scene into areas. This may be most useful for large scenes, where one person works for example on the village, and another person on the forest around it. Of course all of these methods can also be mixed and matched as makes most sense in each project and scene. Tip: To prevent accidentally editing a layer that shouldn't be touched, it is best to either unload it or at least set it to invisible. This way you can't accidentally select and change such an object. Important: The main layer stores references to all other layers. Thus adding a new layer will modify the main layer. In that case, it is best to quickly synchronize this change with all other team members. In case a layer WAS modified by two people at the same time, for example because both added a new layer, such changes are possible to resolve manually or even automatically by tools like git, since the layer files are text based and merge-friendly, as long as there are only few changes. Miscellaneous Tips Some objects are more important during editing than others. For example the objects that configure the overall level lighting, skybox, player start position, physics settings, cameras and so on. It is very useful to put all of these objects into a dedicated layer, because that makes finding them much easier. See Also Scene Editing Editor Documents"
  },
  "pages/docs/scenes/selection.html": {
    "href": "pages/docs/scenes/selection.html",
    "title": "Selecting Objects | ezEngine",
    "keywords": "Selecting Objects This page describes everything related to object selection. Common Pressing ESC will clear the selection. Viewport - Single Selection Left-click on an object to select it. Hold CTRL to add or remove objects from the selection. Hold CTRL and middle-click an object to open its material document. This does not work for prefab instances. Viewport - Marquee Selection Hold SPACE to enable marquee selection Then left-click and drag to add items to the selection Additionally hold CTRL before the left-click to instead remove items from the selection Press ESC to cancel the marquee selection Scene Tree You can filter the scene tree with the search box at the top: Selection Pivot If you select multiple objects, then the object that you add to your selection last determines the position of editing gizmos and thus the pivot point for some operations. For example, you can rotate a group of objects around a specific point, by having a (dummy) object at that point and adding it to your selection last. To change which object is your pivot, just hold CTRL and click an object to remove it from you selection, then click it again to re-add it to the selection. This way it will become the last object in the selection and therefore the new pivot. Selection Outline Selected objects are highlighted with a yellow outline, which is visible through walls. This outline can be toggled with the S key or the respective toolbar button. Shape Icons Some component types use a shape icon as their graphical representation. This makes it possible to select these types of objects in the viewport. Shape icons can be toggle with the I key. Selection Bounding Box When visualizers are enabled, the editor display a yellow bounding box around each selected object. Visualizers can be toggled with the V key. Select Transparent Press the U key to toggle whether transparent objects should be selectable. Disabling this can be very useful if you have large objects with special shaders, for instance to create atmospheric effects like volumetric lighting. Such shapes can prevent you from selecting anything else, even though they are mostly invisible. Scene Layers Scene layers are a convenient tool to organize a scene and make it easier to find important objects. When an object is selected through the viewport, the Layers panel automatically switches the active layer to the one that contains the picked object. See Also Editing Gizmos Greyboxing Scene Editing Scene Layers"
  },
  "pages/docs/sound/fmod-event-component.html": {
    "href": "pages/docs/sound/fmod-event-component.html",
    "title": "FMOD Event Component | ezEngine",
    "keywords": "FMOD Event Component The FMOD event component creates an instance of an FMOD event. An event is usually a 2D or 3D sound, but can also be an environmental effect that changes how other sounds are perceived. FMOD events are very powerful, which is why ezEngine doesn't need to have a large feature set of audio features. No matter what you want to do, pretty much anything is available through FMOD events. A description of FMOD events is out of scope for this documentation. Please see Using FMOD Studio for learning resources. FMOD event components reference sound event assets. The component plays the referenced sound. If the FMOD event has looping regions, the sound will play indefinitely, until it is stopped programmatically, or the component is deleted. There is no looping option on the component, since this feature controlled through FMOD Studio. Advanced FMOD features, like sound cues and adjusting event parameters are only accessible programmatically (C++ or TypeScript). For details please see the API Docs about ezFmodEventComponent. Sound Occlusion By default sounds are only attenuated by distance. FMOD doesn't have a representation of the 3D geometry and therefore can't muffle or disable sounds when they are (visually) occluded. The event component allows you to enable a simple physics raycast based heuristic to determine whether a sound source is occluded by a wall or larger obstacle. If enabled, the occlusion factor is computed and the FMOD event parameter Occlusion is passed into the event. It is your responsibility to set up the FMOD event such that this parameter exists and is used to adjust the event's volume. Component Properties Paused: If set, the referenced sound won't start playing at start. Toggling this value programmatically will pause/resume a playing sound. Volume: Adjusts the volume for this sound. Pitch: Higher pitch means the sound plays faster, a lower pitch makes it play slower (and at lower frequency). SoundEvent: The sound event asset that will be played by this component. UseOcclusion: If enabled, the component will use physics raycasts to determine whether the sound source is occluded by geometry. The occlusion factor is passed to the FMOD event as the event parameter Occlusion. OcclusionThreshold: How strongly the sound source must be occluded, before the occlusion value will be larger than zero. OcclusionCollisionLayer: The physics collision layer to use for the occlusion raycasts. OnFinishedAction: For sounds that end by themselves, this option allows you to specify whether the component should delete itself or its entire object afterwards. ShowDebugInfo: If enabled, the component displays some statistics about its state. See Also FMOD Integration"
  },
  "pages/docs/sound/fmod-listener-component.html": {
    "href": "pages/docs/sound/fmod-listener-component.html",
    "title": "FMOD Listener Component | ezEngine",
    "keywords": "FMOD Listener Component The FMOD listener component represents the position and direction from where the player perceives sound. Every game must have exactly one listener component in the scene, to instruct FMOD how to compute the spatial sound. For first-person and third-person games the listener component would be attached to the character controller, typically the same node where the main camera component is located. For other kinds of games positioning the listener component can be more tricky. For example in a top down strategy game, or a 2D side scroller, you may move the camera very far out (to achieve the desired perspective). Though, if you were to place the listener component at the same position, you would either not hear anything (too far away), or if you adjust sound distances such that you hear something, you may hear sounds that are very far off screen. In such situations it is better to move the listener component much closer to the action. The image below shows such a setup, where the listener is much closer to the action than the camera: The red cone represents what the camera sees. The green circle visualizes the area in which sounds are audible. Multiple Listeners If you build a multi-player game, you should be careful to only instantiate a single listener component for the local player, and don't accidentally duplicate the listeners from remote players. However, FMOD also supports multiple listeners locally, in case you want to do split screen multi-player on the same machine. This is automatically enabled if multiple listeners are in the scene, with non-zero ListenerIndex. In multi-listener mode FMOD will deactivate many spatial audio effects and try to render the audio in the most sensible way. For details refer to the FMOD documentation. Component Properties ListenerIndex: Which listener this component represents. Should always be zero, unless you do a split-screen multi-player game. See Also FMOD Integration"
  },
  "pages/docs/sound/fmod-overview.html": {
    "href": "pages/docs/sound/fmod-overview.html",
    "title": "FMOD Integration | ezEngine",
    "keywords": "FMOD Integration FMOD is a world class sound engine used by many AAA games. FMOD Studio is an incredibly powerful tool to manage your sounds. ezEngine integrates FMOD with a plugin. Important: Be aware that FMOD is a commercial product. Before you distribute a project made with EZ that includes FMOD functionality, check the FMOD licensing options. Video: How to set up Fmod for Sound Using a Different FMOD SDK To compile the FMOD plugin, parts of the official FMOD SDK are necessary. On Windows, EZ contains these files already. If you build EZ for a different platform, or want to use a newer FMOD version, follow these steps: Create a free account at fmod.com and sign in. Download and install the FMOD Studio API SDK for Windows (or whichever platform you need). Download and install FMOD Studio (only needed by people who want to edit FMOD projects). Enable FMOD in the CMake configuration. Clean the CMake cache and regenerate. This way CMake will pick up the installed FMOD SDK rather than the built-in one. Compile the engine. Using FMOD Studio FMOD Studio is the tool with which you edit the FMOD sound events. This tool is not provided with EZ and has to be installed separately: Create a free account at fmod.com and sign in. Download and install FMOD Studio. FMOD Studio has a vast number of features. Describing how it works is out of scope for this documentation. Instead have a look at these resources: FMOD Learning Resources FMOD Studio Documentation There is a set of tutorials about FMOD in Unreal, which is a very good introduction. There are also several videos about FMOD in Unity. Since most of the work is done in FMOD Studio anyway, most things that you see in those videos apply equally to EZ. FMOD Studio for UE4 Video 1 - Getting Started Audio for Unity 5: Viking Village (1/5) - Getting Started FMOD Project Settings For project wide FMOD settings, go to Project > Plugin Settings > FMOD Project Settings... Important: Although you can configure profiles for multiple platforms, at the moment only the Desktop profile will be used. The most important thing to configure here is to choose the Masterbank file. For what a master sound bank is, please refer to the FMOD documentation. If you haven't created any sound banks yet, you should start by creating an FMOD Studio project and come back when you have exported a master bank. The other options are best left at their default values. See the FMOD documentation for details. Once you have these things set up, you can create your first sound bank asset, through which you get FMOD sound data into the engine. Sample Data A sample FMOD Studio project is available under Data/Content/Sound, including pre-exported sound banks. These are also used by the sample projects, such as the Testing Chambers. Note: When opening the sample FMOD Studio projects from EZ, FMOD Studio may ask you to upgrade the project to the latest version. This should work fine. Scene Editing Settings The FMOD editor plugin adds UI elements to mute sound entirely and to adjust the master volume: See Also Sound www.fmod.com"
  },
  "pages/docs/sound/fmod-soundbank-asset.html": {
    "href": "pages/docs/sound/fmod-soundbank-asset.html",
    "title": "FMOD Sound Bank Asset | ezEngine",
    "keywords": "FMOD Sound Bank Asset The sound bank asset is used to import data from an FMOD sound bank into the engine, and expose FMOD events as sound event assets. FMOD Studio (the tool to edit FMOD projects) stores all sound data in so called sound banks. A sound bank contains all the necessary meta data about sounds (called 'events') as well as the sound data. The FMOD runtime loads these sound banks into memory and decodes sound data as needed. In an FMOD Studio project you can split up sound data into as many sound banks as you like, which allows you to organize sounds in such a way that not all sound banks need to be loaded at the same time. To make use of FMOD events inside an FMOD sound bank, you first have to create a sound bank asset in the ezEditor and reference an exported FMOD sound bank file. When you transform the sound bank asset it will extract the meta information about all the events, and create sound event assets, which appear in the asset browser. You can then add sound events to scenes via drag and drop of sound event assets from the asset browser. Asset Properties SoundBankFile: The relative path (from any data directory) to the exported FMOD sound bank file. See Also FMOD Integration FMOD Sound Event Asset FMOD Event Component"
  },
  "pages/docs/sound/fmod-soundevent-asset.html": {
    "href": "pages/docs/sound/fmod-soundevent-asset.html",
    "title": "FMOD Sound Event Asset | ezEngine",
    "keywords": "FMOD Sound Event Asset The sound event asset represents a single FMOD sound event. An event typically represents an actual sound, but may also be something like a reverb effect. Sound events are very versatile, for details refer to the FMOD documentation. You don't create sound event assets. Instead, when you transform a sound bank asset, for every event in the FMOD sound bank, one sound event asset appears in the asset browser. Sound event assets are virtual assets, they don't have a representation on disk. They mainly exist as a UI element, such that you can browse for and select them in the sound event component. You can also instantiate sound events by dragging a sound event asset from the asset browser into a scene. This will automatically create a game object and attach an FMOD event component. See Also FMOD Integration FMOD Sound Bank Asset Assets"
  },
  "pages/docs/sound/sound-overview.html": {
    "href": "pages/docs/sound/sound-overview.html",
    "title": "Sound | ezEngine",
    "keywords": "Sound The engine doesn't have sound support directly built-in, instead all sound functionality is provided through plugins. FMOD Plugin ezEngine comes with support for FMOD. For details, please checkout the page about FMOD. Sound Interface The class ezSoundInterface is an abstract interface that allows you to set some very basic options, like the volume, or the current listener position. These features are mainly needed by the editor such that it can mute sounds and have the editor camera be the listener position, independent of which sound plugin ends up being used. A game can use this interface to adjust the volume of the entire game or specific channels. For more specific actions you need to query the interface of the actual implementation. See Also FMOD Integration"
  },
  "pages/docs/terrain/heightfield-component.html": {
    "href": "pages/docs/terrain/heightfield-component.html",
    "title": "Heightfield Component | ezEngine",
    "keywords": "Heightfield Component The HeightfieldComponent can be used for simple, heightmap based terrain, as well as for adding small details like piles of rubble to a scene. The heightfield component uses an ImageData asset to get the height information. Consequently, the heightmap image has to be imported as such an asset type. Collision Mesh When the tag AutoColMesh is set on the owner game object, the component adds its mesh to the scene collision mesh, just like the greyboxing components. This is only supported for static game objects though. Currently all triangles in the collision mesh will all use the same surface, the one set on the main material. Component Properties HeightfieldImage: The ImageData asset that contains the terrain height information. Material: The single material used for the entire mesh. If you need multiple layers (grass, dirt, rock, etc) you can write a custom visual shader (TODO) that uses a mask texture to fetch and blend the various layers as desired. The vegetation scene in the Testing Chambers project does this. HalfExtents: The size of the terrain in X and Y direction. Height: How much to stretch the terrain along the up axis. Tesselation: How densely to tesselate the graphics mesh. TexCoordOffset, TexCoordScale: An offset and scale to shift and stretch the texture coordinates. Use a scale of one, if the material should be stretched once across the entire terrain. Use a large scale value instead, if the material should be repeated many times. ColMeshTesselation: How densely to tesselate the physics mesh. It often makes sense to only use the half resolution of the graphics mesh. Performance Considerations Consider to deactivate casting shadows by removing the CastShadow tag on the owner game object. It is rare that terrain has such prominent features, that they would be cast visible shadows. However, densely tesselated terrain meshes add a high cost to the shadow map update. Deactivate the collision mesh generation, if the terrain (or pile of rubble) is purely visual. See Also Terrain and Vegetation"
  },
  "pages/docs/terrain/kraut-overview.html": {
    "href": "pages/docs/terrain/kraut-overview.html",
    "title": "Kraut | ezEngine",
    "keywords": "Kraut Kraut is a tool for procedurally generating trees and other plants. Tree files are authored in the stand-alone Kraut editor, which you can get here. The Kraut asset references the .tree file that the Kraut editor writes. You then have to specify the materials that shall be used for each tree part. Note that there are three Kraut specific base materials (under Plugins/Kraut) which have to be used as the base materials. Kraut uses a custom shader, and these base materials pull in the correct shader and configure it to be used for stems (trunk, branches, twigs), fronds (static leaves) or leaves (billboards). Once a Kraut tree config is imported and set up, dragging the asset into a scene, will instantiate a Kraut tree component. See Also Heightfield Component"
  },
  "pages/docs/terrain/procedural/procedural-object-placement.html": {
    "href": "pages/docs/terrain/procedural/procedural-object-placement.html",
    "title": "Procedural Object Placement | ezEngine",
    "keywords": "Procedural Object Placement Creating large terrain with convincing vegetation is a lot of work. Since nature generally follows certain rules, such as on which type of terrain which plant can grow, how densely they are packed and so on, it makes sense to rather build and apply such rules to automatically place objects, than attempting to place vegetation by hand. EZ comes with a procedural generation feature. This is designed specifically for decorating terrain with vegetation, but can also be used in other scenarios. The system is heavily inspired by the procedural vegetation system in Horizon Zero Dawn. See this GDC talk (video, slides) for reference. Sample Scene To get started, the Vegetation scene in the Testing Chambers project shows a simple setup in action. You need to run the scene, because the procedural system is only active during simulation. Technical Overview The procedural generation system is active in a scene once a procedural placement component is added to it. The component defines in which area a certain rule is used to place objects. The rules are set up through ProcGen graph assets. The rules specify which objects to place under which conditions and with what kind of variation. Additional components can be placed to affect the object placement in select areas, such as for clearing an area or increasing the density of a certain type of plant. The procedural placement system only places objects during scene simulation, so without pressing play in the editor, you won't see any placement. The system uses the position of the main camera to determine where to place objects. Object placement is distributed across frames, to prevent stutter. A grid around the camera is used to determine in which area objects have been placed already. When the camera moves, new cells will be populated, and cells that are too far away are cleared again. This way the system makes sure that there is a relatively constant performance impact. Consequently, procedurally placed objects can't have state that needs to be persistent. Each object can have state, for example you could make it place trees that can be burned down, but it must be fine for your game, that the tree will reappear, if the player gets far away and returns. Object placement is deterministic. As long as the placement rules and the conditions (terrain, materials, etc) don't change, the same object will be placed at the same position every single time. See Also ProcGen Graph Asset Procedural Placement Component"
  },
  "pages/docs/terrain/procedural/procgen-graph-asset.html": {
    "href": "pages/docs/terrain/procedural/procgen-graph-asset.html",
    "title": "ProcGen Graph Asset | ezEngine",
    "keywords": "ProcGen Graph Asset The ProcGen Graph Asset is used to configure the rules for procedural object placement. In this graph structure you define which objects should be placed under which conditions. Editing the ProcGen Graph The main area of this document is used to place and connect nodes in a graph. Use the context menu to add nodes. Drag and drop pins from left to right to connect outputs to inputs. Pins usually represent single number values, such as a density value or a single channel of a color (e.g. 'red'). By connecting a pin on the right side of a node (output) to a pin on the left side of a node (input), the output value is forwarded into the other node and affects how that node operates. Nodes that only have pins on their right side, are pure input nodes, they only provide data for other nodes to consume. Nodes that only have pins on their left side, are output nodes. They consume various input values and then create some kind of result, for example they decide whether to place an object at a certain location. When you select a node, the property grid shows additional configuration options. The image below shows a graph with three input nodes and three output nodes. Each input node is connected to one output node and thus affects how that output node places objects. Interactions Right-click and drag in the view, to move it around. Mouse-wheel to zoom. Left-click to select nodes. Left-click and drag to select multiple nodes. Ctrl + left-click to add or remove a node from the selection. Right-click on a node, pin or connection for a context-menu in which you can delete the object. Click on an empty spot to open a context menu from which to create new nodes. Left-click and drag any pin to connect it to another pin. The UI will indicate which pins can be connected. Graph Output What exact output the procedural rules generate depends on which output nodes are present in the graph. Currently these types of output are available: ProcGen Graph Placement Output ProcGen Graph Vertex Color Output (TODO) Tip: To learn the system, it is best to start with the placement output node and ignore the rest. The simplest possible graph only contains a single such node and nothing else. Using a ProcGen Graph The ProcGen graph asset doesn't have any kind of pre-visualization. To see what effect a rule has, you need a scene in which the necessary setup is available. The scene should contain some geometry with collision meshes, such that raycasts can hit the geometry. Greyboxing and heightfield components work just fine for that. You also need a component that applies the ProcGen graph. Depending on the output nodes used in the graph this would be one of these: Procedural Placement Component Procedural Vertex Color Component (TODO) Make sure the ProcGen graph asset is transformed. Then press play to simulate it. If everything is set up right, you should see objects getting placed around the camera, within the specified volume. Live Editing When you edit a ProcGen graph asset, most changes trigger a live update in any running scene. That means you can switch back and forth between the asset and a test scene, and see changes update right away. However, this is limited to certain types of changes. Changes to referenced assets (such as color gradients or prefabs) won't update the already placed objects. If such a change was done, you need to stop simulating a scene, and run it again. Debug Mode It can be difficult to get an idea for the values that a rule graph produces. To visualize the values, you can right-click any pin and enable the Debug flag. This disables all placement output and instead switches to a mode where for every location only a sphere is rendered, and the shade of the sphere represents the value of the pin on which the debug flag is enabled. Black for 0 and white for 1. When you then switch to a scene and run the simulation, you will see this pattern: Building Complex Rules Once you've figured out the basics, you can build more complex rules. Use the ProcGen graph input nodes (TODO) to receive more information about a location, such as its slope or height. Pass the data through ProcGen graph math nodes to adjust it as necessary. Use ProcGen graph modifier nodes to make it possible to configure the rules locally. See Also Procedural Placement Component ProcGen Graph Placement Output Procedural Vertex Color Component (TODO) ProcGen Graph Vertex Color Output (TODO) Procedural Volume Box Component Procedural Volume Image Component"
  },
  "pages/docs/terrain/procedural/procgen-graph-inputs.html": {
    "href": "pages/docs/terrain/procedural/procgen-graph-inputs.html",
    "title": "ProcGen Graph Input Nodes | ezEngine",
    "keywords": "ProcGen Graph Input Nodes These node types are available as inputs for ProcGen graph assets. Slope Input Node When an object gets placed at a specific location, the Slope node calculates the slope of the terrain at that position and determines whether it is within a desired range. The better it is within the range, the closer the output value is to 1, and if the slope is outside the desired range, the output value is 0. The output value of this node can be passed unchanged as Density into the placement output node. In this case the slope directly decides whether an object gets placed or not. It may, however, also be passed into other values, for instance to affect the color of an object. On the node you select a MinSlope and a MaxSlope which define the desired range. For example, if the MinSlope is set to 0 (flat ground) and the MaxSlope is set to 20 (slightly uphill), then objects will only be placed on nearly flat terrain. If, however, MinSlope is set to 30 (steep) and MaxSlope is set to 70 (nearly vertical), then objects will only be placed along strong slopes, for example the sides of mountains. The LowerFade and UpperFade values determine how quickly the output value fades towards zero when the slope approaches MinSlope or MaxSlope respectively. With a fade value of zero, the cut off is very abrupt, with a fade value of one, the output value declines earlier, but also more gradually. Node Properties MinSlope, MaxSlope: The slope range (in degree) between which the output value is non-zero. LowerFade, UpperFade: How quickly to fade the output value from one towards zero, when the slope approaches the min (lower) or max (upper) value. Fade values of zero mean an abrupt change from one to zero at the boundaries, a value of one means there is always some fade out, except right at the center of the value range. Height Input Node The Height node works mostly the same way as the Slope node, except that it uses the height (z value) of the potential object position. The Height node determines the z value of the location where an object shall be placed. It then checks whether the value is between MinHeight and MaxHeight. If not, it outputs the value 0. Otherwise, it outputs a non-zero value. LowerFade and UpperFade are used to decide whether, and how much, to fade the output value from 1 towards 0. This node can be used to place objects only at specific altitudes, or to change object sizes or colors at higher elevations. Node Properties MinHeight, MaxHeight: The height range between which the output value is non-zero. LowerFade, UpperFade: How quickly to fade the output value from one towards zero, when the height approaches the min (lower) or max (upper) value. Fade values of zero mean an abrupt change from one to zero at the boundaries, a value of one means there is always some fade out, except right at the center of the value range. Mesh Vertex Color Input Node TODO See Also Procedural Object Placement ProcGen Graph Math Nodes"
  },
  "pages/docs/terrain/procedural/procgen-graph-math.html": {
    "href": "pages/docs/terrain/procedural/procgen-graph-math.html",
    "title": "ProcGen Graph Math Nodes | ezEngine",
    "keywords": "ProcGen Graph Math Nodes These node types implement math functions for ProcGen graph assets. Blend Node The Blend node provides the most common math expressions to combine two values. Node Properties Mode: Determines how the two input values get combined. InputA, InputB: Fallback values for pins A and B respectively, in case one of the pins isn't connected. Use this, in case you want to combine one value with a constant (e.g. to multiply A by two, leave B disconnected and set InputB to 2). ClampOutput: If this is enabled, the output value is clamped to [0;1] range. Perlin Noise Node The Perlin Noise node outputs a Perlin Noise value for the current location. This value can be used to add variety, however, not through completely random values, but rather ones that gradually change. So for instance, if the quality of soil varies, the look of vegetation may be different. However, bushes that grow next to each other are affected the same way, so their look will be similar. In the two images below, the same plants are placed. However, in the second image, Perlin noise is used to affect their color. Note how in the second image all plants in an area become darker or brighter. Node Properties Scale: Over which area the noise is stretched. Large values mean that the noise value changes slowly over large distances, whereas smaller values result in higher frequency noise. If the value is too small for the used object density, the results lose their gradually changing quality. Offset: Pushes the values along the respective axis. NumOctaves: How many Perlin noise values to combine for the final result. More octaves give a more varied pattern and can be scaled across a larger area without showing obvious repetitions, but also cost more performance to evaluate. OutputMin, OutputMax: The output value will be between these two values. Random Node The Random node outputs a random number for the current location. The result is deterministic for a given position and seed value, however, contrary to the Perlin noise node, the values for locations right next to each other have no correlation. This type of noise can be used to add variation between plants, where there is no shared reason for the variation (such as soil quality). Note: Properties such as object scale and color variation CAN be controlled through input pins and thus random values can be passed in, however, if no such input is provided, they will already vary randomly between their allowed values. Unless more control is needed, it is therefore not necessary to add and connect a Random node. Node Properties Seed: A seed value for the random number generator. If a fixed seed is chosen, the random number output is always exactly the same. OutputMin, OutputMax: The output value will be between these two values. See Also Procedural Object Placement ProcGen Graph Input Nodes (TODO)"
  },
  "pages/docs/terrain/procedural/procgen-graph-modifiers.html": {
    "href": "pages/docs/terrain/procedural/procgen-graph-modifiers.html",
    "title": "ProcGen Graph Modifier Nodes | ezEngine",
    "keywords": "ProcGen Graph Modifier Nodes The output that the rules in a ProcGen graph produce, mostly depends on the terrain on which it is applied. Flat plains, steep cliffs and different surface types determine where which kind of vegetation grows. However, often it is necessary to have some more control. By itself, the rules rarely create a clearing in a forest, and even if they do, it is hard to control where it is and how it looks like. The procedural nature of the system takes away control from the level designers. To give this control back, without reintroducing the need for lots of manual work, the system allows you to place modifiers in the world, which affect the rules as you like. The modifiers typically are simple volumes such as spheres and boxes. They are tagged to differentiate what they represent. The rule graph can evaluate whether a location is influenced by certain volumes and change the output accordingly. What the system does with this additional information is up to the person who sets up the rules. You can create volumes that modify the density of specific plants down to zero, meaning that it suppresses their placement. Or a volume may affect the color, size or other parameter. Example The following graph has a single placement output node, which places a single object type. However, both the Density and the ColorIndex inputs are connected to a modifier node. The default value for each input is configured on those nodes. In this case, the default density is 0, meaning that without an extra modifier volume in the scene, this type of plant won't be placed, at all. The placement node also has a color gradient from which the color of each plant is selected. The default value makes the plant green, but by providing a different ColorIndex, it can be shifted towards red/brown. The image below shows how we can use this. There are two sphere volumes in our scene. One is tagged to be picked up by the node that feeds into the Density input, the other is tagged to be picked up by the ColorIndex modifier node. The sphere on the left overrides the Density to be 1 and thus inside its area of influence, vegetation appears. The sphere on the right overrides the ColorIndex towards red/brown, thus plants inside its volume have a different color. Note that the rules in the graph are not tied to specific volumes in a scene. Rather the modifier nodes use tags to filter which volumes are considered as their inputs. Thus you can use as many volumes as you like to locally control the rule output. In the image below, the same rules are applied, but now there are three spheres in the scene that override Density and another one for color: ApplyVolumes Node The ApplyVolumes node looks up the scene for ProcGen volume components at each location where a plant shall be placed. It uses tags to filter volumes. Thus you need to set up different tags to have volumes affect the placement rules in different ways. The node then takes its own value (either provided through the In pin or the InputValue property) and modifies it with each value provided by local volumes, according to their BlendMode setting. If multiple volumes are found for the same location, their SortOrder and their overall size are used to decide in which order their values are applied. This way a smaller volume typically takes precedence over larger volumes, but using the SortOrder you can force a desired priority. If a volume additionally uses an image (such as the volume image component) to provide detailed data, the ImageVolumeMode is used to determine either which channel (red, green, blue or alpha) should be considered, or whether a specific color represents whether the volume should be applied. Choosing a channel means that you can only control four different things, but you have smooth values ([0; 1] range) to work with. So a channel could control how strongly to tint a color or how think a density should be. When using a reference color instead, you can have many more inputs, but each one can only be on or off and they can't overlap. This is useful to tag areas of a type. So for instance brown could represent \"swamps\", green \"forests\", blue \"water\", grey \"roads\" and so on. Such general information about the area type can then be used to decide which types of plants to place. Node Properties IncludeTags: These tags control which volumes are considered. If your volumes seem to have no effect, make sure this tag is set correctly both on the node and on the game object to which the volume component is attached. Set up different tags to differentiate what volume shall affect what parameter. InputValue: If no value is provided through the In input pin, this is the default value to use. The final output Value is determined by taking this value and combining it with all the values of the volumes, using their individual BlendMode. The SortOrder on the volumes controls in which order the values are combined. If the sort order values are equal, larger volumes are applied first and then smaller volumes. This way a more local volume have 'the last word'. ImageVolumeMode: If a location is modified by an image volume, this mode specifies how the image data is used. ReferenceColor: The color in the image is compared with RefColor. If the colors roughly match, the volume takes effect, otherwise it is as if the volume wasn't present, at all. Red/Green/Blue/Alpha Channel: The volume always has an effect, but its Value is additionally multiplied with the value from the chosen channel of the image. RefColor: If ImageVolumeMode is set to ReferenceColor, this volume only has an effect, if the image at the sampled location (roughly) matches this color. See Also Procedural Object Placement Procedural Volume Box Component Procedural Volume Sphere Component Procedural Volume Image Component"
  },
  "pages/docs/terrain/procedural/procgen-graph-output-placement.html": {
    "href": "pages/docs/terrain/procedural/procgen-graph-output-placement.html",
    "title": "ProcGen Graph Placement Output | ezEngine",
    "keywords": "ProcGen Graph Placement Output The Placement Output node is at the heart of the procedural placement system. This node specifies which prefabs to spawn, what density to use, how to position each object and how to add variety. A ProcGen graph can already work, even if it contains just a single of these nodes. However, you can add as many of these nodes as you like. Every placement node represents a different type of object, with different conditions under which they are spawned. For example one node would be used to spawn stones on rocky surfaces, another node spawns bushes on grassy surfaces, a third spawns trees and so on. Every type of object has a footprint, meaning an certain size of area in which only one of them will be placed. The larger the footprint, the more sparse these objects are spawned. The node itself can already filter whether to place an object, at all, by inspecting the surface. This way vegetation isn't planted on concrete or water. More elaborate filtering can be achieved with modifier nodes. For example an image can represent where exactly what type of object should appear. Node Properties Name: A custom display name that is shown in the node's title bar. This has no function other than to make it easier to find in the graph. Objects: A list of prefabs. When an object gets placed, which one to use is either chosen through the ObjectIndex pin, or randomly, if the pin is not connected. Footprint: The radius (in meters) of the circle in which a single object gets placed. Increase the footprint to make object placement more sparse, decrease it to make it more dense. MinOffset, MaxOffset: How much the position of the placed object may randomly deviate from the center position. If this is large enough and the footprint is too small, neighboring objects may end up overlapping. It is common to set MinOffset to -MaxOffset. If an object may only move upwards, but not downwards, keep MinOffset.z at 0 and only set MaxOffset.z to a positive value. YawRotationSnap: All placed objects are rotated randomly along the up axis. If this is non-zero, the used rotation angles are always a multiple of it. AlignToNormal: How much to blend the placed object up direction towards the underlying surface's slope. If this is zero, the placed object always stands upright, regardless of the slope. If it is set to one, the placed object will fully follow the slope of the terrain. Anything in between, the object bends more or less towards the slope. MinScale, MaxScale: How large the placed object is at least (MinScale) and at most (MaxScale). The final scale is a blend between min and max, so if both values are uniform (x = y = z), the object's scale is also always uniform. ColorGradient: An optional color gradient from which a random color is chosen to set the mesh color (in fact a ezMsgSetColor is sent when the object is spawned, so this could also be used to implement other features). If the ColorIndex pin is connected, the incoming value in the [0;1] range is used instead to lookup the color from the gradient. CullDistance: At what distance to start spawning this type of object. For large objects with a low density, this distance can be increased. Small objects with high density should use a small distance, to prevent performance issues. PlacementMode: How to determine the location where to place objects. Raycast: In this mode a physics ray is cast downwards from the volume of the placement component. Using the CollisionLayer and Surface as filters, the closest intersection point is used. Fixed: In this mode objects are always placed at the height of the placement component. No ray is cast, and no location is filtered out. This can be used for 2D games where no collision geometry exists. Custom filtering can still be achieved through image volumes and other modifiers. CollisionLayer: The collision layer to use when PlacementMode is set to Raycast. The collision layer decides which physical objects will be hit by the raycast and thus on which surfaces objects may get spawned at all. Note that in Raycast mode objects can only be placed, if a collision meshes exist in the scene at all (greyboxes and heightfields set these up automatically). Surface: An optional surface that's used to filter object placement. Objects will only be placed on surfaces of this type (or derived). This is used to only plant certain vegetation on each type of ground. Filtering by surface can also be very useful to prevent procedural objects from getting spawned on top or inside of other procedural objects. The left image shows gras being placed on top of the procedurally placed rocks, in the right image a surface filter prevents this from happening: Input Pins Density: A value in [0;1] range that determines how likely it is that an object gets spawned. A lower density means that fewer objects get spawned. If this pin is not connected, a default density of 1 is assumed. Connect this pin to a Perlin noise node or an modifier node to vary density by location. The image below shows varying density using Perlin noise. Scale: A value in [0;1] range to blend between MinScale and MaxScale. This can be used to scale objects up or down by location or other environmental influences. If this pin is not connected, every object uses a random scale factor. The image below shows varying scale using Perlin noise. ColorIndex: If a ColorGradient has been set, connecting this pin allows you to decide which color to use for an object. The value is in [0;1] range, with 0 selecting the left-most color from the gradient and 1 the right-most color. The image below shows varying color tint using Perlin noise. ObjectIndex: If more than one prefab is added to the object list, this pin can be used to select a specific one. The value is in normalized [0;1] range, so if two objects are in the list, the first one is selected by any value between 0 and 0.5 and the second by values between 0.5 and 1. The image below shows how varying prefabs are picked by Perlin noise. See Also Procedural Object Placement Procedural Placement Component ProcGen Graph Asset"
  },
  "pages/docs/terrain/procedural/procgen-graph-output-vertexcolor.html": {
    "href": "pages/docs/terrain/procedural/procgen-graph-output-vertexcolor.html",
    "title": "ProcGen Graph Vertex Color Output | ezEngine",
    "keywords": "ProcGen Graph Vertex Color Output Node Properties Name: See Also Procedural Object Placement ProcGen Graph Vertex Color Output (TODO) ProcGen Graph Asset"
  },
  "pages/docs/terrain/procedural/procgen-placement-component.html": {
    "href": "pages/docs/terrain/procedural/procgen-placement-component.html",
    "title": "Procedural Placement Component | ezEngine",
    "keywords": "Procedural Placement Component The procedural placement component is used to apply the rules that are defined by a ProcGen graph asset within a defined volume of space. The most common use case is to procedurally place vegetation. The main purpose of the component is to define the region where the procedural rules are to be evaluated. It is not intended to move the volume around at runtime. Rather, the system already automatically takes care of only placing objects in the vicinity of the camera, and delete objects that have are too far away. Component Properties Resource: The ProcGen graph asset to use for deciding where to place which type of object. Extents: The size of the volume in which to evaluate the placement rules. This has to overlap with the terrain geometry where objects shall be placed. See Also Procedural Object Placement ProcGen Graph Placement Output"
  },
  "pages/docs/terrain/procedural/procgen-vertex-color-component.html": {
    "href": "pages/docs/terrain/procedural/procgen-vertex-color-component.html",
    "title": "Procedural Vertex Color Component | ezEngine",
    "keywords": "Procedural Vertex Color Component Component Properties Mesh: Color: Materials: Resource: Output Descriptions: See Also Procedural Object Placement ProcGen Graph Vertex Color Output (TODO)"
  },
  "pages/docs/terrain/procedural/procgen-volume-box-component.html": {
    "href": "pages/docs/terrain/procedural/procgen-volume-box-component.html",
    "title": "Procedural Volume Box Component | ezEngine",
    "keywords": "Procedural Volume Box Component The procedural volume box component defines a box shaped volume in which the rules of ProcGen graphs are modified. Not every graph has to make use of this information, and what the exact effect is, is up to the ProcGen graph. For more details see the chapter on ProcGen graph modifier nodes. The image below shows a box volume used to locally increase the scale of an object type. A fade out value of 0.5 makes the transition soft. Component Properties Value: A single number value. This is combined with the InputValue from the modifier node in the graph, using the BlendMode formula. SortOrder: If multiple modifier volumes overlap, the SortOrder can be used to control in which order the volumes are evaluated. BlendMode: How to combine Value with the InputValue from the modifier node in the graph. The Set mode just sets the result to Value and ignores the other operand. Extents: The size of the box volume in which the modifier is active. FadeOutStart: The influence of the volume can fade out towards its edges, for smooth transitions. This value controls for every axis, at what distance from the center point the fade out starts. So if this is set to zero, the fade out starts immediately at the middle (towards the closest edge), whereas if it is set to one, there will be no fade out, at all, and rather the influence of the volume stops abruptly at its border. See Also Procedural Object Placement ProcGen Graph Modifier Nodes"
  },
  "pages/docs/terrain/procedural/procgen-volume-image-component.html": {
    "href": "pages/docs/terrain/procedural/procgen-volume-image-component.html",
    "title": "Procedural Volume Image Component | ezEngine",
    "keywords": "Procedural Volume Image Component The procedural volume image component defines a box shaped volume in which the rules of ProcGen graphs are modified. Not every graph has to make use of this information, and what the exact effect is, is up to the ProcGen graph. For more details see the chapter on ProcGen graph modifier nodes. In contrast to the procedural volume box component, this component additionally samples an ImageData asset, such that it can provide fine grained information for every location within the box. This can be utilized to, for example, use a mask of the terrain that defines where roads are, where which type of plant shall grow and so on, to describe precisely how the procedural placement shall look like. The image below shows an image volume used to only place the vegetation on the darker terrain patches. Component Properties Value: A single number value. This is combined with the InputValue from the modifier node in the graph, using the BlendMode formula. SortOrder: If multiple modifier volumes overlap, the SortOrder can be used to control in which order the volumes are evaluated. BlendMode: How to combine Value with the InputValue from the modifier node in the graph. The Set mode just sets the result to Value and ignores the other operand. Extents: The size of the box volume in which the modifier is active. FadeOutStart: The influence of the volume can fade out towards its edges, for smooth transitions. This value controls for every axis, at what distance from the center point the fade out starts. So if this is set to zero, the fade out starts immediately at the middle (towards the closest edge), whereas if it is set to one, there will be no fade out, at all, and rather the influence of the volume stops abruptly at its border. Image: The ImageData asset that is used as the reference data. See Also Procedural Object Placement ProcGen Graph Modifier Nodes"
  },
  "pages/docs/terrain/procedural/procgen-volume-sphere-component.html": {
    "href": "pages/docs/terrain/procedural/procgen-volume-sphere-component.html",
    "title": "Procedural Volume Sphere Component | ezEngine",
    "keywords": "Procedural Volume Sphere Component The procedural volume sphere component defines a sphere shaped volume in which the rules of ProcGen graphs are modified. Not every graph has to make use of this information, and what the exact effect is, is up to the ProcGen graph. For more details see the chapter on ProcGen graph modifier nodes. The image below shows a sphere volume used to set the density of an object type locally to zero. A fade out value of 0.5 makes the transition soft. Component Properties Value: A single number value. This is combined with the InputValue from the modifier node in the graph, using the BlendMode formula. SortOrder: If multiple modifier volumes overlap, the SortOrder can be used to control in which order the volumes are evaluated. BlendMode: How to combine Value with the InputValue from the modifier node in the graph. The Set mode just sets the result to Value and ignores the other operand. Radius: The radius of the sphere volume in which the modifier is active. FadeOutStart: The influence of the volume can fade out towards its edges, for smooth transitions. This value controls at what distance from the center point the fade out starts. So if this is set to zero, the fade out starts immediately at the middle (towards the closest edge), whereas if it is set to one, there will be no fade out, at all, and rather the influence of the volume stops abruptly at its border. See Also Procedural Object Placement ProcGen Graph Modifier Nodes"
  },
  "pages/docs/terrain/terrain-overview.html": {
    "href": "pages/docs/terrain/terrain-overview.html",
    "title": "Terrain and Vegetation | ezEngine",
    "keywords": "Terrain and Vegetation Terrain Currently there are no tools for creating terrain. Terrain meshes are best build in external tools and imported as regular static meshes. A popular method to represent terrain are heightmaps - 2D grayscale images, where the brightness of each pixel represents the height of the terrain at that location. ezEngine provides a heightfield component which enables you to get such terrain data into the engine easily. However, this is only meant for simple use cases. Vegetation Vegetation can be created with standard meshes. Using custom visual shaders (TODO), a basic per-vertex wind animation can be applied. Additionally, ezEngine has built in support for Kraut, a tool that allows you to procedurally generate tree meshes. Finally, there is a system to procedurally place objects, typically plants, around the current player position. This system is currently undocumented, but the Testing Chambers project contains scenes which show basic usage. See Also Heightfield Component"
  },
  "pages/docs/tools/archivetool.html": {
    "href": "pages/docs/tools/archivetool.html",
    "title": "ArchiveTool | ezEngine",
    "keywords": "ArchiveTool The ArchiveTool is used to create or extract .ezArchive files. Archives are similar to zip files, they contain all the files in a folder, using compression. ezArchive files can be mounted at runtime as data directories. ezArchive Format The internal structure of ezArchives is optimized to make mounting as a data directory extremely efficient. The files are memory mapped and file lookups are faster than for regular folders. Each file in the archive may use compression or not, depending on whether it would make sense for the particular file. Different compression algorithms are possible, though the main compression used is zstd which yields good compression and is extremely fast to decode. Usage The ArchiveTool is a command line tool. Default Usage The most convenient way to use it, is to pass a single path as the only argument: ArchiveTool.exe C:/my/data When the path points to a folder, it will compress the folder and store the ezArchive file next to it. In the example above: C:/my/data.ezArchive ArchiveTool.exe C:/your/data.ezArchive When the path points to an existing archive, the tool will extract the data to a folder next to the file. In the example above: C:/your/data All Options The following options allow you to be more specific: -pack \"path/to/folder\" \"path/to/another/folder\" ... -unpack \"path/to/file.ezArchive\" \"another/file.ezArchive\" -out \"path/to/file/or/folder\" -pack and -unpack can take multiple inputs to either aggregate multiple folders into one archive (pack) or to unpack multiple archives at the same time. -out specifies the target to pack or unpack things to. For packing mode it has to be a file. The file will be overwritten, if it already exists. For unpacking the target should be a folder (may or may not exist) into which the archives get extracted. If no -out is specified, it is determined to be where the input file is located. If neither -pack nor -unpack is specified, the mode is detected automatically from the list of inputs: If all inputs are folders, mode is going to be 'pack'. If all inputs are files, mode is going to be 'unpack'. Examples Pack all data in \"C:\\Stuff\" into \"C:\\Stuff.ezArchive\": ArchiveTool.exe \"C:\\Stuff\" Pack all data in \"C:\\Stuff\" into \"C:\\MyStuff.ezArchive\": ArchiveTool.exe \"C:\\Stuff\" -out \"C:\\MyStuff.ezArchive\" Unpack all data from the archive into \"C:\\Stuff\" ArchiveTool.exe \"C:\\Stuff.ezArchive\" Unpack all data from the archive into \"C:\\MyStuff\" ArchiveTool.exe \"C:\\Stuff.ezArchive\" -out \"C:\\MyStuff\" See Also Data Directories FileSystem"
  },
  "pages/docs/tools/fileserve.html": {
    "href": "pages/docs/tools/fileserve.html",
    "title": "FileServe | ezEngine",
    "keywords": "FileServe This is the GUI front-end and the server of the file-serving functionality. It is used to stream project data over a network to a connected (mobile) device. This feature is fully functional, but currently undocumented. See Also FileSystem"
  },
  "pages/docs/tools/headercheck.html": {
    "href": "pages/docs/tools/headercheck.html",
    "title": "HeaderCheck Tool | ezEngine",
    "keywords": "HeaderCheck Tool Types of Header Files The code in ezEngine differentiates between two types of header files: Public Header Files: Public header files are header files that can be included by third party. These header files should not leak any implementation details like platform headers. A third party is any library or executable outside of the currently compiled library / executable. For example when ezFoundation is compiled, everything else is considered a third party. Internal Header Files: Internal header files may include platform headers and leak implementation detail, but can only be used within a subcomponent of ezEngine (for example only inside ezFoundation). Using them from outside of the component will cause a compiler error. To mark up a header file as a internal header file, first include the component's internal.h file and then use the component specific macro. The component's internal header file is called ComponentInternal.h and the macro is called EZ_COMPONENT_INTERNAL_HEADER. The following example shows how to mark a header file as internal for ezFoundation: #include <Foundation/FoundationInternal.h> EZ_FOUNDATION_INTERNAL_HEADER The Header Checker Tool The header checker tool will automatically be run by the continues integration to check for leakage of implementation detail. If a leak is found the build will fail. Usually you will see an error message such as: Including 'wrl/wrappers/corewrappers.h' in ezEngine/Code/Engine/Foundation/Strings/StringConversion.h:9 leaks underlying implementation detail. Including system or thirdparty headers in public EZ header files is not allowed. Please use an interface, factory or pimpl to hide the implementation and avoid the include. In this example including wrl/wrappers/corewrappers.h is illegal. This header file is included from ezEngine/Code/Engine/Foundation/Strings/StringConversion.h at line 9. To fix these issues follow one of the techniques below to hide implementation details. Hiding Implementation Detail To consider the different options of hiding implementation detail have a look at the following example #include <d3d11.h> class ezTexture2D { public: void Bind(); private: ID3D11Texture2D* m_ptr; }; If a user includes this header file, the underlying implementation detail is leaked as the user will need the d3d11.h header in order to compile the code. Furthermore the user might need exactly the same version of the d3d11.h file in order for the code to compile. This is a leaky abstraction. Ideally classes that wrap functionality should not leak any of their implementation details to the user. The following techniques can be used to hide implementation detail. Forward Declarations Forward declarations can be used to remove the need to include a header file, therefor removing the leaky abstraction. Consider the following fixed version of the ezTexture2D class: struct ID3D11Texture2D; // Forward declare ID3D11Texture2D class ezTexture2D { public: void Bind(); private: ID3D11Texture2D* m_ptr; }; This header is no longer a leaky abstraction as the user is no longer required to have a copy of d3d11.h. Forward declarations can be made for: Class or struct members if they are pointers or references. All types used as arguments to functions. Template arguments if the usage follows the two above rules. Forward declarations can't be made for: Class or struct members that are 'inline' because the compiler needs to know the size and alignment. Base classes. Enums can be forward declared if they are given an explicit storage type. So ideally to make enums forward declarable always manually specify a storage type. enum MyEnum : int; // Forward declaration enum MyEnum : int // declaration { One, Two }; Nested types can never be forward declared. A nested type is a type that is inside a class or struct. // does not work // struct Outer::Inner; struct Outer { struct Inner { int i; }; }; So prefer to put nested types into namespaces instead of structs or classes: // Forward declaration namespace Outer { struct Inner; } // Declaration namespace Outer { struct Inner { int i; }; } Templates can also be forward declared: // forward declaration template<typename> struct Example; // Usage of forward declaration void bar(const Example<int>& arg); // declaration template<typename T> struct Example { T t; }; Advantages: No runtime overhead Disadvantages: Forward declarations and actual declaration have to be kept in sync. Moving Implementation Details Out Of Templates Consider the following example which leaks implementation details: // Application.h #include <roapi.h> template <typename AppClass> void RunApplication(AppClass& app) { RoInitialize(RO_INIT_MULTITHREADED); app.Init(); while(!app.Run()) {} app.DeInit(); RoUninitialize(); } The two functions RoInitialize and RoUninitialize are platform specific functions and require the include roapi.h. We can't move the function into a .cpp because the implementation for templates needs to be known when using them. As a result this template leaks its implementation detail. To fix this issue we need to wrap the leaking function calls into separate functions and forward declare these functions. // Application.h void InitPlatform(); void DeInitPlatform(); template <typename AppClass> void RunApplication(AppClass& app) { InitPlatform(); app.Init(); while(!app.Run()) {} app.DeInit(); DeInitPlatform(); } // Application.cpp #include \"Application.h\" #include <roapi.h> void InitPlatform() { RoInitialize(RO_INIT_MULTITHREADED); } void DeInitPlatform() { RoUninitialize(); } As you can see we removed the include to roapi.h from the header file and moved it into the cpp file. This way our header no longer leaks underlying implementation details, as the user won't see the cpp file when using our library. If considerable parts of the template don't depend on the template arguments this pattern can also be used to reduce code bloat by moving the non dependent parts out into non-templated functions. Pimpl Light The pattern that I call \"Pimpl light\" can be used to hide implementation detail at the cost of an additional allocation: Consider our original ezTexture2D example it would be modified like this: // Texture2D.h class ezTexture2D { public: ezTexture2D(); ~ezTexture2D(); void Bind(); private: struct Impl; // forward declaration ezUniquePtr<Impl> m_pImpl; }; // Texture2D.cpp #include \"Texture2D.h\" #include <d3d11.h> // Declaration of ezTexture2D::Impl struct struct ezTexture2D::Impl { ID3D11Texture2D* m_ptr; }; ezTexture2D::ezTexture2D() : m_pImpl(EZ_DEFAULT_NEW(Impl)) { } // all constructors / destructors / assignment operators must be in .cpp file otherwise forward declaration will not work. ezTexture2D::~ezTexture2D() { } ezTexture2D::Bind() { // Use the implementation detail m_pImpl->m_ptr->Bind(); } This is an easy pattern to hide implementation details. Advantages: Simple to implement, hides nasty implementation details well Disadvantages: Additional allocation Additional indirection Pimpl Inheritance The Pimpl pattern can also be implemented by using inheritance instead of a forward declared struct. For our ezTexture2D example it would look like this: // Texture2D.h class ezTexture2D { public: ezUniquePtr<ezTexture2D> Make(); // factory function, could also return a shared ptr. virtual ~ezTexture2D(); void Bind(); private: ezTexture2D(); // All constructors must be private friend class ezTexture2DImpl; // This is the only class allowed to derive from ezTexture2D }; // Texture2D.cpp #include \"Texture2D.h\" #include <d3d11.h> // Actual implementation class ezTexture2DImpl : public ezTexture2D { public: ezTexture2DImpl() : ezTexture2D() {} ~ezTexture2DImpl(){} ID3D11Texture2D* m_ptr; }; ezTexture2D::ezTexture2D() {} ezTexture2D::~ezTexture2D() {} ezUniquePtr<ezTexture2D> ezTexture2D::Make() { return ezUniquePtr<ezTexture2D>(EZ_DEFAULT_NEW(ezTexture2DImpl)); } ezTexture2D::Bind() { // Use the implementation detail reinterpret_cast<ezTexture2DImpl*>(this)->m_ptr->Bind(); } As you see this version of pimpl hides the implementation detail similar to pimpl light. Advantages: No additional indirection (compared to pimpl light) Disadvantages: Additional allocation Can no longer inherit from ezTexture2D ezTexture2D can't be final Opaque array of bytes We can also place an opaque array of bytes large enough to store our implementation detail. Considering our ezTexture2D example it would look like this: // ezTexture2D.h class ezTexture2D { public: void Bind(); private: #if EZ_ENABLED(EZ_PLATFORM_32BIT) struct EZ_ALIGN(Impl, 4) { ezUInt8 m_Data[4]; }; #else struct EZ_ALIGN(Impl, 8) { ezUInt8 m_Data[8]; }; #endif Impl m_impl; }; // ezTexture2D.cpp #include \"Texture2D.h\" struct ezTexture2DImpl { D3D11Texture2D* m_ptr; }; static_assert(sizeof(ezTexture2D::Impl) == sizeof(ezTexture2DImpl), \"ezTexture2D::Impl has incorrect size\"); static_assert(alignof(ezTexture2D::Impl) == alignof(ezTexture2DImpl), \"ezTexture2D::Impl has incorrect alignment\"); void ezTexture2D::Bind() { // Use implementation detail reinterpret_cast<ezTexture2DImpl*>(&m_impl)->m_ptr->Bind(); } This again hides the implementation details in the header file. Advantages: No runtime overhead Disadvantages: High maintenance burden. Especially if implementation detail size varies on different platforms. Ignore the problem You can choose to ignore the leaky abstraction issue and tell the header checker tool to ignore a certain file to be included or give a certain file the permission to include anything. Each module in ezEngine that uses the header checker has a headerCheckerIgnore.json file where you can add ignores. It looks like this: { \"includeTarget\" : { \"byName\" : [ \"a.h\" ] }, \"includeSource\" : { \"byName\" : [ \"b.h\" ] } } In the above file every time a.h is included and would generate an error in the header checker tool, that error will be ignored. Every time b.h includes a header file that would cause an error, this error will also be ignored. Advantages: Less work Disadvantages: Longer compile times Conflicts due to global namespace pollution Requires users to have all header files for implementation details available See Also"
  },
  "pages/docs/tools/inspector.html": {
    "href": "pages/docs/tools/inspector.html",
    "title": "ezInspector | ezEngine",
    "keywords": "ezInspector ezInspector is a tool to monitor some internal state of an application. It helps observing how the application operates, which resources it accesses and why it might behave as it does. The current version allows to monitor the following data: Log: The Log panel displays all the log messages. It allows to filter by severity and search by keywords. Memory Usage: The Memory panel displays the number of allocations, the amount of memory in use (per allocator) and a time-line how memory usage changes. Input: Shows which physical buttons are available and what their state is. Also displays the virtual input actions, by which keys they get triggered and what their current state is. Subsystems: Displays information about all the available subsystems in the engine. Plugins: Shows which plugins are loaded and which other plugins they depend on. Global Events: Shows which global events are registered and how often they occur. File Operations: This panel shows which files get accessed by the engine, whether they occur on the main thread, how much data is read or written per operation, how much time that takes (and thus why an application might be blocking or stuttering). Allows to sort and filter by different criteria to get a better grasp at what and how data is accessed. CVars: This panel displays all CVars that are available. You can not only see their current values, but also modify them, such that you can change the behavior of the application without restarting it. This allows to quickly change parameters of things that you are trying out, such that you can see the effects immediately. Console: The CVar panel also displays a console window, where you can type commands the same way as in the in-game console. Pressing TAB auto-completes input, arrow up/down searches the history, and so on. This can be used to modify CVar state, but also to execute console functions. Stats: Using ezStats a game can display the status of certain internals. This allows to make it easy to inspect what a game object is doing or what state some component is in. So instead of printing this debug information on screen inside the game, you can watch it with ezInspector. Additionally ezInspector allows to mark stats as 'favorites' which means you can output hundreds of stats in your game, but easily only display the subset that you are currently interested in inside ezInspector. Additionally, it is now possible to display the history of a stat variable in a separate panel as a graph. This makes it easy to observe how some stats behave over time (such as frame time, frames-per-second, etc.). Time: Displays all ezClock instances that are active. Shows the raw time step and the filtered time step, which allows to see hiccups and general performance characteristics of the application. Reflection: Shows all reflected types and their class hierarchy. Also shows which properties each reflected type provides. Data Transfer: This panel allows to pull data from an application. What data can be pulled is determined by what the application provides. For example an application might provide the G-Buffer as a set of images to be pulled. See ezDataTransfer for further details. Resources: This panel shows all loaded resources. You can filter by type and name and you can sort the resources by various criteria. Setting up your game to support ezInspector Note: None of this setup is required when you use ezGameApplication as your application base class, or even better, your game only implements an ezGameState and is run directly through ezPlayer. The inspection functionality is implemented in the ezInspectorPlugin plugin, so you need to have that compiled. In your application you can then either simply always link against that plugin to activate the functionality, or you can load it dynamically at runtime. Additionally the ezInspectorPlugin uses ezTelemetry to phone home, so you need to have that activated. // Activate ezTelemetry such that the inspector plugin can use the network connection. ezTelemetry::CreateServer(); // Load the inspector plugin // The plugin contains automatic configuration code (through the ezStartup system), // so it will configure itself properly when the engine is initialized by calling ezStartup::StartupCore(). // When you are using ezApplication, this is done automatically. ezPlugin::LoadPlugin(\"ezInspectorPlugin\"); You should insert this code somewhere in the engine initialization. When you are using ezApplication, put this into the AfterEngineInit function. Additionally you need to make sure that ezTelemetry is updated once per frame, to ensure that all changes are sent to ezInspector regularly: // Call this once per frame to make sure all changes are transmitted ezTelemetry::PerFrameUpdate(); And that's it! The rest is done automatically. Connecting to a Process If you run a custom app or ezPlayer on the same machine, ezInspector should manage to automatically connect. If you are running your app on a different machine, you need the respective IP address. For ezPlayer and custom apps, the default port is 1040. For EditorEngineProcess.exe, the default port is 1050. So if you want to connect with ezInspector to the editor, you need to provide this port instead. How to get the best out of ezInspector Some tips, what to do to benefit from the inspection functionality: Use the logging system ezLog to output what your application is doing, and which errors it encounters. Use EZ_LOG_BLOCK to group logging information. Use the ezStats system to write out information about what is going on in your application. The more information that you track, the easier you can figure out what is going wrong. Use CVars to allow configuration of your code at runtime. It is easy to add CVars and thus you should use them whenever you are working on something new, to be able to tweak its behavior quickly. Once you are finished with something you should strip all unnecessary CVars again, but often it makes sense to still keep some configuration options for later. When you are developing larger subsystems that you might want to know the memory consumption of, use a custom allocator for all allocations in that subsystem, then you can track its memory behavior better. See Also CVars Logging Stats"
  },
  "pages/docs/tools/minidumptool.html": {
    "href": "pages/docs/tools/minidumptool.html",
    "title": "MiniDump Tool | ezEngine",
    "keywords": "MiniDump Tool The MiniDumpTool writes a mini-dump (memory, call-stacks) of an application. The mini dump can be used for diagnosing why an application crashed. The tool is used by the unit tests. Arguments The tool takes exactly two arguments: MiniDumpTool -PID 1234 -f \"C:/crashdumps/justnow.dmp\" The first argument is the Process ID of the process for which the memory shall be dumped, the second argument specifies the file where the dump should be written to. Automatic Execution You can integrate writing crash dumps into your application by setting the ezCrashHandler_WriteMiniDump through ezCrashHandler::SetCrashHandler(). ezCrashHandler_WriteMiniDump has options to generate the filename automatically using the current date and time. See Also Unit Tests"
  },
  "pages/docs/tools/player.html": {
    "href": "pages/docs/tools/player.html",
    "title": "ezPlayer | ezEngine",
    "keywords": "ezPlayer The ezPlayer is a stand-alone application that can run any ezEngine game that is properly embedded in its own DLL. The ezEditor can launch a scene directly in the ezPlayer application. The ezPlayer is meant for testing and as a very slim example of how to write a custom game application. Arguments The Player takes these command line arguments: Player.exe -project \"ProjectPath\" -scene \"ScenePath\" [-wnd \"optional/path/to/Window.ddl\"] [-profile \"OptionalAssetProfileName\"] With: ProjectPath: The absolute path to the project directory. ScenePath: A relative path to a scene file. It is relative to the data directory that it resides in. If it is a path to an .ezScene or .ezPrefab file, ezPlayer automatically redirects the path to the corresponding exported .ezObjectGraph file in the AssetCache. Typically you only need to pass the path to the project and scene (or prefab) file. The other options are used by the ezEditor to select different configurations. Execution The Player will automatically detect the projects directory by searching the file system for an ezProject file. It then executes the core ezEngine functionality, meaning it reads the configuration for the data directories as well as the engine plugins from the project config files. If the scene requires custom (game) plugins, they must be referenced in those config files. Then it will execute the regular game loop. Thus, if the scene contains game objects to spawn a character controller or otherwise handle input and move the camera, you will be able to interact with it. If a custom plugin implements a custom game state that will be instantiated and can take full control over the application logic. If no such functionality is available, the Player will instantiate the ezFallbackGameState which will spawn a player object, if a ezPlayerStartPointComponent is part of the scene. Otherwise it will provide a simple WASD camera movement scheme. If ezCameraComponents are placed in the scene, you can cycle through them using Page Up and Page Down. Pressing Escape will close the Player application (unless overridden by a custom game state). Common Application Features Since ezPlayer is built on the application (TODO) framework, it provides a set of useful features common to all EZ applications. See this page for details. See Also Game States Engine Plugins Projects ezEditor Overview"
  },
  "pages/docs/tools/shadercompiler.html": {
    "href": "pages/docs/tools/shadercompiler.html",
    "title": "ShaderCompiler | ezEngine",
    "keywords": "ShaderCompiler The ShaderCompiler is a command-line application to precompile shader permutations. This tool can be used to prepare shaders and shader permutations before they are needed at runtime. On platforms where runtime shader compilation is possible, EZ will compile shader permutations on demand. This leads to a small delay when a new permutation is encountered, but is very convenient during development. By precompiling all necessary permutations, it is possible to prevent this delay. On platforms that do not allow runtime shader compilation this is even necessary for the shaders to be available, at all. Command Line Options For the full list of available command line options, run ShaderCompiler.exe -help -project <path>: The compiler takes a path to a project to resolve paths relative to the project directory. -platform <name>: The name of the target platform for which to compile the shaders. For example DX11_SM50. See the command line help for all options. -shader <path1;path2>: Semicolon separated list of paths to shader files or folders containing shaders. Paths may be absolute or relative to the -project directory. If a path to a folder is specified, all .ezShader files in that folder are compiled. -perm <PERM1=TRUE PERM2=11>: List of permutation variables to set to fixed values. For all other permutation variables, all possible combinations are used to compile the shaders. Spaces are used to separate multiple arguments. Example ShaderCompiler.exe -project \"C:\\ez\\Data\\Base\" -platform DX11_SM50 -shader \"Shaders\\Debug\" -perm TOPOLOGY=TOPOLOGY_LINES CAMERA_MODE=CAMERA_MODE_PERSPECTIVE See Also Shaders"
  },
  "pages/docs/tools/staticlinkutil.html": {
    "href": "pages/docs/tools/staticlinkutil.html",
    "title": "StaticLinkUtil | ezEngine",
    "keywords": "StaticLinkUtil When statically linking libraries into an application the linker will only pull in all the functions and variables that are inside translation units (CPP files) that somehow get referenced. In EZ a lot of stuff happens automatically (e.g. types register themselves etc.), which is accomplished through global variables that execute code in their constructor during the application's startup phase. This only works when those global variables are actually put into the application by the linker. If the linker does not do that, functionality will not work as intended. Mitigation Contrary to common believe, the linker is not allowed to optimize away global variables. The only reason for not including a global variable into the final binary, is when the entire translation unit where a variable is defined in, is never referenced and thus never even looked at by the linker. To fix this, the StaticLinkUtil inserts macros into each and every file which reference each other. Afterwards every file in a library will reference every other file in that same library and thus once a library is used in any way in some program, the entire library will be pulled in and will then work as intended. These references are accomplished through empty functions that are called in one central location (where EZ_STATICLINK_LIBRARY is defined), though the code actually never really calls those functions, but it is enough to force the linker to look at all the other files. Usage Call this tool with the path to the root folder of some library as the sole command line argument: StaticLinkUtil.exe \"C:\\ezEngine\\Code\\Engine\\Foundation\" This will iterate over all files below that folder and insert the proper macros. Also make sure that exactly one file in each library contains the text EZ_STATICLINK_LIBRARY(); The parameters and function bodies will be generated automatically and later updated, you do not need to provide more. You only need to run this tool, if you intend to link statically, which is only needed on some platforms. Even then, most new code will work even without always keeping the static link macros up to date, as the issues that it fixes are not too common. If, however, you notice that some types are missing (such as new components) that were just added, you should rerun this tool on the affected libraries. See Also"
  },
  "pages/docs/tools/texconv.html": {
    "href": "pages/docs/tools/texconv.html",
    "title": "ezTexConv | ezEngine",
    "keywords": "ezTexConv TexConv is a command-line tool to process textures from typical input formats like PNG, TGA, JPEG and DDS into optimized formats for runtime consumption. The most common scenario is to convert a single input file A.xxx into an optimized format B.yyy. However, the tool has many additional options for advanced uses. Command-line Help Run TexConv.exe with the --help parameter to list all available options. Additionally, TexConv prints the used options when it is executed, to help understand what it is doing. Consult this output for details. General Usage TexConv always produces exactly one output file. It may use multiple input files to assemble the output from. For the assembly, it also needs a channel mapping, which tells it which channel (Red, Green, Blue or Alpha) to take from which input file and move it into which channel of the output image. The most straight forward command line is this: TexConv.exe -out D:/result.dds -in0 D:/img.jpg -rgba in0 -out specifies the output file and format -in0 specifies the first input image -rgba tells it that the output image should use all four channels and that they should be taken 1:1 from the input image Multiple Input Files To assemble the output from multiple input files, specify each input file using the -in option with an increasing number: -in0 D:/img0.jpg -in1 D:/img1.jpg -in2 D:/img2.jpg ... When assembling a cubemap from 2D textures, one can also use -right, -left, -top, -bottom, -front, -back or -px, -nx, -py, -ny, -pz, -nz. To map these inputs to the output file, a proper channel mapping is needed. Channel Mappings The channel-mapping options specify from which input to fill the given output channels. You can specify the input for each channel individually like this: -r in0.b -g in0.g -b in0.r -a in1.r Here the RGB channels of the output would be filled using the first input image, but red and blue will get swapped. The alpha channel of the output would be filled with the values from the red channel of the second input image. Specifying the mapping for each channel separately gives the greatest flexibility. For convenience the same can be written using \"swizzling\" operators: -rgb in0.bgr -a in1.r Output Channels The following channel-mapping options are available: -r, -g, -b, -a : These specify single channel assignments. -rg : Specify the red and green channel assignments. -rgb : Specify the red, green and blue channel assignments. -rgba : Specifies all four channel assignments. Mentioning only the R, RG or RGB channel, instructs TexConv to create an output file with only 1, 2 or 3 channels respectively. Input Swizzling When stating which input texture should fill which output channel, one can swizzle the input: -rgba in0 is equivalent to -rgba in0.rgba -rgba in0.bgra will swizzle the input channels -rgb in0.rrr will duplicate the red channel into all channels One may also fill channels with either black or white: -rgb in0 -a white will create a 4 channel output texture but set alpha to fully opaque -rg black -b white will create an entirely blue texture Common Options The most interesting options are listed below. More options are listed by TexConv --help. Output Type -type 2D : The output will be a regular 2D image. -type Cubemap : The output will be a cubemap image. Only supported for DDS output files. When this is specified, one can assemble the cubemap from 6 regular 2D input images. Image Compression -compression none : The output image will be uncompressed. -compression medium : If supported, the output image will use compression without sacrificing too much quality. -compression high : If supported, the output image will use compression and sacrifice quality in favor of a smaller file. Mipmaps By default, TexConv generates mipmaps when the output format supports it. -mipmaps none : Mipmaps will not be generated. -mipmaps Linear : If supported, mipmaps will be generated using a box filter. Usage (sRGB / Gamma Correction) The -usage option specifies the purpose of the output and thus tells TexConv whether to apply gamma correction to the input and output files. The usage only affects the RGB channels. The alpha channel is always considered to contain 'linear' values. If usage is not specified, the 'auto' mode will try to detect the usage from the format and file-name of the first input image. For instance, single and dual channel output formats are always linear. Check the output to see what decision TexConv made. -usage Linear : The output image contains values that do not represent colors. This is typically the case for metallic and roughness textures, as well as all kinds of masks. -usage Color : The output image represents color, such as diffuse/albedo maps. The sRGB flag will be set in the output DDS header. -usage HDR : The output file should use more than 8 bits per pixel for encoding. Consequently all values are stored in linear space. For HDR textures it does not matter whether the data represents color or other data. -usage NormalMap : The output image represents a tangent-space normal map. Values will be normalized and mipmap computation will be optimized slightly. -usage NormalMap_Inverted : The output is a tangent-space normal map with Y pointing in the opposite direction than the input. Image Rescaling -minRes 64 : Specifies the minimum resolution of the output. If the input image is smaller, it will get upscaled. -maxRes 1024 : Specifies the maximum resolution of the output. If the input image is larger, it will get downscaled. -downscale 1 : If this is larger than 0, the input images will be halved in resolution N times. Use this to apply an overall quality reduction. Examples Convert a Color Texture TexConv.exe -out D:/diffuse.dds -in0 D:/diffuse.jpg -rgba in0 -usage color Convert a Normal Map TexConv.exe -out D:/normalmap.dds -in0 D:/normalmap.png -rgb in0 -usage normalmap Create an HDR Cubemap TexConv.exe -out \"D:/skybox.dds\" -in0 \"D:/skymap.hdr\" -rgba in0 -type cubemap -usage hdr A great source for HDR cubemaps is hdrihaven.com. Bake Multiple Images into One TexConv.exe -out \"D:/Baked.dds\" -in0 \"D:/metal.tga\" -in1 \"D:/roughness.png\" -in2 \"D:/DiffuseAlpha.dds\" -r in1.r -g in0.r -b black -a in2.a -usage linear Extract a Single Channel TexConv.exe -out D:/alpha-mask-only.dds -in0 D:/DiffuseAlpha.dds -r in0.a See Also"
  },
  "pages/docs/tools/unit-tests.html": {
    "href": "pages/docs/tools/unit-tests.html",
    "title": "Unit Tests | ezEngine",
    "keywords": "Unit Tests ezEngine has a strong focus on reliability. Consequently, testing is taken very seriously. Due to the way game engines work, typical test frameworks are not a good fit, which is why EZ has its own test framework. Test Framework The test framework can be built purely for console execution or to have a graphical user interface. If the CMake variable EZ_ENABLE_QT_SUPPORT is set, the test applications will show a GUI (unless skipped via command line). Command Line Options Run any test suite with the argument -help to get the full list of available options. Test Structure The test framework works with tests and subtests. A test takes care of the slow initialization, like setting up the engine, the subtests then check various functionality, without rerunning the same initialization procedures. Tests and subtests can be disabled to focus on a specific issue. All tests are derived from ezTestBaseClass and global instances in code are automatically picked up and shown in the UI. For trivial tests, as used in the FoundationTest application there are additional helper macros EZ_CREATE_SIMPLE_TEST_GROUP and EZ_CREATE_SIMPLE_TEST to add a new test with just two lines of code. While running use the macros EZ_TEST_INT/FLOAT/STRING/... to check an assumption and make the test fail if it doesn't hold. There is also EZ_TEST_IMAGE and EZ_SCHEDULE_IMAGE_TEST to compare a screen capture against a stored reference image. You create reference images by running a succesful test once and then copying the result images to the proper folder. This can be done automatically for you through the Test Data menu in the GUI. Writing tests is generally quite easy. All the test infrastructure is well documented. The best way to figure out how to write a test, is to run the different test suites to see which test is similar to what you want to do. Then look at that test, and jump to the C++ code comments of the test infrastructure to learn what it is for. Test Suites The tests can be included or excluded from the build using the CMake variable EZ_BUILD_UNITTESTS. If enabled, they show up in MSVC under the top level UnitTests folder. The following test suites are available: FoundationTest: These tests are for the absolute base functionality, that's found in the Foundation project and the Texture project. CoreTest: These tests are for the core engine functionality, like the scenegraph and resource management (TODO). RendererTest: This tests the basic rendering functionality that's available with the rendering API abstraction. It doesn't test the high level rendering features. ToolsFoundationTest: This is for testing editor and tools specific infrastructure. GameEngineTest: These tests use the full engine functionality to test various high level features. Therefore they cover various rendering features as well as game features. EditorTest: This test runs the editor and exercises things like creating all types of asset documents, and so on. RemoteTestHarness: This is a helper project to run tests on UWP. See Also"
  },
  "pages/docs/ui/imgui.html": {
    "href": "pages/docs/ui/imgui.html",
    "title": "ImGui | ezEngine",
    "keywords": "ImGui Dear ImGui is a well known library for building immediate mode GUIs. The ImGui library was built to make it quick and easy to create GUIs that need to be functional, but not pretty. ImGui is popular with programmers, because it only takes a few lines of code to build UI panels with buttons, sliders, text boxes, checkboxs and many more. A very common use case for ImGui is for quick developer tools and for exposing options in tech demos. On the other hand, ImGui is not meant to be styled. Changing the appearance of ImGui elements is difficult, and controlling the layout of elements is only very basic. Using ImGui Dear ImGui is integrated by the singleton class ezImgui. To use ImGui, you need to allocate one such instance first: #ifdef BUILDSYSTEM_ENABLE_IMGUI_SUPPORT if (ezImgui::GetSingleton() == nullptr) { EZ_DEFAULT_NEW(ezImgui); } #endif This can be done for example in OnActivation() of a custom game state. Similarly, you should delete the ezImgui instance at shutdown: #ifdef BUILDSYSTEM_ENABLE_IMGUI_SUPPORT if (ezImgui::GetSingleton() != nullptr) { ezImgui* pImgui = ezImgui::GetSingleton(); EZ_DEFAULT_DELETE(pImgui); } #endif During a frame, the ezImgui instance needs to know which view to render the UI elements to. Therefore you should call this every frame: ezImgui::GetSingleton()->SetCurrentContextForView(m_hMainView); Often you only want to pass input to ImGui during certain phases of your game. This can be controlled via ezImgui::SetPassInputToImgui() and whether ImGui currently has focus in a certain UI element can be queried through ezImGui::WantsInput(). From there on, all the functionality of the Dear ImGui library is used directly, without any EZ specific wrappers. For example a simple panel is created like this: ImGui::SetNextWindowSize(ImVec2(200, 100), ImGuiCond_FirstUseEver); ImGui::Begin(\"Imgui Window\", &window); ImGui::Text(\"Hello World!\"); ImGui::SliderFloat(\"Slider\", &slider, 0.0f, 1.0f); ImGui::ColorEdit3(\"Color\", color); if (ImGui::Button(\"Toggle Stats\")) { stats = !stats; } if (stats) { ImGui::Text(\"Application average %.3f ms/frame (%.1f FPS)\", 1000.0f / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); } ImGui::End(); Samples The RTS Sample and the Sample Game Plugin demonstrate how to use ImGui. Limitations It is very difficult to adjust the appearance of ImGui elements. Use ImGui for developer tools and to prototype ideas quickly. Prefer to use RmlUi once you need more control over the look and feel. See Also Ingame UI RmlUi"
  },
  "pages/docs/ui/rmlui-canvas2d-component.html": {
    "href": "pages/docs/ui/rmlui-canvas2d-component.html",
    "title": "RmlUI Canvas 2D Component | ezEngine",
    "keywords": "RmlUI Canvas 2D Component The RML UI integration is in a good state, and building UIs is very much possible, however, the functionality is currently undocumented. See Also RmlUi"
  },
  "pages/docs/ui/rmlui.html": {
    "href": "pages/docs/ui/rmlui.html",
    "title": "RmlUi | ezEngine",
    "keywords": "RmlUi RmlUi is a third-party GUI library that uses an HTML-like syntax to describe UI elements, and CSS to style them. RmlUi is lightweight, yet flexible. Support for RmlUi is provided through a dedicated engine plugin. To enable it in your project, activate the plugin in the project settings. Rml Documentation The documentation for RmlUi can be found here. Please refer to that documenation for any questions around how to use RmlUi. Sample The RTS Sample shows how to use RmlUi. Have a look at it the project in the editor, it contains Rml assets. The editor shows a live preview for Rml canvases, and you can edit the respective .rml files to see the effect: The sample uses multiple RmlUi Canvas 2D components in its scene to place the UI elements. At runtime the RTS sample's game code accesses the RmlUi functionality through the ezRmlUiCanvas2DComponent. Search the sample's code for those places to see how to interact with the GUI. Using ezRmlUiCanvas2DComponent::GetRmlContext() you get access to the ezRmlUiContext. This class implements Rml::Core::Context. This gives you access to all the RmlUi features. See the RmlUi documentation for details. Work in Progress The integration of RmlUi into ezEngine is functional, but still a work-in-progress. At the moment you can only build 2D GUIs that appear on top of the screen. In the future we plan to also support placing GUI elements inside the 3D environment. Also, currently you need to work directly with the RmlUi C++ code to interact with the UI. We may add convenience functionality to make simple use cases of GUIs easier. There is no TypeScript binding for RmlUi. Most likely this will never change, however, if we get around to adding some convenience features, that may make it possible to use a limited feature set from script code. See Also Ingame UI ImGui"
  },
  "pages/docs/ui/ui.html": {
    "href": "pages/docs/ui/ui.html",
    "title": "Ingame UI | ezEngine",
    "keywords": "Ingame UI To create ingame user interfaces, there are two options available: ImGui RmlUi ImGui is very easy to use. With just a few lines of code you can add a 2D user interface to your game. It is very limited in styling, though. It's main purpose is either to mock up a UI quickly, or to expose developer tools. RmlUi on the other hand is meant for building proper game UI, and therefore supports lots of styling options through CSS and and HTML like syntax for layouting. It is more work to set up, but gives you all the flexibilty you need. It is recommended to use ImGui during early development and switch to RmlUi once your game reaches a more mature state. It is also possible to use both UI systems in parallel. See Also ImGui RmlUi"
  },
  "pages/docs/xr/xr-components.html": {
    "href": "pages/docs/xr/xr-components.html",
    "title": "XR Components | ezEngine",
    "keywords": "XR Components Stage Space Component The stage space defines the position, rotation and scale relative to which the user is placed in the virtual world. The ezStageSpaceComponent should be placed on the game object that best represents the character's position in the world. By scaling the stage space, you can change the scale of the virtual user relative to the world. Turning you into a giant or a small ant. This can be very useful for physics precision for example which might not work well with very small object scales. You can compensate this by building a larger world and scaling the stage space down. Component Properties Stage Space: Can be either Standing or Seated. This defines the offset of the HMD to the stage space. In Standing mode, the stage space should be placed on the floor as the HMD will be placed relative to it matching the physical height of the user's head over the physical floor. In Seated mode, the HMD position relative to the stage space will match the users' head position relative to their head position when the app was started. Device Tracking Component To automatically make a game object follow the HMD or one of the controllers, you can attach a ezDeviceTrackingComponent to a game object. Component Properties Device Type: Which device we want to track. E.g. the HMD (a.k.a. your head) or one of the controllers. Pose Location: Some input devices have different poses for Grip and Aim. E.g. your hand's grip position is in the middle of your fist pointing upwards, while the aim position is at your index finger pointing forward. Transform Space: Whether the local or global transform should be set to the input device's transform. Rotation: Whether to apply rotation to the game object. Translation is always applied but in some cases it might be useful to not apply rotation. E.g. for helper nodes in the scene. Scale: Whether to apply scale to the game object. This has no affect in local space as no device type has scale. If Transform Space is set to global, the scale of the object will will match the scale of the stage space. Spatial Anchor Component Many AR scenarios require virtual objects to be anchored in the real world. The simplest way to achieve this is to add an ezSpatialAnchorComponent to the game object in question. This component does not have any properties. Persistance of anchors across sessions is not yet supported. Once the component is added, it will use ezXRSpatialAnchorsInterface to stabilize the object's transform in the real world. Visualize Hand Component For debugging it can be useful to visualize tracked hands. Create a ezVisualizeHandComponent and attach it to a random game object to achieve this. No properties exist for this component. See Also XR Overview XR Input"
  },
  "pages/docs/xr/xr-graphics.html": {
    "href": "pages/docs/xr/xr-graphics.html",
    "title": "XR Graphics | ezEngine",
    "keywords": "XR Graphics XR needs to render two images, one for each eye. Therefore, special care needs to be taken when authoring shaders to make sure they support stereo rendering. Material Shaders To make a material shader work with stereo rendering, it must contain the following sections: [PERMUTATIONS] // Will be set to CAMERA_MODE_STEREO. CAMERA_MODE // Defined if the GPU supports setting the render target array index in the vertex shader. If not, a geometry shader will be used. VERTEX_SHADER_RENDER_TARGET_ARRAY_INDEX [VERTEXSHADER] #include <Shaders/Materials/MaterialVertexShader.h> VS_OUT main(VS_IN Input) { // FillVertexData will set s_ActiveCameraEyeIndex to either 0 or 1. s_ActiveCameraEyeIndex is used in all camera related functions to pull in the correct eye projection / transform etc. VS_OUT Output = FillVertexData(Input); //... return Output; } [GEOMETRYSHADER] // Will only be active if VERTEX_SHADER_RENDER_TARGET_ARRAY_INDEX is not supported. #include <Shaders/Materials/MaterialStereoGeometryShader.h> [PIXELSHADER] // If you use the default MaterialPixelShader.h and just implement GetDiffuseColor() etc then all stereo rendering is done for you. If you write a custom pixel shader, you will need to add this at the start: // #if CAMERA_MODE == CAMERA_MODE_STEREO // s_ActiveCameraEyeIndex = Input.RenderTargetArrayIndex; // #endif #include <Shaders/Materials/MaterialPixelShader.h> Postprocessing Shaders Post-processing shaders are a bit more complicated than material shaders as they usually pull in data from a previous render pipeline pass which will now be an array texture as the input will be stereo as well. Here is a small example of a full-screen render pass and what it requires in order to work in stereo mode: [PERMUTATIONS] // Will be set to CAMERA_MODE_STEREO. CAMERA_MODE // Defined if the GPU supports setting the render target array index in the vertex shader. If not, a geometry shader will be used. VERTEX_SHADER_RENDER_TARGET_ARRAY_INDEX [VERTEXSHADER] #include <Shaders/Pipeline/FullscreenTriangleVertexShader.h> [GEOMETRYSHADER] // Will only be active if VERTEX_SHADER_RENDER_TARGET_ARRAY_INDEX is not supported. #include <Shaders/Pipeline/FullscreenTriangleStereoGeometryShader.h> [PIXELSHADER] #include <Shaders/Pipeline/FullscreenTriangleInterpolator.h> // Note that this will work fine in non-stereo rendering as well as 2D textures are just 2D-array texture with very few slices. Texture2DArray Input; float4 main(PS_IN input) : SV_Target { // To make all camera related functions work correctly, this must be called at the very start to define the right eye. #if CAMERA_MODE == CAMERA_MODE_STEREO s_ActiveCameraEyeIndex = Input.RenderTargetArrayIndex; #endif float4 res = Input.Sample(LinearClampSampler, float3(input.TexCoord0, s_ActiveCameraEyeIndex)); //... return res; }"
  },
  "pages/docs/xr/xr-input.html": {
    "href": "pages/docs/xr/xr-input.html",
    "title": "XR Input | ezEngine",
    "keywords": "XR Input XR input is not much different from regular input. XR controllers are normal input devices that are provided by the ezXRInputDevice and work just like any other input device with the added feature set that the controllers can be tracked in space. XR Feature Flags There are various kinds of XR controllers which varying feature sets. At runtime, you can query the existance of a type of device via the ezXRInputDevice::GetDeviceIDByType function and its supported features by using the ezXRInputDevice::GetDeviceFeatures function. ezXRInterface* pXRInterface = ezSingletonRegistry::GetSingletonInstance<ezXRInterface>(); ezXRInputDevice& xrInput = pXRInterface->GetXRInput(); ezXRDeviceID deviceID = pXRInterface->GetXRInput().GetDeviceIDByType(ezXRDeviceType::LeftController); if (deviceID != -1) { ezBitflags<ezXRDeviceFeatures> features = pXRInterface->GetXRInput().GetDeviceFeatures(deviceID); if (features.IsSet(ezXRDeviceFeatures::GripPose)) { Besides using the feature flags, you can also query the name of the controller via the ezXRInputDevice::GetDeviceName call. This will return e.g. Mixed Reality Motion Controller or Hand Interaction. XR Device Presence The ezXRDeviceType::HMD represents your head and is always present. This is not true for other controller. You can either poll their state via the ezXRInputDevice::GetDeviceIDByType function or you can subscribe to ezXRInputDevice::GetInputEvent which will inform you whenever input devices are added or removed. XR Input Mapping XR input slots are defined in Code\\Engine\\GameEngine\\XR\\XRInputDevice.h and all start with xr_. Use the same machanism as for other input controllers to create an input set configuration. Pose Tracking XR controllers also provide positional data. You can either use the ezDeviceTrackingComponent to automatically make a game object follow a controller or you can manually query the controller transform using the ezXRInputDevice::GetDeviceState function. Many devices support two poses Grip and Aim. The difference between the two is nicely explained here. ezXRInterface* pXRInterface = ezSingletonRegistry::GetSingletonInstance<ezXRInterface>(); ezXRInputDevice& xrInput = pXRInterface->GetXRInput(); ezXRDeviceID deviceID = pXRInterface->GetXRInput().GetDeviceIDByType(ezXRDeviceType::LeftController); if (deviceID != -1) { const ezXRDeviceState& state = pXRInterface->GetXRInput().GetDeviceState(deviceID); if (state.m_bDeviceIsConnected && state.m_bGripPoseIsValid) { ezVec3 vPosition = state.m_vGripPosition; ezQuat qRotation = state.m_qGripRotation; You should check for m_bDeviceIsConnected and either m_bGripPoseIsValid or m_bAimPoseIsValid before accessing the transform. Due to e.g. tracking loss, the controller can provide invalid poses at any point. Hand Tracking Hands (if supported by the platform) are exposed as input controllers via the input system as well as via raw hand-tracked data. If basic click interaction and pose tracking is not enough, you can use the ezXRHandTrackingInterface if present, to query the bone positions of your hands: ezXRHandTrackingInterface* pXRHand = ezSingletonRegistry::GetSingletonInstance<ezXRHandTrackingInterface>(); if (!pXRHand) return; ezHybridArray<ezXRHandBone, 6> bones; for (ezXRHand::Enum hand : {ezXRHand::Left, ezXRHand::Right}) { for (ezUInt32 uiPart = 0; uiPart < ezXRHandPart::COUNT; ++uiPart) { ezXRHandPart::Enum part = static_cast<ezXRHandPart::Enum>(uiPart); if (pXRHand->TryGetBoneTransforms(hand, part, ezXRTransformSpace::Global, bones) == ezXRHandTrackingInterface::HandPartTrackingState::Tracked) { See Also Input XR Overview XR Components"
  },
  "pages/docs/xr/xr-overview.html": {
    "href": "pages/docs/xr/xr-overview.html",
    "title": "XR | ezEngine",
    "keywords": "XR NOTE: XR support is still in development. You will need to enable Show in Development Features in the editor settings to use it. XR stands for both VR (virtual reality) as well as AR (augmented reality) devices. Currently supported devices: VR: Windows desktop VR devices that support DX11 and OpenXR. AR: HoloLens 2 via Windows UWP, DX11 and OpenXR. Currently there are three XR implementations: ezDummyXR: If XR is requested and no other plugin is available the dummy XR implementation is used. This one allows for stereo rendering to be tested on a PC without needing to use an actual HMD (head mounted device). ezOpenXR: This plugin uses OpenXR and supports both AR and VR devices, DX11 only for now. ezOpenVR: WIP, currently not functional. Getting started To get started with creating an XR application, please follow the XR Project Setup guide. See Also XR Project Setup XR Graphics XR Input XR Components"
  },
  "pages/docs/xr/xr-project-setup.html": {
    "href": "pages/docs/xr/xr-project-setup.html",
    "title": "XR Project Setup | ezEngine",
    "keywords": "XR Project Setup XR Project Config As an example, we will use the Testing Chambers project and its Surfaces scene to set up XR rendering for desktop VR use. Start by opening the project and scene. Enable Show in Development Features in the editor settings and restart it to see XR features in the editor. To use XR in your project you must first load a plugin to support XR devices. In the plugin selection dialog, select your custom XR plugin. In this case, we select the Open XR plugin and close the dialog. Next is to enable XR rendering under asset profiles. Select the profile you want to enable XR in and then check the Enable XR checkbox. You also need to select the XR Render Pipeline here. Currently, both the MainRenderPipeline and the HololensRenderPipeline fully support XR rendering. Let's select the MainRenderPipeline for this example and close the dialog. Pressing 'Play the Game' in the scene should now already start rendering on the HMD and you can look around but it will not be very interactive. Character Rig Setup To be able to move around and use our controllers, we need to modify our character rig to support XR input scenarios. As an example, we will modify the Player prefab found in the testing chambers project. Stage Space Once a XR plugin is active it takes control of the rendering camera. This means that the transform and projection of the camera component is no longer taken into account when rendering. To move the user in XR, we need to instead move the stage space that the XR system is using as a reference point to place the user into the scene. The ezStageSpaceComponent does just that. In our example, add the component to the root Player object of the prefab as it best represents the players position in the world. If the player moves, it will also move the XR camera and controllers relative to it. Device Tracking As mentioned above, the XR plugin takes ownership of the rendering camera. However, in many cases you will want to reflect the HMD position and controller positions in the scene as well. A Simple way of achieving this is to add a ezDeviceTrackingComponent to a game object that you like to follow one of the XR input devices. Add one component to the Camera game object with default settings and one to the Gun game object with device type Right Controller, pose location Aim and Global transform. Safe the prefab and play the scene again. You should now be able to move the controller via a gamepad and point the gun at things using your right controller. Input Mapping Next, you will need to map XR controller input to input actions. Go to the Input Set Configuration dialog and change Shoot to xr_hand_right_select_click. If you play the scene again, you should be able to shoot with your right controller. More details can be found in the XR Input chapter. Rendering Multithreading By default, the engine renders multithreaded. This means that one frame of delay is introduced. This will worsen the stability of the XR experience. If enough CPU headroom is available, consider disabling the cvar Rendering.Multithreading using the methods outlined here. See Also XR Graphics XR Input XR Components"
  },
  "pages/getting-started/binaries.html": {
    "href": "pages/getting-started/binaries.html",
    "title": "Prebuilt Binaries | ezEngine",
    "keywords": "Prebuilt Binaries Packages with prebuilt binaries are published with GitHub releases. Follow @ezEngineProject for release announcements. The latest version is: 23.12"
  },
  "pages/getting-started/editor-overview.html": {
    "href": "pages/getting-started/editor-overview.html",
    "title": "ezEditor Overview | ezEngine",
    "keywords": "ezEditor Overview ezEditor is the central application for authoring content and bringing existing assets together. It includes a scene editor, functionality for working with meshes, textures, materials, particles and sounds, as well as a visual shader editor (TODO), visual scripting and prefabs. The editor transforms assets from source data into the optimized runtime formats and keeps track which assets are up to date. The editor can also run the game logic inside the viewport while making edits, or start a complete play-the-game mode which lets you experience your creation without delay. The runtime functionality of the editor lives in a separate engine process, which makes the editor very robust. If the engine crashes, the editor can just relaunch it within a second, without loss of work and with minimal interruption. Compiling the Editor The editor currently only builds on Windows and requires Qt. See Building ezEngine. Sample Projects See Testing Chambers. Setting Up a Custom Project See Projects. Importing Assets See Asset Import Video: ezEngine Overview Video: Demo Level Playthrough See Also Samples Videos"
  },
  "pages/getting-started/faq.html": {
    "href": "pages/getting-started/faq.html",
    "title": "Frequently Asked Questions | ezEngine",
    "keywords": "Frequently Asked Questions Platforms The ezEngine base libraries (Foundation, Core) have been built to be fully cross platform. The corresponding unit tests are run on Windows, Linux, Mac and Android every day. There are some stubs for functions that are currently only needed on Windows, but implementing them is easy to do, when needed. Everything higher level (editor, tools, rendering) is currently only implemented for Windows 11. There is no (official) Linux, no Mac and also no Windows 8 or lower support. A Linux port is currently being worked on, though. Render API At the moment we (officially) only provide a DX11 renderer. A Vulkan renderer is in development. Networking & Multiplayer The engine ships with the Enet networking library included. This is a great, easy to use library that was developed for an FPS game. There is no multiplayer support in EZ, whatsoever. For the foreseeable future we have no plans to add something. Multiplayer is a very complicated topic and the solutions vary drastically between genres, so providing something that \"just works\" isn't really possible. If you want to do multiplayer, you have to implement that aspect yourself. This is possible, of course, and for games with simpler networking logic (turn based, slow or simply not competitive) it may not be too hard (still lots of work, though, as all game development is). Visual Scripting Yes, EZ has visual scripting. You can do simple level logic, like \"if that lever is pulled, open that door\". You shouldn't expect to use it for larger features, though. We are generally not convinced that visual scripting is a great way to program (in no engine, no matter how good their tools are). However, for taking care of smaller tasks and as glue code between systems, it is a great way to get things done that would otherwise be quite cumbersome. Scripting with TypeScript We have a TypeScript binding which is pretty decent. The entire game logic in the Testing Chambers project is done with this. Terrain We currently have no terrain system. We have several ideas how we would like to do this, but this is very low priority at the moment. If you want to do terrain, you should just import static meshes. Of course that also means you need to do terrain sculpting in a separate tool. What we do have, is a simple heightfield component. For basic scenarios this may already be sufficient. AI We have an integration for building nav-meshes with Recast. We also have some really crappy components to do simple steering. Those are awful and need to be rewritten. If you have experience with AI and would like to contribute something in this regard, we would be happy to get some help. AI has low priority for us at the moment, but since this is a very isolated problem domain, you could probably improve the status quo significantly, without having to know the engine too much in detail. Streaming All our resources (textures, materials, shaders, ...) always use streaming. However, we have no level streaming. Since we also don't have a terrain system, this is currently not really needed. However, we do have asset collections, which can be used to load data in the background. So you could build a system that instantiates e.g. a part of a level only once the background loading is finished. The building blocks are there, but it's not working out of the box. Roadmap We don't plan very far ahead. Here are the things we intend to work on in the near term. Vulkan renderer Linux port Animation system improvements Better ragdoll system General usability improvements Game AI functionality History & Team The core team is about 5 people, who work on this project in their spare time. Time is our most precious resource, which is why we focus it on aspects where the project benefits the most. Good infrastructure has always been a focus for us, as this will save so much time in the long run. The team consists of engineers who previously worked on the Vision Engine. We started working on ezEngine with an empty folder and a first blank .h file in November 2012. Every line of code was written from the ground up. The first project was a unit test framework. For source control we started with Mercurial. In 2013 we switched to Subversion. The history from SVN was migrated to git in 2018 (f86ff53686f839a5f729ff70b9ec09e110cbef94 is the first commit). Since then all development is done in the open through GitHub. From the start our intention was to create something that is free to use with no restrictions. An important aspect was always to not create a monolithic engine that can only be used for certain types of games, but something that is flexible and can be adapted to many different use cases. Therefore, most functionality is plugin based and it is easy to strip out things that you don't need or replace them with a custom implementation. This design philosophy is for example why things like our texture processing are stand-alone tools, instead of being integrated into the editor, so that you can utilize these tools, even when you don't want to use the editor. Consequently, good documentation both directly inside code, as well as external, is very dear to us. The best technology is worthless, if only few people know how to use it. We would love to see EZ being used by other people. We try to fix issues and help out as well as possible, but there is only so much we can do with our time. We understand that for many people other engines are a better fit. If EZ does fit your needs, that's great. And if you are able to help out make it better, that's really awesome. See Also How to Contribute Contact"
  },
  "pages/getting-started/how-to-contribute.html": {
    "href": "pages/getting-started/how-to-contribute.html",
    "title": "How to Contribute | ezEngine",
    "keywords": "How to Contribute If you want to contribute to ezEngine, there are several ways how you can help out. Using the Engine Just using the engine and filing bug reports, feature requests or generally giving feedback is already a very, very valuable way of contributing. Be sure to include a detailed description of what you where doing, what you saw vs. what you expected, such that it is easy for us to reproduce. We cannot test every feature under all conditions and we do not know all the ways people want to use a feature, so giving feedback is a great way of contributing. Extend the Samples The samples are there for you to get to know the engine and have a playground to try out stuff. As you may notice, most samples are not \"complete\", ie. they could be extended a great deal with functionality, with code comments, etc. This was not a didactic decision, but rather due to limited time on our side, so if you play with a sample and improve it, even just slightly, please feel welcome to contribute your changes back, so that others can benefit from it. Of course you are also invited to write entirely new samples. Code Documentation We try to document our code as well as possible, but there are undocumented or poorly documented pieces, and of course there are also comments that are outdated or not as good as they could be. If you improve any documentation, please contribute it back, even if you only did minor changes or fixed typos. Bugfixes If you find a bug, and manage to fix it yourself, don't hold back! Create a PR and we will gladly integrate it. Become our personal hero by also adding an automated test to prevent regressions. If you itch to fix something you can also search our bug tickets. Items that we consider 'relatively easy' to fix without much knowledge of the engine, have the label good first issue. Unit Tests EZ ships with a large number of unit tests. Especially the lower level functionality is well covered with tests, but the higher up code coverage becomes more and more spotty. If you want to add a test, that's great. Especially if you run into a bug, no matter whether you are able to fix it yourself or not, adding a test that reproduces the bug (and thus ensures it won't reappear after a fix) is very useful. If you do not want to go through the hassle of setting up a proper test, even just posting a piece of code that reproduces an issue in general, allows us to put that into a proper test scenario with little effort. And if you really want to contribute to the overall test coverage, search the code-base for \"\\\\\\test\" or \"TODO\" to find code pieces that developers already marked up to need a test. Features We also value feature contributions, but with those we might be much more picky. If you feel confident enough to add a whole new feature, go for it. Contact us if you need help with something. In fact, unless it is a very small feature, it may be beneficial to contact us just to get on the same page what a feature should do, how it should be exposed to the user and what's the best way to implement it. You should have a strong background in C++, though, as we simply don't have the time to walk you through basic programming steps, we can only give high-level help on how to make stuff work inside EZ. See Also Contact Frequently Asked Questions"
  },
  "pages/getting-started/videos.html": {
    "href": "pages/getting-started/videos.html",
    "title": "Videos | ezEngine",
    "keywords": "Videos If you are interested in learning more about ezEngine, this page lists a couple of interesting videos. For the full list of available videos, see the ezEngine YouTube channel. ezEngine Overview ezEditor: Basic Editing ezEditor: Documents ezEditor: Asset Browser ezEditor: Gizmos ezEditor: Camera Controls ezEditor: Views ezEditor: Configurations Live Asset Reloading Surfaces Asset Curator Scripting with TypeScript"
  },
  "pages/releases/releases.html": {
    "href": "pages/releases/releases.html",
    "title": "Releases | ezEngine",
    "keywords": "Releases Releases can be downloaded from GitHub. Follow @ezEngineProject for release announcements. The latest version is: 21.6.1"
  },
  "pages/samples/asteroids.html": {
    "href": "pages/samples/asteroids.html",
    "title": "Asteroids Sample | ezEngine",
    "keywords": "Asteroids Sample The Asteroids Sample is a small game that shows how to use various components and messages to create simple game logic. Prerequisites Note: The project is only available when the solution is built with EZ_BUILD_SAMPLES activated. The game itself is a stand-alone application (TODO), but the assets that it uses need to be transformed by ezEditor. Open the editor project in Data/Samples/Asteroids with ezEditor, then open the asset browser and click the Transform All button (white box with red arrow). Afterwards you can run the Asteroids application. Code The code is a slightly more complex demonstration of how to write custom components and let them interact to create game logic. See Also Samples Videos"
  },
  "pages/samples/cs-histogram.html": {
    "href": "pages/samples/cs-histogram.html",
    "title": "Compute Shader Histogram Sample | ezEngine",
    "keywords": "Compute Shader Histogram Sample The ComputeShaderHistogram Sample shows how to write a simple compute shader, which takes an input image and generates a color histogram. Prerequisites Note: The sample is only available when the solution is built with EZ_BUILD_SAMPLES activated in CMake. Code The code shows how to use the EZ rendering API to set up buffers and textures, load shaders, etc. It also demonstrates how one can watch a directory for file changes and reload resources on the fly, to iterate on shaders. See Also Samples Videos"
  },
  "pages/samples/line-count.html": {
    "href": "pages/samples/line-count.html",
    "title": "LineCount Sample | ezEngine",
    "keywords": "LineCount Sample The LineCount Sample is a command line application that counts the lines in source code and prints out some statistics. Prerequisites Note: The sample is only available when the solution is built with EZ_BUILD_SAMPLES activated in CMake. Code The LineCount sample shows how to create a basic application using the ezEngine framework. The code derives directly from ezApplication, as it does not implement any game functionality. It then shows how to setup the filesystem, and do basic operations with files, strings and logging. See Also Samples Videos"
  },
  "pages/samples/monster-attack/devlog-1.html": {
    "href": "pages/samples/monster-attack/devlog-1.html",
    "title": "MA Devlog 1 - Intro | ezEngine",
    "keywords": "MA Devlog 1 - Intro For some reason the next two weeks I have more spare time than usual. So I decided to use this time to try to make a demo that is a bit more elaborate. And I decided to mention this here, so that I have more reasons to actually continue working on it. You know, I'm more of a technical person, and doing \"game development\" quickly becomes boring to me. I prefer to write the systems behind the scenes. Anyway, this project is limited to two weeks anyway, so let's see how far I get. Since I'm bad at game design, I need something that's already designed. So I chose to do a tower defense game. More specifically, a clone of Orcs Must Die!, a game that is a lot of fun (though I only know part 1 and 2). The nice thing about this game is, that the core mechanic is relatively quick to implement, but then there are lots and lots of details that can be added over time. In the past few days I've set up some basics of the project, gathered some assets (from https://quaternius.com and https://kenney.nl) and implemented the basic character controller and monster functionality. So I now have a simple level layout and some monsters that run from the start point to the end point: You can also already shoot and kill the monsters: See Also Monster Attack Sample"
  },
  "pages/samples/monster-attack/devlog-2.html": {
    "href": "pages/samples/monster-attack/devlog-2.html",
    "title": "MA Devlog 2 - Spike Trap | ezEngine",
    "keywords": "MA Devlog 2 - Spike Trap So today was all about getting my first trap working. Here is the result: The most flexible way to do these things, is to just use custom C++ components for everything. However, I want to test out our other infrastructure as well, for example the new visual scripting, the state machines, and so on, so the goal is to prefer those, and only use C++ for the things that really need it. At the moment I only have two custom C++ components, one for the player logic and one for the monsters. The former mostly does input handling and forwarding to the character controller, the latter mainly does the path finding and steering. So today I had to figure out how to build my first trap. If I were (or had) an artist, I would use an animated mesh with multiple animations for the different trap states. However, I only have two static meshes, one with spikes, one without, and I really want to make things work with the assets as I have them. So I needed a way to do the visuals without animations. Here is what it looks like in a close up: The trap has four states, therefore I built a state machine. In the Dormant state it does nothing. This is the state where the trap is not dangerous. It takes a second and then transitions into the Loaded state. In the loaded state, the spikes show up and peak through the bottom. Now when a creature walks over it, the trap enters the Firing state where it makes damage. After a short time, it enters the Retracting state, where the spikes move down and then it starts over. Again, there are different ways how you could achieve the animations and the behavior, but I wanted to use as much existing functionality as possible. For each State in the state machine you select what code it should run. And one existing type of state is the Switch Object state. What this will do, is it simply activates / deactivates game objects in your object structure. So you could for example use this to enable a particle effect node and thus make your object burning or switch to a different mesh object, so that it looks broken. For example the Retracting state deactivates all objects under the States group, but enables the Retracting object: My trap prefab looks like this: On the root node, there is my state machine component. Directly attached to it are the base mesh (second to last node) and a trigger (last node). Inside the States group there are the four different groups that represent the different states. For example, the Dormat group is just empty. The three other groups add the spiky mesh and the Firing group additionally adds area damage. Now as I said, for the animation one should use a skinned mesh and just play animations, but to achieve the up/down movement without this, I used the Slider component. Also I added a new Reset Transform component, to make the sub-objects move back into place each time. So now the Retracting and Firing group uses these components: All this gives you the visuals. Now on the state machine you have transitions (the arrows between the states). For each transition you again have to choose the \"type\". The transition type determines the logic to decide whether the transition should be taken or not. So you can have complex logic here. To start, I just used the Timeout transition, which would just cycle through the states with a 1 second delay. This is fine for all transitions, except for the one from Loaded to Firing. Here we only want to transition when a creature walks into the trap. To detect this, I decided to use a physics trigger. First this meant that my creatures need to have some kind of physics representation, which the physics trigger can detect at all. So I added a kinematic capsule shape, which just moves along with my creature (they currently don't use a character controller). Now whenever the creature walks into a trap, the trigger in my trap prefab will fire an event. However, so far this event won't have any effect. I need to hook up the event from the trigger to my state machine. And this is what visual scripts are really good for. On the root node of my trap, I added a Script Component: The script is quite trivial: Whenever the trigger fires, the script's OnMsgTriggerTriggered node gets executed. We then switch over the state and only react to Activated events. If so, we call FireTransitionEvent on the sibling StateMachineComponent and tell it to Fire. So now we have it. When a creature walks into the trigger, the visual script gets the physics trigger event, forwards that to the state machine and when the state machine happens to be in the Loaded state, it will transition into the Firing state. The state machine then changes which sub-objects in the trap are active, which in turn starts the spike movement and applies area damage to anything close by. See Also Monster Attack Sample"
  },
  "pages/samples/monster-attack/devlog-3.html": {
    "href": "pages/samples/monster-attack/devlog-3.html",
    "title": "MA Devlog 3 - Sound | ezEngine",
    "keywords": "MA Devlog 3 - Sound Today I spent some time on improving and polishing what I built so far. First I set up sound. EZ uses Fmod, so following the documentation and my own tutorial video, I created a new Fmod Studio project, downloaded a couple of sounds from <freesound.org> and added sounds for footsteps, the player's projectile and the trap. Footsteps for the player are quite easy, because the character controller already supports this through surface interactions. Basically, whenever something needs to interact with a surface, for example a bullet hitting a wall, we can easily spawn a surface interaction. Usually polygons are linked to a surface and the surface acts as a lookup table. So if I walk over a stone surface and I want to spawn a footstep interaction, the stone surface defines which prefab to use, and if I walk over a metal surface, it may define a different prefab to use. Surfaces are hierarchical, so both stone and metal surfaces are built on top of the Default surface, and as long as they don't define an override, the value from the Default surface is used. Therefore, on the character controller, all that we need to define is the name of the surface interaction, and how quickly those should be spawned when walking or running. The same system is used by the projectile to spawn a prefab when it hits something. For my trap, all I needed to add was a sound event to the Loaded state, so that it makes a mechanical noise when it is ready. The next thing I did, was to add code to the player component, so that the player can place traps. This is all done in C++. EZ has an abstract interface ezPhysicsWorldModuleInterface which you can query from the world, that gives you access to things like raycasts. To get this interface, you call ezPhysicsWorldModuleInterface* pPhysics = GetOwner()->GetWorld()->GetModule<ezPhysicsWorldModuleInterface>(); This is probably one of the most important such interfaces, since physics queries are so ubiquitously useful for all sorts of game mechanics. For now I simply use this to check where the player is looking. Meaning, I shoot a ray through the camera like this: ezPhysicsCastResult result; pPhysics->Raycast(result, pCameraObject->GetGlobalPosition(), pCameraObject->GetGlobalDirForwards(), 10.0f, params); Now I have the point that the player is looking at. The next step is to validate, that one can place a trap there. For now I only do very simply position snapping and some rotation, I don't yet prevent the player from placing traps where they don't belong. Of course, while in trap placement mode you want to have a preview how things would look like, so I built a copy of my trap prefab, that has no functionality, and I add that to the scene (and move it around) to show where the trap would end up. At some point this should probably also use a custom shader for a nice \"preview effect\", and some sounds when placing traps for better feedback. At some point I noticed that when a monster dies, it doesn't dissappear and a lot of bodies where piling up. So now the }monster component simply puts itself into a queue when it starts playing the die animation to keep track of dead bodies, and I delete the oldest one when I have more than 20. Finally, I wanted to have a second trap. My favourite trap in Orcs Must Die is the one that shoots arrows out of a wall. From its logic it's very similar to the spike trap, so I copied and adjusted it. This is what it looks like: I've added a Dart projectile prefab and this trap simply uses 24 spawn components to shoot a lot of those. This is it in action: However, I wanted the trap to shoot arrows three times in quick succession. Because that's way cooler. Turns out, this was absolutely trivial to do with the state machine. All I needed to do, was to copy two of the states a few times and set up a short time delay as transitions: And now it behaves like this: And together they already create quite some mayhem: Finally, in game, it looks and sounds like this: See Also Monster Attack Sample"
  },
  "pages/samples/monster-attack/devlog-4.html": {
    "href": "pages/samples/monster-attack/devlog-4.html",
    "title": "MA Devlog 4 - Tar Trap | ezEngine",
    "keywords": "MA Devlog 4 - Tar Trap Today I added a simple trap that just slows down monsters when they walk over it. As always, there are many ways how one can implement this. I chose to simply do a raycast at the monster position downwards and find the physics object beneath their feet. From there I could use the surface to determine whether to slow down the monster. That means that my trap needs to actually contain a physical collider object, so that the raycast can hit anything. Therefore my trap now looks like this: Here I use a query shape actor rather than a static actor, because I only want raycasts to be able to hit this collider, I do not want any other objects (like my player's character controller) to collide with it. But that also means that when I do my raycast, I need to make sure to include Query objects: Now this kinda worked, but I ran into the problem that the raycast would often not hit the ground, but the monster itself. To show the problem, in the video above I enabled the skeleton collider visualization for the blue monsters. Those are the animated shapes that are used for shooting the monsters. My raycast would sometimes hit that and then not detect the correct ground type. I had to set up proper collision layers and assign the right ones to the monster shapes, the ground shapes, and the raycast. Unfortunately this isn't fun, because there is a limit of 32 layers and you have to be very careful how to set them up, because their number can quickly grow. For now my setup looks like this: For debugging purposes, I wanted to see when exactly the monsters are slowed down, so I used the ezMsgSetColor to just dim the monsters mesh color when slowed: Finally, I exposed the health and walk speed as properties, so that I could configure my two monster types slightly differently. In C++ this macro block is used to declare which variables are configurable: And then these show up in the editor: See Also Monster Attack Sample"
  },
  "pages/samples/monster-attack/devlog-5.html": {
    "href": "pages/samples/monster-attack/devlog-5.html",
    "title": "MA Devlog 5 - Level Logic | ezEngine",
    "keywords": "MA Devlog 5 - Level Logic Today I've looked into doing some of the level logic. Meaning when monsters are spawned, when you win or lose, and so on. I've also added that placing traps costs money and you only have a limited amount of money. It's all working, but just so, and I'm not happy enough with it, that I think it already makes sense to dive deep into any of it. However, I can post some teasers at least. Here is the state machine for my test level: Since the game is very linear in nature, working with state machines makes a lot of sense, and makes my life indeed much easier. I can create building blocks for the different phases, and if I ever build multiple levels, they can be easily re-used in different configurations. For example here is the visual script that is used by the Countdown phase: It's using the cool new coroutine feature to run a script across many frames. It literally just counts down a number from 3 to 1 and then ends. In parallel the Update hook makes sure to print that number to screen every frame. Quite neat. See Also Monster Attack Sample"
  },
  "pages/samples/monster-attack/devlog-6.html": {
    "href": "pages/samples/monster-attack/devlog-6.html",
    "title": "MA Devlog 6 - More Level Logic | ezEngine",
    "keywords": "MA Devlog 6 - More Level Logic The last days I ran into expected problems. After having a prototype up an running, I found several bugs and encountered usability issues. My main usability issue was with the global state handling. The game needs to track a few things (for example, how many monsters are currently alive). These values need to be initialized and reset properly every time I start a level. I do all development from inside the editor, so I restart the game frequently, and this needs to work reliably. So one thing I did to make this easier, is to add the Global Blackboard Component. All it does, is to initialize blackboard values for you at level start. Then all other scripts work as desired. Since global state is truly global, it would otherwise persist even across runs and across level transitions (which can be useful). Another thing I needed, was to spawn many monsters in an area over a certain time, so that they disperse a bit. I've previously hacked this together using the spawn component but since this is a quite useful feature, I decided to write a proper new component for this. So now I can do this: Of note here is that all our \"gameplay\" components are mostly meant as demonstrations how one could do some feature. If those components are sufficient for you, that's great, but for a lot of use cases they won't be perfect and you are encouraged to write your own. The great thing about open source is obviously, that you can look at how the built-in features are done for inspiration. So these components give you a decent starting point for doing copy & paste, or how I like to call it \"CTRL+C, CTRL+Inspire\" ;) Another thing I've done, is to get my level state machine working properly. It currently looks like this: This nicely visualizes the level logic. We start at the initial state, after that the player has unlimited time to build traps (but limited money). The game displays \"Press 'G' to start\". Once the player does so, a short countdown is displayed, then the first wave of monsters is spawned and the player has to fight. Once all monsters from that wave are dead, a second countdown is shown, the second wave of (different) enemies is spawned and finally when all are dead, the player wins. State machines are made up of states and transitions between them. Only one state is active at any time. When a state is active, it may execute code. This can be C++ or Visual Script code. For example the \"Wave 1\" state looks like this: Here I've selected the script state type and given it my custom script. Additionally I can pass in parameters, so here I tell it which game object to use for spawning monsters. This way I can reuse the same script for different monster types. The script isn't very spectacular: All it does is find the desired game object via a global key, retrieves its spawn box component and tells it to start spawning. The actual game logic is mostly implemented on the transitions. Every state may have multiple outgoing transitions to different states. If the condition of a transition is met, the state machine follows that transition and switches to another state. What transition types are available can be extended with C++, but the most common ones are the timeout transition, which just waits for a bit, and the blackboard transition which checks whether some values are as desired. For example here is the Fight -> Countdown transition setup: This uses a compound to combine multiple transition types. Here a timeout of 10 seconds is used to enforce a minimum delay. Additionally we wait for the Monsters value in the blackboard to reach the value 0. Once this is the case, the Fight phase is over and the next phase can begin. Finally, you certainly noticed the Any State. This is a convenience feature, that allows you to basically add a transition from all states to another state, without having to set all that up manually. The Any State itself doesn't do anything, it is a placeholder. What you do configure, is the transition: Here our simple logic just says, that if the Points blackboard value reaches zero, we transition into the Lost state. You lose points every time a monster arrives at the level end. So how do we get all this running in our game? We just add a State Machine object into our scene: Here I also gave the object the global key LevelLogic, so that my C++ custom game state can also find it and send a message to it for the G input, but other than that it is self sufficient. Well, that was a long post. If you managed to read this far, here is a video of it all in action. See Also Monster Attack Sample"
  },
  "pages/samples/monster-attack/devlog-7.html": {
    "href": "pages/samples/monster-attack/devlog-7.html",
    "title": "MA Devlog 7 - Level Pieces | ezEngine",
    "keywords": "MA Devlog 7 - Level Pieces Yesterday I spent some time turning my greybox level into something actually presentable. For this I used the great assets from Quaternius. Here are a few screenshots of the results: Since my gameplay is modular (traps can only be placed on a grid), my level design also needs to cater to this, and my workflow can benefit from it. My first attempt was to place the modular assets as I thought they should fit. This didn't work out, though, and I needed to go for a more rigid approach. So my next attempt was to build modular prefabs that always take up either 2x2 meters or a multiple of that. So a floor piece looks like this: Now I needed some guidance on how to place walls. Would you place walls ON the floor plates? That's what I did in my first, failed attempt. So this time I made wall pieces take up their own tile and to make this very clear during placement, they contain their floor section as well: Thus my level is made up of relatively large and easy to place pieces: Once I had my level rebuilt with these pieces, I wanted some lighting and decoration. This becomes annoying pretty fast, especially when you want certain things to always look the same. For example the torches were all supposed to be in similar places (same height etc). So I had the idea to just put some decoration elements directly into my wall prefabs and make them an optional switch. I used exposed prefab parameters to achieve this. Basically, I just expose the Active property of some group object under a nice name. For example for the floor prefab I added that it can optionally have a ceiling, and I added a second option to have a ceiling with a grate in it: So now when I place this prefab, I get these options: With these prefabs at hand, building my level was quite straight forward and it is relatively quickly modified. It's always amazing, how a few nice assets turn something from \"crappy proof of concept\" into \"looks like a proper game\", even though it's still way closer to the former :D See Also Monster Attack Sample"
  },
  "pages/samples/monster-attack/monster-attack.html": {
    "href": "pages/samples/monster-attack/monster-attack.html",
    "title": "Monster Attack Sample | ezEngine",
    "keywords": "Monster Attack Sample Monster Attack (MA) is a tower-defense sample game inspired by the game Orcs Must Die!. The sample demonstrates the use of a wide range of features. Parts of its development are covered in a series of Devlogs. The code and data are hosted in a separate GitHub repository. Please see its main Readme file for up-to-date build instructions, game controls and other details. Video Click the image below for a demo playthrough video: Devlogs This sample is accompanied by a series of Devlogs (see sidebar on the left) which describe how to use various engine features to accomplish different game mechanics. See Also Samples Videos"
  },
  "pages/samples/pacman.html": {
    "href": "pages/samples/pacman.html",
    "title": "PacMan Sample | ezEngine",
    "keywords": "PacMan Sample This sample is a basic implementation of the classic PacMan game. It demonstrates various aspects of the engine, for example how to add custom code through the C++ project generation. How to Run the Game To run this sample, you have to first compile the code for it. Select to Project > C++ Project > Compile Plugin. Make sure the compilation step succeeded. If it fails, select Project > C++ Project > Setup C++ Plugin... and make sure the solution can be generated successfully, then open the solution and make sure it compiles correctly. Afterwards run the scene. For the best experience, use the Export and Run mode (Ctrl+R). Video: Game Tutorial - PacMan This tutorial video shows how to build a PacMan game from scratch. It walks you through the project setup and the fundamental steps to get the game logic working. To get a deeper insight into how to make a more fully functional game, have a look at the code of the sample project. Project The editor project belonging to this sample can be found under Data/Samples/PacMan. It contains a few assets taken from Quaternius and freesound.org. Code The code uses a custom C++ plugin, generated using the C++ project generation. It provides custom components, and also utilizes a custom game state for the higher level logic. It is advised to watch the tutorial video first, to get a grasp of the basics, and then study the code for details that aren't covered in the video. For example the project also shows: how to play sounds how to use a blackboard for tracking state how to reset a scene (by loading it from scratch) how to draw some (debug) text to the screen See Also Samples Videos"
  },
  "pages/samples/rts.html": {
    "href": "pages/samples/rts.html",
    "title": "RTS Sample | ezEngine",
    "keywords": "RTS Sample The RTS Game Plugin is a more complex example of how to write a plugin that adds custom components and game logic. Prerequisites Note: The sample is only available when the solution is built with EZ_BUILD_SAMPLES activated in CMake. The sample will only work, if the RtsGamePlugin has been compiled. Project The editor project belonging to this sample can be found under Data/Samples/RTS. Open the Space scene document. Make sure to transform all assets (in the AssetBrowser panel the box with the red arrow). Then press 'Play the Game' in the scene. Use the mouse to create, select and send ships around. Further usage instructions are listed by the game UI. Code The code shows how to use a custom ezGameState for high-level game logic, as well as a number of custom ezComponent classes for various different game elements. See Also Samples Videos"
  },
  "pages/samples/sample-game-plugin.html": {
    "href": "pages/samples/sample-game-plugin.html",
    "title": "Sample Game Plugin | ezEngine",
    "keywords": "Sample Game Plugin The SampleGamePlugin demonstrates the basics of how to build a custom plugin for game code that can run both in a stand-alone application (such as ezPlayer) as well as inside the editor. Prerequisites Note: The project is only available when the solution is built with EZ_BUILD_SAMPLES activated. GameState The SampleGameState class shows how to implement a simple game state that adds high-level game logic, such as handling a game UI. See ezGameState and ezGameApplication for further details. Components The DemoComponent shows how to modify the transform of an object dynamically. The DebugRenderComponent shows how to use debug rendering. For further details also see ezComponent. Project Under Data/Samples/SampleGame you will find an editor project which uses the SampleGamePlugin. Note that the project references the plugin as a runtime plugin (under Project > Plugin Settings > Plugin Selection...). This makes the custom components available to the editor. When you press 'Play' in the editor, the scene will be simulated and the custom components, such as the DemoComponent, will take effect. When you press 'Play the Game' a full game window is launched and now even the custom game state is instantiated and executed. Consequently, the UI will appear and you can interact with it. Note that this still runs inside the editor process. You can also export and run the scene externally in the stand-alone ezPlayer application. See Also Samples Running a Scene Game States Custom Code Videos"
  },
  "pages/samples/samples-overview.html": {
    "href": "pages/samples/samples-overview.html",
    "title": "Samples | ezEngine",
    "keywords": "Samples Sample data is mostly stored in the Content git sub-module. If any data is missing, make sure your git sub-module is updated (git submodule update). Some samples may come purely as data (editor projects). Other samples require custom code. Sample code is generally only available if the CMake build switch EZ_BUILD_SAMPLES is set. Make sure to compile all sample projects, if you want to try them out. Getting Started The Testing Chambers project is a good start point to see what you can do purely through the editor and using scripting. No custom C++ code is used in this project. If you want to see how you can extend the engine with your own C++ code through a plugin, have a look at the Sample Game Plugin or the RTS Sample. The former shows just the basics to get your own code running. The latter goes a bit further and shows how you can make a simple game. See Also Videos"
  },
  "pages/samples/screenshots.html": {
    "href": "pages/samples/screenshots.html",
    "title": "Screenshots | ezEngine",
    "keywords": "Screenshots For tutorial videos, have a look here. Tools Samples UI Skeletal Animations Particle Effects Physics Terrain Vegetation Decals AI Other See Also Samples Videos"
  },
  "pages/samples/shader-explorer.html": {
    "href": "pages/samples/shader-explorer.html",
    "title": "Shader Explorer Sample | ezEngine",
    "keywords": "Shader Explorer Sample The ShaderExplorer Sample shows how to render a scene entirely in the shader, with no polygonal geometry. The shader evaluates geometric shapes and computes color and shadows inside the shader. Prerequisites Note: The sample is only available when the solution is built with EZ_BUILD_SAMPLES activated in CMake. Code The C++ code uses a minimal setup to load the shaders and render a quad on screen. Check out the shader code to see how the scene rendering is done. See Also Samples Videos"
  },
  "pages/samples/testing-chambers.html": {
    "href": "pages/samples/testing-chambers.html",
    "title": "Testing Chambers | ezEngine",
    "keywords": "Testing Chambers The Testing Chambers Project is an editor-only project (no sample code required) that shows various features of the engine. It is a good starting point to play around with and get to know the editor. Prerequisites This sample is data only and no extra code needs to be compiled to make it run. Project The editor project belonging to this sample can be found under Data/Samples/Testing Chambers. Open the Corridor scene document. Make sure to transform all assets (in the AssetBrowser panel the box with the red arrow). Then press 'Play the Game' in the scene. Use WSAD to move around, left click to shoot, Space to jump, CTRL to crouch and 1, 2 and 3 to switch weapons. See Also Samples Videos"
  },
  "pages/samples/texture-sample.html": {
    "href": "pages/samples/texture-sample.html",
    "title": "Texture Sample | ezEngine",
    "keywords": "Texture Sample The TextureSample demonstrates the streaming functionality of the resource manager. It does so by creating a custom resource loader, which provides the engine with a sheer endless amount of textures. Click and move the mouse around to scroll over the grid and thus move the camera to 'unexplored' areas. The resource manager will then stream in the required resources. Prerequisites Note: The sample is only available when the solution is built with EZ_BUILD_SAMPLES activated in CMake. Code The code creates a custom resource loader, which virtualizes reads to a large amount of textures (placed on a grid), by redirecting all reads to the same file on disk. See Also Samples Videos"
  },
  "privacy-policy.html": {
    "href": "privacy-policy.html",
    "title": "Privacy Policy for ezEngine Website | ezEngine",
    "keywords": "Privacy Policy for ezEngine Website *Effective Date: January 15, 2024 Thank you for choosing ezEngine. This Privacy Policy explains how we handle your information when you visit our website. Please read this policy carefully to understand our practices regarding your data. Information We Don't Collect: Personal Information: We do not collect any personally identifiable information such as names, addresses, phone numbers, or email addresses. Browsing Information: We do not track your individual browsing activities on our website. User Accounts: We do not require users to create accounts, and consequently, we do not store any account-related information. Information We Collect: Theme Cookie: We use a single cookie to store your user-selected theme preference. This cookie is strictly limited to enhancing your experience on our site and does not contain personal information. Information Sharing: No Third-Party Sharing: We do not share any collected information, as there is no personal data collected. Data Security: Secure Connection: Our website uses secure connections (HTTPS) to protect the information during transmission. Your Choices: Cookie Management: You can manage your cookie preferences through your browser settings. However, disabling the theme cookie may affect your user experience. Changes to this Privacy Policy: Policy Updates: Any changes to this Privacy Policy will be posted on this page. Please review this policy periodically for updates. Contact Us: If you have any questions or concerns about our privacy practices, please contact us at ezengineproject@gmail.com. Thank you for trusting ezEngine."
  }
}